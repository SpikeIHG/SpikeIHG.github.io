<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Winter&#39;s</title>
  
  
  <link href="https://spikeihg.github.io/atom.xml" rel="self"/>
  
  <link href="https://spikeihg.github.io/"/>
  <updated>2024-10-26T11:54:15.781Z</updated>
  <id>https://spikeihg.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>渡口牛仔</title>
    <link href="https://spikeihg.github.io/2024/10/14/%E6%B8%A1%E5%8F%A3%E7%89%9B%E4%BB%94/"/>
    <id>https://spikeihg.github.io/2024/10/14/%E6%B8%A1%E5%8F%A3%E7%89%9B%E4%BB%94/</id>
    <published>2024-10-13T16:05:40.000Z</published>
    <updated>2024-10-26T11:54:15.781Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Space-Cowboy"><a href="#Space-Cowboy" class="headerlink" title="Space Cowboy"></a>Space Cowboy</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="D:\github.1.0\My_blog_hexo\source\images\cowboy2.png"                                     ></p><h2 id="感谢SpaceX的所有人，让我离成为太空牛仔又近了一步-ω"><a href="#感谢SpaceX的所有人，让我离成为太空牛仔又近了一步-ω" class="headerlink" title="感谢SpaceX的所有人，让我离成为太空牛仔又近了一步 ^ω^"></a><font color =pink>感谢SpaceX的所有人，让我离成为太空牛仔又近了一步 ^ω^</font></h2><h2 id="哎-老了-too-old"><a href="#哎-老了-too-old" class="headerlink" title="哎 老了  too old"></a>哎 老了  too old</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Space-Cowboy&quot;&gt;&lt;a href=&quot;#Space-Cowboy&quot; class=&quot;headerlink&quot; title=&quot;Space Cowboy&quot;&gt;&lt;/a&gt;Space Cowboy&lt;/h2&gt;&lt;p&gt;&lt;img  
                     la</summary>
      
    
    
    
    
    <category term="职业规划" scheme="https://spikeihg.github.io/tags/%E8%81%8C%E4%B8%9A%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>云</title>
    <link href="https://spikeihg.github.io/2024/09/02/%E4%BA%91%E5%90%96/"/>
    <id>https://spikeihg.github.io/2024/09/02/%E4%BA%91%E5%90%96/</id>
    <published>2024-09-02T07:41:08.000Z</published>
    <updated>2024-11-15T11:47:36.471Z</updated>
    
    <content type="html"><![CDATA[<h1 id="拾云"><a href="#拾云" class="headerlink" title="拾云"></a>拾云</h1><h2 id="写在扉页"><a href="#写在扉页" class="headerlink" title="写在扉页"></a>写在扉页</h2><p><em><strong>献给我家的小狗，如果他也喜欢看云并且讨厌java的话（我指的咖啡，确信 : -）</strong></em></p><p><em><strong>To my puppy , if he loves clouds and hates java(I mean the coffee , IMAO : -)</strong></em></p><h2 id="拾云-1"><a href="#拾云-1" class="headerlink" title="拾云"></a>拾云</h2><p><strong>拾云(Cloudsification)，一款给云分类，创建云朵图集的简单应用拾云 （拾取，拾取落叶，贝壳一样哈哈）移除电台LSD，加入绿日牌吗啡（又移除绿日牌吗啡，改为亚当版安慰剂）</strong></p><hr><h2 id="—-gt-apk下载地址点我点我-lt-—"><a href="#—-gt-apk下载地址点我点我-lt-—" class="headerlink" title="—- &gt;apk下载地址点我点我&lt;—-"></a>—- &gt;<a class="link"   href="https://www.bilibili.com/video/BV1yR4y1A74Z/?spm_id_from=333.788.recommend_more_video.6&vd_source=c8c7f6103570a31005f12d5a33a60b47" >apk下载地址 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><a href="/doc/%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6(42%E7%A7%8D%E7%97%85%E6%AF%92%E7%86%AC%E5%88%B6).apk">点我点我</a>&lt;—-</h2><hr><h2 id="WARNING-郑重声明"><a href="#WARNING-郑重声明" class="headerlink" title="WARNING(郑重声明)"></a><font color=gold>WARNING(郑重声明)</font></h2><ul><li>首先，上面写在扉页的话，只是笔者模仿一些文学作品瞎扯的，笔者的小狗没喝过咖啡。但他很忧郁，时常仰望天空。</li><li>该应用只需要几个简单的权限，大概就是没事偷偷调动你的摄像头拍个照，或者看看你内存里qq的聊天信息，或者就是自爆。</li></ul><h2 id="ERROR-警告"><a href="#ERROR-警告" class="headerlink" title="ERROR(警告)"></a><font color = red>ERROR(警告)</font></h2><ul><li>该程序很糟糕，但是无毒无害，笔者已替大家提前品尝过了 (^ω^)</li></ul><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><ul><li>首先感谢组员@twilight，尽管他前期一直在划水，但是他后期依旧在划水，什么也没做。bro唯一的贡献就是实验课帮忙占位置。（报告就交给你了）</li><li>感谢组员@？！ 在数据集与模型训练上的贡献</li><li>感谢bro @43Cas @听风 提供的宝贵反馈意见</li><li>感谢笔者家的小狗，他将永久负责代码库的维护</li></ul><hr><ul><li>github仓库地址 <a class="link"   href="https://github.com/SpikeIHG/Cloudsification" >https://github.com/SpikeIHG/Cloudsification <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li></ul><p>下同github readme </p><h1 id="Cloudsification"><a href="#Cloudsification" class="headerlink" title="Cloudsification"></a>Cloudsification</h1><h2 id="cloudsfication-means-clouds-identification"><a href="#cloudsfication-means-clouds-identification" class="headerlink" title="cloudsfication means clouds identification"></a>cloudsfication means clouds identification</h2><ul><li><p>An app to help record and identify the clouds 一款使用本地模型和调用接口的云朵分类app</p></li><li><p>平台 : Android Studio  本地模型 : MobileNet V2 API接口 : gpt-4o-2024-08-06</p></li><li><p>更多应用有关配置参数可以查看仓库种的gradle配置文件</p></li><li><p>应用设计细节，详见doc</p></li></ul><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">├── LICENSE</span><br><span class="line"></span><br><span class="line">├── README.md # README</span><br><span class="line"></span><br><span class="line">├── doc   # demo and QaA</span><br><span class="line"></span><br><span class="line">│  └── outline.md  # 项目大纲</span><br><span class="line"></span><br><span class="line">  ├── log.md # 笔者的船长日记，从宁静号退役前的老习惯</span><br><span class="line"></span><br><span class="line">  ├── Reference.md # 技术参考汇总</span><br><span class="line"></span><br><span class="line">  ├── file # 详细的文案以及设计</span><br><span class="line"></span><br><span class="line">  └── image # 一些笔者画的设计草图</span><br><span class="line"></span><br><span class="line">└── .idea # 本地环境配置信息 </span><br><span class="line"></span><br><span class="line">├── .kotlin/error # kotlin 调试信息</span><br><span class="line"></span><br><span class="line">├── app # 程序主体</span><br><span class="line"></span><br><span class="line">├── assets # 笔者的临时文件及</span><br><span class="line"></span><br><span class="line">├── gradle # gradle 依赖</span><br><span class="line"></span><br><span class="line">├── test # as自身测试文件</span><br><span class="line"></span><br><span class="line">└── ...... 一堆配置文件</span><br></pre></td></tr></table></figure></div><ul><li>总之，笔者是临时花了一个多月编写本程序，作为一门实验课程的作业，代码肯定粗糙，欢迎fork 和issue </li><li>还有些东西想不起来了，这两天人有点生病 ~~~ 老矣。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;拾云&quot;&gt;&lt;a href=&quot;#拾云&quot; class=&quot;headerlink&quot; title=&quot;拾云&quot;&gt;&lt;/a&gt;拾云&lt;/h1&gt;&lt;h2 id=&quot;写在扉页&quot;&gt;&lt;a href=&quot;#写在扉页&quot; class=&quot;headerlink&quot; title=&quot;写在扉页&quot;&gt;&lt;/a&gt;写在扉页&lt;/h</summary>
      
    
    
    
    
    <category term="JAVA" scheme="https://spikeihg.github.io/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title>仿生主人会梦见电子狗吗</title>
    <link href="https://spikeihg.github.io/2024/05/29/%E7%BB%84%E5%8E%9F/"/>
    <id>https://spikeihg.github.io/2024/05/29/%E7%BB%84%E5%8E%9F/</id>
    <published>2024-05-29T03:29:24.000Z</published>
    <updated>2024-10-13T16:40:45.948Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于计算机组成原理的相关思考"><a href="#关于计算机组成原理的相关思考" class="headerlink" title="关于计算机组成原理的相关思考"></a>关于计算机组成原理的相关思考</h1><ul><li><font color=pink>想到一个故事，想到一个名字，借用仿生人会梦见电子羊吗——仿生主人会梦见电子狗吗。 大概就是通过一个人设计一个电子狗来简单地串联起计算机组成，尽可能自然地从逻辑上推理出计算机的组成，为什么有这些部件，为什么这样设计，问题都能够通过一种比喻的形式得到解答。</font></li></ul><h2 id="码距"><a href="#码距" class="headerlink" title="码距"></a><font color=salmon>码距</font></h2><ul><li><font color=pink>Diane使用的是从废品点淘来的机械零件，大多来自于第二废世战争的遗留。所以，这些古董零件不可避免具有很大的出错率，为了能过保证绝大多数情况下的成功传送，更近一步，甚至能够定位每一次传送中的错误并且相应地进行修改，Diane研究了传送的条文，提出加入冗余的信息，这就是校验码的提出。 本质 增加码距。利用1的个数，如果我们出错位数大多数都只是1位的情况下的话。多重奇偶校验，构建一个矩阵的思想 </font></li></ul><h2 id="所以，仿生添添会梦见电子我吗"><a href="#所以，仿生添添会梦见电子我吗" class="headerlink" title="所以，仿生添添会梦见电子我吗"></a><font color=gold>所以，仿生添添会梦见电子我吗</font></h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;关于计算机组成原理的相关思考&quot;&gt;&lt;a href=&quot;#关于计算机组成原理的相关思考&quot; class=&quot;headerlink&quot; title=&quot;关于计算机组成原理的相关思考&quot;&gt;&lt;/a&gt;关于计算机组成原理的相关思考&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=pink</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>关于计算机操作系统的一些思考</title>
    <link href="https://spikeihg.github.io/2024/05/11/%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"/>
    <id>https://spikeihg.github.io/2024/05/11/%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/</id>
    <published>2024-05-11T08:28:43.000Z</published>
    <updated>2024-10-13T16:14:12.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OS"><a href="#OS" class="headerlink" title="OS"></a><font color=pink>OS</font></h1><ul><li><font color=skyblue>最近正好在复习计算机系基础，这里就总结一些一些东西，同时记下我自己关于计算机系统学习的一点思考和体会，当然这个也是对CSAPP的再次回顾。</font></li><li><font color=salmon>发现一个奇巧的东西，先按ctrl然后按tab可以实现整个段的tab</font></li></ul><h2 id="补码横空出世"><a href="#补码横空出世" class="headerlink" title="补码横空出世"></a>补码横空出世</h2><ul><li><font color=yellow>简化了底层电路运算的设计，加减统一了，实现运算器的时候只需要根据加还是减法的标记位进行减数的取反。</font></li><li><font color=yellow>注意，对于无符号数和有符号数，他们的最大位为1其他位全0的数互为相反数。这种特殊情况值得注意一下。</font></li><li><font color=yellow>如果同时存在有符号数和无符号数，一个表达式里，会讲有符号提升为无符号，至于编译器确定字面值时，根据范围去确定有无符号，有点奇巧。但是最新标准中是符合直觉的</font></li><li><font color=yellow>浮点数的设计很巧妙，具体记住几个关键点就可以了。尾数省略了一个1，阶数一个bias，等于2^(k-1) -1k&#x3D;8&#x2F;12两种精度下的取值，同时全0和最大是非规格数 ，阶码为0 用于表示0。 以及首位为0的数保证一个平滑地过渡。阶码最大就是NaN 和无穷大的表示。同时 符号位决定这个异常是否通知。1通知 符号位1是负数，注意小数部分的转换成二进制的方法。乘基取余，然后顺序从上至下。 总算知道为什么忽略1可以提高表示范围了，因为一个二进制数的第一位一定是1。 知道一个ascii 中数字刚好对应十六进制表示的30H 开始</font></li><li><font color=yellow>注意按位运算 ~ 和逻辑运算 !</font></li><li><font color=yellow>基于位运算的认识，以及补码，无符号有符号运算的认识，我们可以设计加法器，这个也正式引入一些控制信号，ZF CF OF SF。在汇编语言中我们也会再次见到 这四个标志位的设置非常简洁巧妙</font></li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/ZF.png"                                     ></li><li><font color=yellow>浮点运算 对阶，检查阶数是否正确。总结一下，布尔代数的完整性和数学上的正确性，保证了计算机数据运算的合理，我们只需要做的就是从工程上实现布尔代数。 这一部分可以作为了解，锻炼思考能力，不用强行记忆。</font></li></ul><h2 id="汇编与机器指令"><a href="#汇编与机器指令" class="headerlink" title="汇编与机器指令"></a>汇编与机器指令</h2><ul><li><font color=yellow>今天才知道解释器是逐行翻译运行，微程序其实与机器指令也存在一种对应关系，具体就是指令的操作码相当于微程序的序号，操作数相当于参数 有点类似一种伪函数调用过程</font></li></ul><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul><li><font color=yellow>链接是一个相对新奇的东西，在我第一次接触的时候。现在回想一遍，感觉主要思考一个关于重定位的问题。以及最后的加载执行问题就可以了。不过感觉要想真正的完整的认识链接，还是需要在理解了虚拟内存的基础上。</font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;OS&quot;&gt;&lt;a href=&quot;#OS&quot; class=&quot;headerlink&quot; title=&quot;OS&quot;&gt;&lt;/a&gt;&lt;font color=pink&gt;OS&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=skyblue&gt;最近正好在复习计算机系基础，这里就总结一</summary>
      
    
    
    
    
    <category term="Linux" scheme="https://spikeihg.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>JSGKD</title>
    <link href="https://spikeihg.github.io/2024/04/28/JSGKD/"/>
    <id>https://spikeihg.github.io/2024/04/28/JSGKD/</id>
    <published>2024-04-28T02:15:39.000Z</published>
    <updated>2024-04-30T03:22:05.005Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如果今天也是晴天"><a href="#如果今天也是晴天" class="headerlink" title="如果今天也是晴天"></a><font color=pink>如果今天也是晴天</font></h1><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/20s.jpg"                                     ></p><p><font color=orange>村平处女作——《四人分别华尔兹》 大致构思了一下</font></p><p>感觉五六十年代的乡村音乐和越战场景绝配。 库布里克果然大师。</p><p>hello vietnam 。 魔幻世界里的丛林不一定在拉美，也可以在东南亚，越南。献给在宏大叙事下竭力骂人的个体。</p><p><font color =pink>《西贡后摇》</font>&gt; 老早的想法了，用现有电影拼接出一部新的电影出来。可以命名为 《村平狂想曲》001 号 002号 这样遍下去 ，因为难得取名 或者说因为是拼接的作品所以存在无限多的解释，给予所有观众最大的自己理解空间。 看来确实是一项大工程 需要反复看一些候选电影 分析很多段落镜头 然后记忆 联想合乎母题的其他镜头。 good 对脑子里的电影回收再利用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如果今天也是晴天&quot;&gt;&lt;a href=&quot;#如果今天也是晴天&quot; class=&quot;headerlink&quot; title=&quot;如果今天也是晴天&quot;&gt;&lt;/a&gt;&lt;font color=pink&gt;如果今天也是晴天&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;&lt;img  
               </summary>
      
    
    
    
    
    <category term="NATURE" scheme="https://spikeihg.github.io/tags/NATURE/"/>
    
  </entry>
  
  <entry>
    <title>AI_Singer</title>
    <link href="https://spikeihg.github.io/2024/04/12/AI-Singer/"/>
    <id>https://spikeihg.github.io/2024/04/12/AI-Singer/</id>
    <published>2024-04-12T09:16:48.000Z</published>
    <updated>2024-11-09T13:44:13.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AI-SINGER"><a href="#AI-SINGER" class="headerlink" title="AI_SINGER"></a>AI_SINGER</h1><h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><p>罗列相关链接，总结一些FAQ</p><ul><li>训练的时候改一下保存模型的step间隔 400 或者600 也许可以 </li><li>拼接音轨</li><li>后期处理 bandlab 也许不错 melodyne 修音软件 似乎更好一点。可以学学</li></ul><h2 id="PEFT-参数高效微调"><a href="#PEFT-参数高效微调" class="headerlink" title="PEFT 参数高效微调"></a>PEFT 参数高效微调</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a><font color=pink>背景</font></h3><ul><li>深度学习的研究中出现了许多大型预训练模型，例如GPT-3、BERT等，这些模型可以在多种自然语言处理任务中取得优异的性能表现。然而，这些大型预训练模型的训练成本非常高昂，需要庞大的计算资源和大量的数据，一般人难以承受。这也导致了一些研究人员难以重复和验证先前的研究成果。为了解决这个问题，研究人员开始研究Parameter-Efficient Fine-Tuning (PEFT)技术。PEFT技术旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，PEFT技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。</li></ul><h3 id="总览"><a href="#总览" class="headerlink" title="总览"></a><font color=pink>总览</font></h3><ul><li>冻结预训练模型的某些层并仅微调特定于下游任务的最后几层</li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/peft.png"                                     ></li></ul><h3 id="常见的几种方法的基本思想与原理简介"><a href="#常见的几种方法的基本思想与原理简介" class="headerlink" title="常见的几种方法的基本思想与原理简介"></a><font color=pink>常见的几种方法的基本思想与原理简介</font></h3><ul><li><p>BitFit </p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/bitfit.png?raw=true"                                     ></p><p>优点：代码简单，原理简单。 缺点：优化效果不清楚，具体效果随着不同模型的参数规模变化较大，不具有太多的实际用处。</p></li><li><p>prefix tuning</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/prefix.png?raw=true"                                     ></p><p>Prefix Tuning方法由斯坦福的研究人员提出，与Full-finetuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。同时，为了防止直接更新Prefix的参数导致训练不稳定的情况，他们在Prefix层前面加了MLP结构(相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果)，训练完成后，只保留Prefix的参数。</p></li><li><p>prompt tuning</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/prompt.png?raw=true"                                     ></p><p>该方法可以看作是Prefix Tuning的简化版本，只在输入层加入prompt tokens，并不需要加入MLP进行调整来解决难训练的问题。同时还有一个区别就是，prompt tuning只在输入层加入，而prefix 是在transformer计算的每一层都加入。同时，prompt tuning 又分为soft , hard prompt 两种，区别就是hard 通常初始化的prompt是明确指定了与下游任务有关的。soft可以理解为泛化的。同时prompt tuning 在参数规模增大时表现出的效果更加突出。</p><ul><li><p>p tuning<br><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/pt.png?raw=true"                                     ></p><p>用MLP+LSTM的方式来对prompt embedding进行一层处理</p></li></ul></li><li><p>Adapter tuning</p><p>如下图所示的Adapter结构，将其嵌入Transformer的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter结构进行微调。</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/adap.png?raw=true"                                     ></p><p>存在一个问题，因为额外引入的一个adapter层，可能导致最终推理时长增加。</p></li><li><p>lora</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/lora.png?raw=true"                                     ></p><p>核心思想就是将权重更新的Δw分解为两个低秩小矩阵。 在训练时，我们同时走两条通路，一条Δw，另一条就是ab矩阵，最后优化时我们只更新ab矩阵，最后合并回去。具有推理时没有额外计算开销的优点。prompt系列会额外引入参数。</p></li><li><p>IA</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/ia.png?raw=true"                                     ></p><p>原理简言之，就是加入四个可学习向量处理原本transformer中的k,v,q,ffn 。 优化时只改变这几个可学习向量。最终也没有额外计算开销。</p></li></ul><h3 id="效果总览"><a href="#效果总览" class="headerlink" title="效果总览"></a>效果总览</h3><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/fitt.png?raw=true"                                     ></p><h3 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h3><p>目前使用最为广泛的peft的现成库是hugging face的peft，高度集成，调用方便，具体使用的时候查看相关的文档。</p><h3 id="更多思考"><a href="#更多思考" class="headerlink" title="更多思考"></a>更多思考</h3><p>上述所有的方法，具体应用时存在的问题，以及超参数的选择都存在很大的调整空间，这里提供几个参考资料供想要深入探索的群友参考</p><p><a class="link"   href="https://arxiv.org/pdf/2303.15647" >更多的方法与更加具体的原理 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link"   href="https://www.bilibili.com/video/BV1Y8411k7yD/?spm_id_from=333.788&vd_source=c8c7f6103570a31005f12d5a33a60b47" >一些方法的具体代码演示 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link"   href="https://link.zhihu.com/?target=https://github.com/huggingface/peft" >peft的代码实现 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link"   href="https://arxiv.org/pdf/1902.00751" >peft的起源 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><h2 id="谢谢大家-：）"><a href="#谢谢大家-：）" class="headerlink" title="谢谢大家 ：）"></a>谢谢大家 ：）</h2><p>PS : 上次没有解答的关于lora 为什么分解成两个矩阵可以优化</p><p><a class="link"   href="https://arxiv.org/pdf/2106.09685" >https://arxiv.org/pdf/2106.09685 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>总结一下就是：是将Δw矩阵分解，但是是一个向低维空间投射，也就是说降秩。也就是说lora有一个假设的前提就是我们微调一个矩阵降秩后的矩阵也可以达到接近的效果。</p><ul><li><p>idea  创建虚拟人格 举个例子 用户自定义加入一个爱观测星空的特质 然后可能设置一个遗忘机制 如果超过多久的对话中没有提及这个特质  就是说 是的 我都很久没有观测星空了</p></li><li><p>github界面的搜索 就是 点击一下变量名就可以了 我去</p></li><li><p>alt + enter 键解决导包 自动导包</p></li><li><p>desc 降序 或者 描述缩写</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AI-SINGER&quot;&gt;&lt;a href=&quot;#AI-SINGER&quot; class=&quot;headerlink&quot; title=&quot;AI_SINGER&quot;&gt;&lt;/a&gt;AI_SINGER&lt;/h1&gt;&lt;h2 id=&quot;Overall&quot;&gt;&lt;a href=&quot;#Overall&quot; class=&quot;he</summary>
      
    
    
    
    
    <category term="DL" scheme="https://spikeihg.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Bash</title>
    <link href="https://spikeihg.github.io/2024/04/11/Bash/"/>
    <id>https://spikeihg.github.io/2024/04/11/Bash/</id>
    <published>2024-04-11T08:08:31.000Z</published>
    <updated>2024-04-11T08:49:47.676Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bash-脚本总结"><a href="#Bash-脚本总结" class="headerlink" title="Bash 脚本总结"></a>Bash 脚本总结</h1><h2 id="命令总结"><a href="#命令总结" class="headerlink" title="命令总结"></a>命令总结</h2><ul><li><p>多命令执行 ; 顺序执行 &amp;&amp; ，前面执行成功后后面才会执行， || 前面错误后面才会执行</p></li><li><p>*拓展0个或者多个字符， ? 拓展一个任意字符 [] 匹配里面任意的字符 &gt; 重定向 &gt;&gt; 接着后面键加入 &lt; 作为命令的输入</p></li><li><p>readonly 关键字 来声明变量 让变量作为一个只读变量 unset可以删除变量</p></li><li><p>printenv 和 env都可以查看环境变量</p></li><li><p>$0   $1  $2  $3 0 代表的是脚本的名字 1 2 3 分别代表说输入到命令行 的参数名</p></li><li><p>read  read varg 命令可以读取对应的的和终端输入并存入对应的变量中</p></li><li><p>关系比较符号 -eq -ne -lt -gt -ge -le ! &amp;&amp;</p></li><li><p>算术表达式 $(()) $ 这个就是用来引用值的</p></li><li><p>条件控制语句 就是使用 if [] ; then  注意一定还有一个 fi 作为结束 </p></li><li><p>for循环 注意一样的 还是有一个done作为一个结束的标志</p></li><li><p>#!&#x2F;bin&#x2F;bash</p><p>for i in {1..5}; do<br>if [ $i -eq 3 ]; then<br>    continue<br>fi<br>echo “循环次数：$i”<br>if [ $i -eq 4 ]; then<br>    break<br>fi<br>done</p></li><li><p>还有一个函数 有需求再去学习</p></li></ul><h2 id="感觉还是应该需求为导向-想要是实现什么功能的-时候就去看一看"><a href="#感觉还是应该需求为导向-想要是实现什么功能的-时候就去看一看" class="headerlink" title="感觉还是应该需求为导向 想要是实现什么功能的 时候就去看一看"></a><font color=salmon>感觉还是应该需求为导向 想要是实现什么功能的 时候就去看一看</font></h2><ul><li>awk 似乎是一个有趣的指令  来处理文本呢 男泵  seq 还有一个seq 流处理指令以及一些简单的文本处理指令   &amp;&gt; </li><li>著名的这个垃圾桶位置 &#x2F;dev&#x2F;null</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Bash-脚本总结&quot;&gt;&lt;a href=&quot;#Bash-脚本总结&quot; class=&quot;headerlink&quot; title=&quot;Bash 脚本总结&quot;&gt;&lt;/a&gt;Bash 脚本总结&lt;/h1&gt;&lt;h2 id=&quot;命令总结&quot;&gt;&lt;a href=&quot;#命令总结&quot; class=&quot;headerli</summary>
      
    
    
    
    
    <category term="Linux" scheme="https://spikeihg.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Fine_Tuning</title>
    <link href="https://spikeihg.github.io/2024/03/25/Fine-Tuning/"/>
    <id>https://spikeihg.github.io/2024/03/25/Fine-Tuning/</id>
    <published>2024-03-25T08:32:02.000Z</published>
    <updated>2024-04-11T12:43:48.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Fine-tuning-practice"><a href="#Fine-tuning-practice" class="headerlink" title="Fine_tuning practice"></a>Fine_tuning practice</h1><ul><li><p>找点事做，算是。哎，道路曲折。</p></li><li><p><font color=salmon>主机huggingface password iW7GHBpZ</font></p></li><li><p>现在基本上是使用一个预训练模型加上一个prompt 或者是fine tuning</p></li><li><p>transformer 一个生态系列 很多的库 Lora </p></li><li><p>使用的是miniconda</p></li><li><p>后运行job的指令 第一个就是末尾加&amp; 第二个就是ctrl+z stop他和current job然后使用一个bg 指令 jobs 显示当前进行i的job 然后还有 fg指令前台化</p></li><li><p>pip 或者整个虚拟环境的 一个原理就是每个包所具有的依赖都是存在于对应的包里面 然后下载的时候要确定对应的 解释器</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/pipi.png"                                     ></p></li><li><p>离线下载就是下载好了文件 然后scp然后AutoModel.from_pretrained()里面写文件夹的名字</p></li><li><p>三个数据集合 </p></li><li><p>training set 用于训练更新参数的</p></li><li><p>test set 用于最终评价的 不用于 改变参数</p></li><li><p>validation set 可以理解为中间的一个模拟测试 来反馈我们的超参数设置情况。</p></li><li><p>数据集的加载 使用的是dataset 这个包</p></li><li><p>数据集的划分 train_test_split 按照比例划分</p></li><li><p>选取 用 select 过滤用 filter加lamda 函数</p></li><li><p>预处理 使用map 方法 结合tokenizer </p></li><li><p>存放就是 load and save</p></li><li><p>加载本地文件 ——  可以直接加载整个文件夹 对</p></li><li><p>很多方法 殊途同归 然后可以自己写脚本来</p></li><li><p>tokenizer 中 inputs_id 就是转换后的唯一id attentino_mask 就是判断改数字是否是填充padding type——id就是确定这个句子是第几个。</p></li></ul><h2 id="显存优化"><a href="#显存优化" class="headerlink" title="显存优化"></a>显存优化</h2><h2 id="CSAPP-LAB-3-attack-lab"><a href="#CSAPP-LAB-3-attack-lab" class="headerlink" title="CSAPP LAB 3 attack lab"></a>CSAPP LAB 3 attack lab</h2><ul><li><p>几个编译选项的意思 可以使用 man gcc进行查找 &#x2F;keyword n 就是到下一个匹配项， N上一个匹配项</p></li><li><p>top 查看机器进程情况的时候 ， 可以使用 c 查看详细命令</p></li><li><p>占个坑 想学一学基本的shell 脚本<a class="link"   href="https://blog.csdn.net/qq_29864051/article/details/132650005" > <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>解决requirement already satisfied 的方法就是使用target指定包的存放地址 一般再这个lib&#x2F;python3.x&#x2F;site-package里面</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/eason.png"                                     ></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Fine-tuning-practice&quot;&gt;&lt;a href=&quot;#Fine-tuning-practice&quot; class=&quot;headerlink&quot; title=&quot;Fine_tuning practice&quot;&gt;&lt;/a&gt;Fine_tuning practice&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
    <category term="DL" scheme="https://spikeihg.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>GoMars</title>
    <link href="https://spikeihg.github.io/2024/03/05/GoMars/"/>
    <id>https://spikeihg.github.io/2024/03/05/GoMars/</id>
    <published>2024-03-05T00:37:12.000Z</published>
    <updated>2024-03-14T15:31:08.480Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GoMars"><a href="#GoMars" class="headerlink" title="GoMars"></a>GoMars</h1><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><ul><li>fortran syntax __ % like dot . in c++</li><li>isend 必须使用wait 以为我们需要保证这个已经完成communicte isend + wait  &#x3D; send  sendres就是两个的简单集合</li><li>vscode 可以使用这个alt+left right 进行trace back and forth</li><li>几个问题 第一个 什么是incremental builds</li><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GoMars&quot;&gt;&lt;a href=&quot;#GoMars&quot; class=&quot;headerlink&quot; title=&quot;GoMars&quot;&gt;&lt;/a&gt;GoMars&lt;/h1&gt;&lt;h2 id=&quot;Prerequisite&quot;&gt;&lt;a href=&quot;#Prerequisite&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="HPC" scheme="https://spikeihg.github.io/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>6.S081</title>
    <link href="https://spikeihg.github.io/2024/02/29/6-S081/"/>
    <id>https://spikeihg.github.io/2024/02/29/6-S081/</id>
    <published>2024-02-29T11:32:09.000Z</published>
    <updated>2024-04-09T03:49:10.801Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Operating-System"><a href="#Operating-System" class="headerlink" title="Operating System"></a>Operating System</h1><ul><li><p>退出qemu ctrl x a . 注意还有一个点号！！！</p></li><li><p>在google中使用ctrl shift q 使用google scholar!。</p></li><li><p><font color=lavender>需要永久改变vim的配置 可以修改 ~&#x2F;.vimrc 文件</font></p></li><li><p><font color=orange>Spack 是一个包管理系统 主要用于很多科学专业的计算使用  $foo is a variable while $(foo) is a result of a command 变量名可以用{} 包围着 <br> 脚本中常见的if 中对文件使用的短句命令 -d foo 如果foo 存在且为目录 -e 如果存在 -f 如果存在且为普通文件  pushd 指令就是在程序执行的时候对所在的目录位置存在一个栈 ，pushd 后这个目录就位于top位置同时也变成现在的工作目录 功能和cd 类似的 但是在连续多步骤跳跃的时候很有用，这是cd可能需要多次连续使用</font></p></li><li><p>还有一招可以切换桌面 就是win 然后触摸板使用四个指同时滑动。</p></li><li><ul><li>一般來説，我们的这个page有一个高为的全为0  的 guard page in threre  这个保护页没有相应的PTE 所以一旦超过了这个范围就会导致一个page fault<ul><li>所以实际上这个映射是很复杂的 可以是多对多 也可以是多对一 一对多 所有的这些编程技巧都是通OS 实现的</li></ul></li><li>np 代表这个进程的数量</li><li>id 是在process mod 里面确定</li><li>ids ide  是在 mesh里初始化的 大小似乎就是点的个数</li></ul></li><li><p>学习了几个奇淫巧计 </p></li><li><p>tmux 打开窗口 然后可以Ctrl+b c create the new windows and the ctrl+b % to split the window 拆分窗口 ctrl+b “ 水平拆分 ctrl + b o 来回切换 注意这里的一个使用方法就是先点击 ctrl + b 然后在点击其余的按键</p></li><li><p><a class="link"   href="https://stackoverflow.com/questions/10534798/debugging-user-code-on-xv6-with-gdb" >这是xv6 gdb tutorial <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>首先咱们使用的命令是 gdb-multiarch 注意中间的横线一定不能少</p></li><li><p>然后就是上面的网址里写的</p></li><li><p>回顾一下gdb的基本用法</p></li><li><p>tmux 的一些用法 prefix &amp; 关闭当前的windows </p></li><li><p>tmux attach 进入最近使用的session  </p></li><li><p>prefix 数字 进入对应的windows</p></li><li><p>find -name {} 方便查找文件</p></li><li><p>总结而言 一般我们先在一个windows运行 gdb qemu 然后一个新的window split 分为 gdb 和源码</p></li><li><p>prefix o 是在不同 pane 间移动</p></li><li><p>注意启动gdb 后需要使用 target remote localhost:26000来关联上</p></li><li><p>在 gdb 中我们使用 layout split 来同时显示源码和汇编</p></li><li><p>在vim 里面也可以拆分 : split 然后 ctrl + w+w 连按两次 w 来移动窗口 垂直是vsplit  在内部打开另一个文件就是使用 open + filename 注意文件路径</p></li><li><p>qemu 可以使用这个监视模式 ctlr a c</p></li></ul><h2 id="Trap"><a href="#Trap" class="headerlink" title="Trap"></a>Trap</h2><ul><li>为了在用户代码与内核间思华切换 隔离安全性。</li><li>mode 管理员模式其实只是能够读写一些控制控制急寄存器 然后是以哦个PTE中没有PTE_U 的模式位的页表条目</li><li>ecall 开始经过trampoline 的两个中间函数 然后使用sys_call 指c函数看跳转表 感觉中间的两个函数是进行一个硬件准备 然后都是 使用汇编语言写的 感觉这些中间工作差不多就是一些内存上下文保存  还有虚拟地址映射的切换之类的</li><li>系统调用指令 ecall 与普通的call 指令相似 传入的调用参数使用通用寄存器 riscv 就是a0 a1 a2 之类的 </li><li>两个额外的标记位 一个是a 表似乎是否是被读过 一个是d 表示是否是被写过 也许后面驱逐页的时候就需要</li><li>感觉目前接触的最终要的一个感觉就是riscv 尽可能的简单 从而能够给予操作系统设计者最大的自由发挥的空间 ecall实际上只完成三件事 一件事是保存pc 第二就是读取trampoline 到 pc 第三件事就是变换mode 从user到管理员模式 同时很多指令行为的考虑没有一个通用的模式 例如保存在tramframe只是因为假设可能存在没有普通函数栈的情况</li><li>所以我们抽象地来看其实内核模式与用户模式就是简单地在于虚拟空间以及内部使用的指令的不同 还有就是具体的位不同 通过几个底层的东西我们就可以抽象出如此美妙的东西 ！ </li><li>trampoline在用户和内核的虚拟页表都是相同的映射所以可以实现无缝切换</li><li>数据库 本身也是一种建立在集合运算上 的一种简洁的抽象。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Operating-System&quot;&gt;&lt;a href=&quot;#Operating-System&quot; class=&quot;headerlink&quot; title=&quot;Operating System&quot;&gt;&lt;/a&gt;Operating System&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;退出qem</summary>
      
    
    
    
    
    <category term="OS" scheme="https://spikeihg.github.io/tags/OS/"/>
    
  </entry>
  
  <entry>
    <title>New_Life</title>
    <link href="https://spikeihg.github.io/2024/01/01/New-Life/"/>
    <id>https://spikeihg.github.io/2024/01/01/New-Life/</id>
    <published>2024-01-01T04:35:55.000Z</published>
    <updated>2024-01-01T05:04:28.067Z</updated>
    
    <content type="html"><![CDATA[<p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/cowboy.gif"                                     ></p><h1 id="I-can-fix-that"><a href="#I-can-fix-that" class="headerlink" title="I can fix that"></a>I can fix that</h1><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/ww.jpg"                                     ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img  
                     lazyload
                     src=&quot;/images/loading.svg&quot;
                     data-src=&quot;/../images/cowboy.gif&quot;</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>环形物语</title>
    <link href="https://spikeihg.github.io/2023/12/19/%E7%8E%AF%E5%BD%A2%E7%89%A9%E8%AF%AD/"/>
    <id>https://spikeihg.github.io/2023/12/19/%E7%8E%AF%E5%BD%A2%E7%89%A9%E8%AF%AD/</id>
    <published>2023-12-19T05:22:15.000Z</published>
    <updated>2024-02-28T08:31:33.190Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tales-from-the-Loop"><a href="#Tales-from-the-Loop" class="headerlink" title="Tales from  the Loop"></a><font color=deepskyblue>Tales from  the Loop</font></h1><h1 id="环形物语"><a href="#环形物语" class="headerlink" title="环形物语 "></a><font color=pink>环形物语</font> <br></h1><h2 id="环就在你心中"><a href="#环就在你心中" class="headerlink" title="环就在你心中"></a><font color=lightgreen>环就在你心中</font></h2><ul><li>一个假期，很多东西都忘记了，尤其是一些快捷键和命令之类的，以及blog的使用，这里就记下一些entries,在以后出现这种情况的时候可以快速复原。<ul><li>Blog : 首先powershell到博客文件夹， win x the i;  page up to find the cd command ; the hexo new to create  a md file with the title you give,hexo s just to start the local server, hexo g to generate your commits, hexo d to deploy you blog;</li><li>Google: 使用的一些东西 ctrl + tab tab之间进行切换  ctrl+ 数字 to jump to the given tab, ctrl + t create new tab, ctrl+ n create new window, alt+v translate the chosen word; </li><li>Windows: alt+tab switch the tasks; ctrl + win switch the background;</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Tales-from-the-Loop&quot;&gt;&lt;a href=&quot;#Tales-from-the-Loop&quot; class=&quot;headerlink&quot; title=&quot;Tales from  the Loop&quot;&gt;&lt;/a&gt;&lt;font color=deepskyblue&gt;Tale</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>APSP</title>
    <link href="https://spikeihg.github.io/2023/12/05/APSP/"/>
    <id>https://spikeihg.github.io/2023/12/05/APSP/</id>
    <published>2023-12-05T08:33:50.000Z</published>
    <updated>2023-12-13T08:36:45.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="APSP"><a href="#APSP" class="headerlink" title="APSP"></a><font color=deepskyblue>APSP</font></h1><ul><li><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a><font color=pink>Prerequisites</font></h2><ul><li><font color=yellow><a class="link"   href="https://www.bilibili.com/video/BV1YW411h7Pk/?spm_id_from=333.337.search-card.all.click&vd_source=c8c7f6103570a31005f12d5a33a60b47" >morphine <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 先来一剂Green Day 牌吗啡！<br>在结尾处，准备总结一下实验的一些基本环境配置和有用链接以及FAQ留个之后看到这个网页以及遇到问题的人，所以如果是有问题需要解决可以先看看结尾。保证可以复现乐 : ) </font></li><li><a class="link"   href="https://cmake.org/cmake/help/latest/guide/tutorial/A%20Basic%20Starting%20Point.html" >Cmake <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><font color=yellow>熟悉cmake 的常见函数或命令以及了解下makefile 的原理 有点搞笑 我们的这个prefix 的路径似乎不能使用 ~ 而必须使用绝对路径 </font></li><li><a href="/doc/%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E5%AF%BC%E8%AE%BA.pdf">OpenMP</a><font color=yellow>OpenMP</font><ul><li><font color=lightgreen>首先，我们知道对于floyd算法的三层for循环中 ,最外层k 是无法并行化的，因为每一个k的路径比较依赖于已有的路径，需要顺序进行更新，但是内层i,j循环在src dest一定时，k 可以按照任何顺序选取所以是可并行的。因此private(i,j).</font></li><li><font color=lightgreen>OpenMP 原理 就是启用多线程，具体运行的时候可以跑在多核上，并行运算。底层就是pthread 实现的</font></li><li><font color=lightgreen>平衡负载，我采用的是dynamic ，不过可以多试试看看，static guide 都可以尝试，默认的也可以，感觉每次迭代的计算量似乎是随机的但是总体是均匀的 感觉static 应该就可以，dynamic 还是用在计算量会增加的比较好</font></li><li><font color=lightgreen>线程数的设置，过多会增大合并开销，同时还存在内存分配问题，降低效率，数量过少会导致并行度不够，根据我的猜想，看CUDA简介的经验，使用一个和迭代数以及某些硬件属性数的倍数或者因子书。32 64 之类的。问题不大，实践是检验真理的唯一方式，多试试就可以了，试了再来补充。</font></li><li><font color=lightgreen>编译器优化猜想，首先就是编译器可能帮我做了loop unrolling ,估计是fully peel the loop有可能。 然后就是寻址方式，依照CSAPP 上的，对于一个定长二维数组，确定一些基指针，然后使用定长进行改变。减少访存。然后就是使用局部变量来保存一些结果减少内存访问。同时也可能使用了一些向量化操作(AVX指令等等）。常熟计算(constant propgation)编译时计算，也就是constexpr。将小函数进行内联，但是本例中似乎不存在。</font></li><li><font color=lightgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/omp.png"                                     ></font></li><li><font color=lightgreen>这个只是使用了最基本的for 内层循环并行，和负载动态分配</font></li><li><font color=lightgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/simd_omp.png"                                     ></font></li><li><font color=lightgreen>这个比较客观，在第一次基础上使用了omp的 simd优化最内层循环。4096的加速比接近baseline的100倍，当然我这里选择的是64线程数 使用128线程差距不大。不过我个人感觉没有找到更好的方法进行线程数的调参。不知道有没有除了顺序试错外更高效准确的判断方法，还待我考察，欢迎学长指教。</font></li></ul></li><li><font color=yellow><a class="link"   href="https://arxiv.org/pdf/1811.01201.pdf" >AVX参考 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>avx优化</font><ul><li><font color=lightgreen>使用的是AVX512，用一个 mask store 来实现比较运算。</font></li></ul></li></ul></li><li><p><font color=pink><a class="link"   href="https://www.jstage.jst.go.jp/article/transinf/E95.D/12/E95.D_2759/_pdf" >BFW <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>(算法优化——Blocked Floyd Algorithm</font></p><ul><li><font color=lightgreen>查找了一下资料，似乎这个可以提高数据的局部性，当然需要设置好所分的矩阵块的大小，使处理一个数据块的工作集内存大概等于 L2 cache .但是感觉实现这个算法本身加速比不是很明显，然后分块的话额外内存开销较多。（还有就是懒:) 所以就没有进行应用。</font></li></ul></li><li><p><font color=pink>Pthread接口实现多线程</font></p><ul><li><font color=lightgreen>可以，今天用pthread把内层两个循环并行处理了一下，加速比大概500倍。跑4096的图用了13s左右，我自己设置的线程数为<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/Re1.png"                                     >这个时候文件夹还没改名，当然但就算法运行时间大概11s左右</font></li><li><font color=lightgreen>使用局部变量优化了一下 大概8s 多一点<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/Re3.png"                                     ></font></li></ul></li><li><p><font color=pink>AVX512改写内层循环 速度大概2s提升 4倍</font></p><ul><li><font color=lightgreen>不过这里我有一个问题，就是理论上直接来看，应该会提升16倍，实际上只提升4倍左右，我自己猜测的原因是缓存问题，由于嵌套循环，存取的时候空间局部性不是很好，等后面有时间profile 一下。</font></li><li><font color=lightgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/unrool.png"                                     >循环展开64 后 线程数 90 接近突破2s</font></li><li><font color=lightgreen>记录下首次突破 2s 作了128的循环展开 然后将线程数调到了100<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/2s.png"                                     >后续测试感觉 64循环展开 和 128差不多了。感觉实际上我们的128循环展开可能效果还差一点，因为使每一个线程的工作负载变大了。</font></li><li><font color=lightgreen>感觉实在找不到什么可以优化的地方了（在我目前所学的知识范围内）<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/final.png"                                     >大概在1.85s左右 悲。有一个分块floyd算法，改善局部性，感觉就单独实现而言，确实可以通过减少工作集内存的范围来提高空间局部性。但是因为涉及到要重写pthread,感觉反而会增加线程创建的开销。因为需要不断迭代子方块，然后进行pthread_create和pthread_join。</font></li><li><font color=lightgreen>本来想学一学使用 vtune 来剖析一下，似乎集群没有装，然后数据scp 命令没有使用权限，所以没有profile很多对性能的猜测都是自己的直觉 乐！</font></li><li><font color=lightgreen>至于GPU的算法，看了一下，似乎没有看到集群有装CUDA，所以没打算写，看后面有没有时间参考下资料看一看。</font></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;APSP&quot;&gt;&lt;a href=&quot;#APSP&quot; class=&quot;headerlink&quot; title=&quot;APSP&quot;&gt;&lt;/a&gt;&lt;font color=deepskyblue&gt;APSP&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&quot;Prerequisites&quot;&gt;&lt;a</summary>
      
    
    
    
    
    <category term="hpc" scheme="https://spikeihg.github.io/tags/hpc/"/>
    
  </entry>
  
  <entry>
    <title>DL</title>
    <link href="https://spikeihg.github.io/2023/11/23/DL/"/>
    <id>https://spikeihg.github.io/2023/11/23/DL/</id>
    <published>2023-11-23T11:24:27.000Z</published>
    <updated>2023-12-05T15:35:17.243Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deep-learning-premier"><a href="#Deep-learning-premier" class="headerlink" title="Deep learning premier"></a><font color=pink>Deep learning premier</font></h2><p><font color =violet>志を受け継ぎ世界と戦う</font></p><ul><li><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a><font color=lightgreen>Transformer</font></h2><p>transformer是什么？变形金刚！！！ 好吧transformer 是用来解决seq2seq的一个模型。 seq2seq是一种模式，一些常见的情形：翻译，听译，语音辨识，听译 男泵 万恶之源 硬train一发 chatbot 之类的也是</p><p>BOS</p><ul><li><p>NLP QA模式 问题回答的一种模式</p></li><li><p>multi-label 就是每一个输入对象可能身上有多个标签</p></li><li><p>transformer 就是求解s2s的一个模型 encoder 和 decoder</p></li><li><p>residual connection 这个是与self attention 不一样的地方把input 和 output加起来</p></li><li><p>为什么能够应对seq2seq的情况</p><p>输入有两个部分 一部分来则于自己之前的输出</p><p>masked -attention 为什么需要</p><p>单纯看自己的输入作为输出的时候不可能停止下来，机，器，学。习。惯…… 首先我们需要准备以恶搞special toke as end(断) 所以原理就是让机器学习断，在该停止时候最大的概率输出end</p><p>上面是对于AT NAT是同时产生所有的输出 解决停止问题 有两个，第一个单独训练一个classifier来判断长度，方法二，传入很多begin token.忽略end 后的 输出 NAT 更好的并行化，可以控制长度 但是效果很难达到AT multi-modality</p><p>decoder 和 encoder的连接依靠一个叫做 cross-attention 的操作完成的就是 decoder 产生的向量q然后对每一个encoder的output k做kq，然后得到的新的v作为input丢到fc进行之后操作</p><p>train的时候采用分类的视角 计算向量之间的cross-entrophy 并且在训练情况下，我们给decoder的是正确的答案 就是一个监督 tearcher forcing </p><p>训练的一些 tips copy mechanism 例如chat-bot 将一些从来没有见过的词汇 直接进行复制 例如做摘要的时候 pointer network</p><p>TTS 语音合成 guide attention 就是规定 attention的顺序 有固定的过程 需要提前分享任务的特征monotonic attention</p><p>观察确定 我们的encoder 就是输出简单的数据 作为中间向量</p><p><font color=yellow>beam search</font> greedy path可能存在问题 局部最优不代表全局最优 类似于老师讲的王者问题 加入随机 decoder加一点 noise</p><p><font color=pink>BLEU score 用来检测inference 的指标但是训练的时候使用的是cross -entrophy 这两个没有关联的，没有相关性的blet score无法微分 一种策略使用 rl reinforcing learning reward与 agent来硬train一发</font></p><p><font color=cornsilk>在训练的时候加入一些错误的情况，scheduled sampling 也很直觉啊，面对错误的时候得到正确的这样学习应对错误的能力</font></p></li></ul></li><li><h2 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a><font color=deepskyblue>self-attention</font></h2><ul><li>q,k 矩阵学习 multi-head 就是分出多个 q 矩阵 然后得到bi,j 然后再处理成一个 positional embding , hand-crafted 目前的 目前是一个全新的领域 也可以学习。 self-attention 之间是没有位置关系的，然后q k 也不一定需要包含整个窗口</li></ul></li><li><h2 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a><font color=salmon>Batch normalization</font></h2><p>layer norm 是对同一个feature(sample)不同的dim 进行计算 mean和 deviation 而 batch norm 是对所有不同的feature 的同一个dim 进行处理</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/transf.png"                                     ></p><ul><li>position coding 位置资讯 </li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bert.png"                                     ></li></ul></li><li><p>实际设计中还可以在顺序上进行变换 上述大概就是encode 结构</p></li><li><p>decoder</p><ul><li>bos （begin special token）特殊的符号 标记开始</li><li>masked self- attention 和通常的attention 有一些区别</li><li>为什么 因为decoder是一个一个产生的 在产生前面的输出时，后面的右边的输出还没有产生 无法考虑</li><li>autoregressive</li><li>自己不断运作，输出再次作为输入然后一直运行  存在停止的问题 方法就是设置另一个token 有时候与begin其实是同一个 实际实现可能是一个one-hot 向量，end也是自己产生的男泵</li><li>NAT non-autoregressive</li></ul></li><li><h2 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a><font color=white>pytorch</font></h2><ul><li><p>总体流程就是，先定义我们的数据， dataset可以理解为存储我们的数据，feature 和 label ，然后dataloader 以我们想要的方式加载数据，自定义操作，包括batch ,还有 transform 等等。</p></li><li><p>然后就是定义 目标函数 loss ，以及一个优化器 optimizer ，有了之后定义 响应的train 和 test 函数 ，最后还有对模型的保存以及调用 和实际运用。</p></li><li><p>tensor (array)统一的却应用广泛的数据结构 ，就是一个高维数组，最外面的维度为dim&#x3D;0 注意顺序 然后基本上有外面可能用到的所有操作，只需要用时自己去找api 查资料就可以了。</p></li><li><p>dataloader &amp; dataset</p><ul><li>torch.utils.data.dataset torch.utils.data.dataloader 两个基本的提供的模块除此之外很多专门领域的domain-specified 的库都有自己的相关的。 注意一般来说,然后从我们的视角来看，就是一个数组 索引得到feature and label 同时得到。a,b&#x3D;dataset[index]大概这样，然后label 一般存在一个csv文件（逗号分隔文件里面）然后就自定义__init—— len getitem三个函数init 的时候传入的 是一个annotation_file 和 dir 两个都是字符串，然后两个transform 函数一个transform 一个 target_transform</li><li>pandas 是一个数据处理  比如读取csv_file 之类 os 就是处理字符串之类的</li><li>dataloader 的每一次迭代返回两个tensor 对应feature和label</li><li>transform modify featu target_transform modify label<ul><li>常见的totensor 就是实现normalize 并且使得元素在0-1 之间。 vector lambda就是自定义操作</li></ul></li></ul></li><li><p>build the network</p><ul><li>torch.nn 所有存在的neuralnet的父类。 module其实就可以理解为layer 一个神经网络就可以理解为 a module itself that consists of other modules(layers) from torch import nn</li><li>第一步先check一下可不可以用硬件加速</li><li>然后继承 nn.Module 然后定义两个东西 init 和 forward方法</li><li>我们来break down and see every module拆解看看每一层<ul><li>flatten 在图形里面用来将一个image 矩阵转换为一个一维数组</li><li>linear 层就是进行一个线性转换 我们需要预先输入一个weigth 和 bias参数 然后对feature 进行变换</li><li><font color=pink>这里一个额外的补充知识点 就是call 特殊方法可以实现将类作为i函数调用 这就是为什么我们可以调用model，将数据作为参数输入进去 linear其实就是继承了nn.Module</font></li><li>妈的，我逐渐反应过来了，linear层里面的bias ,weight都是默认输出话，随机的其实，因为这个是我们要学习的参数，所以随机初始化就可以了。不需要什么预置输入</li><li>这里提前所以个东西 Relu 可以解决sigmoid 和tanh的过饱和问题缓解过拟合问题 也是目前默认激活函数</li><li>flatten默认改变的是最里面的层 没有改变channel 和batchsize</li><li>nn.sequential 就是一个layer的顺序容器 其实经过一个sequential 我们就可以发现输出了y 相当于fully connected network 啊男泵</li><li>nn.softmax 如果分类还需要过一层 注意通常要指定softmax操作的维度</li><li>parameter() named_parameters()这两个方法可以让神经网络追踪传入的参数哦从而进行学习</li><li>终于逐渐理解了</li></ul></li></ul></li><li><p>automatic differentiation</p><ul><li><p>关于tensor 的矩阵乘法 要了解到tensor最后面的参数一定是列向量 这个有点差异</p></li><li><p>parameter 就是我们需要进行optimize的 weight bias 都是。对于这些参数，我们需要优化，所以我们需要预先设定require_gradient ,默认是false</p><ul><li><p>然后调用Function 类，这个在每一个tensort 的 grad.fn 属性里存这一个指针，表示该tensor所使用的反向传播计算时的函数</p></li><li><p>在tensor的grad 里面维护的是计算得到的梯度值</p></li><li><p><font color=orange>所以现在我们可以这样理解forward就是利用model来进行计算，backword就是train，所以后面我们训练完成的时候，需要使用 with torch.no_grad()的方法</font></p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/DAG.png"                                     ></p></li></ul><p>  现在来审视这个 DAG leaves is input tensor root is output tensor and the node is function !!!!!</p><ul><li>然后实际的是实现中我们使用了雅各布矩阵直接做矩阵乘法！！！</li></ul></li></ul></li><li><p><font color=violet>Optimizing model</font></p><ul><li>在上面的过程我们可以明白了，forward其实就是调用预测时候的算法，所以实际上就是直接调用我们的init 里面的层 然后返回就可以了</li><li><font color=cyan>在这里我们区分一下parameter ,就是在里面起作用的东西，hyperparameter 更像是一些设置配置参数 例如 learning rate ,batch size ,number of epochs</font><ul><li>loss function 。 对于回归问题我们通常使用的是MSELoss, 对于classification 我们通常使用的 是 negative  log likelihood NLLLoss 然后nn.crossentropyloss 就是 logsoftmax和NLLLoss的结合</li><li>我们前面学的adam rmsprop sgd 其实就是不同的optimizer 也就是minimize的时候使用的算法</li><li>实际运行的时候有三个步骤<ul><li>第一步就是先显式置零，同时自动累计grad,以免没重复加</li><li>第二步就是运行反向传播算法</li><li>第三步 调用.step()方法进行更新</li></ul></li><li>最后就是进行实现两个loop 一个train loop 一个 test loop</li></ul></li><li><font color=pink>Save and load the model</font><ul><li>组后要进行保存和加载的化调用响应的方法即可 ，然后注意两个东西就是.eavl() 模式的切换，记住这个是避免dropout和batch normalization 的一个东西</li></ul></li></ul></li><li><h2 id="computational-graph"><a href="#computational-graph" class="headerlink" title="computational graph"></a><font color=salmon>computational graph</font></h2><ul><li><p>反向传播</p><p>前向传播和反向传播 forward and backward torch的抽象 forward 可以理解为求值 从input 到output 从leaf到root 反向传播就是求误差，根据链式法则求取偏导然后更新参数进行optimize。</p></li><li><p>torch的实际实现过程</p><p>根据我们对neuralnet 的定义  一个sequential 里面有很多layer，然后forward就是调用这个module ,在进行forward pass的时候会进行两件事情，第一件事就是按照要求计算得到output tensor第二件事就是建立其DAG，每个结点就是对应的求导函数，在后续的backward pass中调用进行求导</p><p> what you run is what you differentiate.</p><ul><li>tensor 需要保存，用来求导</li><li>对于数学上不可求导的函数或者是未定义情况有指定的数学方式来处理</li><li>先要阻止gradient 可以更改text manager mode 以及.eval(),同时要想控制子图subgraph的性质 可以对单个tensor 调用 require_grad</li><li>torch.nn 中parameter默认要求导，中间变量都会求导</li><li>现在终于懂了</li><li>lossfunction 进行 backward()相当宇求梯度然后得到grad值，optimizer 则是根据不同的类型操作 grad 进行更新所有  parameter  .step()就是进行一步 。 .zero_grad()就是重置tensor</li></ul></li><li><p>梯度消失和爆炸 的原因</p><p>我们在计算梯度的时候采用的是反向传播求取雅各布矩阵，由于很多hidden layer都会采用一些激活函数对于sigmoid而言，其倒数最大值为0.25 如果乘以w 后得到的积仍然小于1 如果很多小于1累计起来可能造成靠近输入层的参数更新极为缓慢，当然输出层的影响较小，爆炸则是产生NaN 或者不稳定。</p></li></ul></li></ul></li><li><h2 id="transformer-is-all-you-need"><a href="#transformer-is-all-you-need" class="headerlink" title="transformer is all you need"></a><font color=deepskyblue>transformer is all you need</font></h2><ul><li><font color=yellow>正向的 rnn 。 双向的rnn 可以正反同时读取我们的，</font></li><li>LSTM 关键四个们 标量也是用来学习的 每个单元可以用来替代之前的neuron 然后只是输入需要四倍，因为三个们和一个输入 。使用的是sigmoid 函数表示打开的程度。实际的模型还会在输入中参考中间步骤的输入</li><li>BPTT learning train  的 方法。考虑时间的关系。</li><li>train的问题 error surface存在很平坦到很陡峭的分界限，所以存在参数的抖动。 clipping 解决方案，就是为gradient 做一个上界，如果超过就直接等于。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Deep-learning-premier&quot;&gt;&lt;a href=&quot;#Deep-learning-premier&quot; class=&quot;headerlink&quot; title=&quot;Deep learning premier&quot;&gt;&lt;/a&gt;&lt;font color=pink&gt;Deep l</summary>
      
    
    
    
    
    <category term="Deep Learning" scheme="https://spikeihg.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Winter</title>
    <link href="https://spikeihg.github.io/2023/11/21/Winter/"/>
    <id>https://spikeihg.github.io/2023/11/21/Winter/</id>
    <published>2023-11-21T08:00:38.000Z</published>
    <updated>2023-11-22T08:53:33.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Winter"><a href="#Winter" class="headerlink" title="Winter"></a><font color=violet>Winter</font></h2><blockquote><p>我是未来世界唯一的程序员。这是一条来自未来的讯息。在我的时代，支配世界的算法被我洞晓。过去的人啦，想要改变未来吗？去探索吧，这个世界背后运行的真理！—— 2079 . 12 . 24 </p></blockquote><p><strong><font color=deepskyblue>世界の仕組みについての真実を理解しました</font></strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Winter&quot;&gt;&lt;a href=&quot;#Winter&quot; class=&quot;headerlink&quot; title=&quot;Winter&quot;&gt;&lt;/a&gt;&lt;font color=violet&gt;Winter&lt;/font&gt;&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;我是未来世界唯一的程序员。这是</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>Algorithm</title>
    <link href="https://spikeihg.github.io/2023/11/02/Algorithm/"/>
    <id>https://spikeihg.github.io/2023/11/02/Algorithm/</id>
    <published>2023-11-02T02:11:09.000Z</published>
    <updated>2023-11-21T06:38:30.103Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a><font color=yellow>Algorithm</font></h1><ul><li><strong><font color=seagreen>最近点对问题</font></strong><ul><li><font color=pink>分治法求解 垂直线分割 实际使用时可以对应实际的问题进行优化 比如使用一个 strip  然后可以在递归的最底层加入一个全局的区间值的判断 双重循环 但是每一层都是O(n^1&#x2F;2)判断方法是否存在在特殊情况失效 例如几乎垂直或者水平的点对。所以可以单独的进行筛选 类似机器学习的算法的改进 多个维度通道 简化的一些思想 </font></li><li><font color=pink>对称性思想 每一个维度都是一样的</font></li><li><font color=pink>建表  对于频繁使用的信息进行见表索引检索哈希码 向量 量化处理</font></li><li><font color=pink>曼哈顿距离</font></li></ul></li><li><strong><font color=seagreen>概率分析与随机算法</font></strong><ul><li><font color=pink></font></li></ul></li><li><strong><font color=seagreen>正太分布问题思考</font></strong><ul><li><font color=pink>解决一种问题 就是关于某个中心点对称任何维度的可能性相同。 同时随着偏离中心概率减小 关键就在于与角度无关 非常的厉害</font></li></ul></li><li><strong><font color=seagreen>随机问题</font></strong><ul><li><font color=pink>随机生成一个数组的方法 一 可以为每个元素生成一个优先级 然后根据rank 进行排序</font></li></ul></li><li><strong><font color=seagreen>排队论</font></strong><ul><li><font color=pink>所有人排在一起，那个窗口空去哪个</font></li></ul></li><li><strong><font color=seagreen>同时获得最大值和最小值</font></strong><ul><li><font color=pink>同时维护两个值比较 n-1</font></li></ul></li><li><strong><font color=seagreen>活动安排问题</font></strong><ul><li><font color=pink>选择一个拥有最多活动的集合 集合的实践区间不存在重叠。使用贪婪算法，按照结束时间排序，然后每次都加入最早结束的时间 最核心的地方就是按结束时间思考，存在多解问题，原因在于开始阶段。</font></li></ul></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Algorithm&quot;&gt;&lt;a href=&quot;#Algorithm&quot; class=&quot;headerlink&quot; title=&quot;Algorithm&quot;&gt;&lt;/a&gt;&lt;font color=yellow&gt;Algorithm&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;f</summary>
      
    
    
    
    
    <category term="Algorithm" scheme="https://spikeihg.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>佩索呀</title>
    <link href="https://spikeihg.github.io/2023/10/29/%E4%BD%A9%E7%B4%A2%E5%91%80/"/>
    <id>https://spikeihg.github.io/2023/10/29/%E4%BD%A9%E7%B4%A2%E5%91%80/</id>
    <published>2023-10-29T08:04:14.000Z</published>
    <updated>2024-05-28T03:46:03.259Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Darling-I-‘m-getting-older-亲爱的，我原来也会变老"><a href="#Darling-I-‘m-getting-older-亲爱的，我原来也会变老" class="headerlink" title="Darling . I ‘m getting older. (亲爱的，我原来也会变老)"></a>Darling . I ‘m getting older. (亲爱的，我原来也会变老)</h3><ul><li><p>Everyday poetry.</p></li><li><p><a class="link"   href="https://poets.us20.list-manage.com/track/click?u=e329a0cb6f08842f08a05d822&id=50cda6fc43&e=dce612a19d" >Sippokni Sia <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a href="https://poets.us20.list-manage.com/track/click?u=e329a0cb6f08842f08a05d822&id=a7f92fa877&e=dce612a19d"><strong>Winnie Lewis Gravitt</strong></a></p><p>I am old, Sippokni sia. Before my eyes run many years, Like panting runners in a race. Like a weary runner, the years lag; Eyes grow dim, blind with wood smoke; A handkerchief binds my head, For I am old. Sippokni sia.Hands, once quick to weave and spin; Strong to fan the tanchi; Fingers patient to shape dirt bowls; Loving to sew hunting shirt; Now, like oak twigs twisted. I sit and rock my grandson. I am old. Sippokni sia.Feet swift as wind o’er young cane shoots; Like stirring leaves in ta falla dance; Slim like rabbits in leather shoes; Now moves like winter snows, Like melting snows on the Cavanaugh. In the door I sit, my feet in spring water. I am old. Sippokni sia.Black like crow’s feather, my hair. Long and straight like hanging rope; My people proud and young. Now like hickory ashes in my hair, Like ashes of old camp fire in rain. Much civilization bow my people; Sorrow, grief and trouble sit like blackbirds on fence. I am old. Sippokni sia hoke.</p><ul><li>我不知道人们所说的衰老是什么样子的，也许是透过镜子发现曾经的皮肤已经松弛下垂，也许当他们发现他们跑不过一个最小的孩子，但我感觉衰老是发生在一瞬间，仅仅一瞬间，就像过了一辈子，只是从前漫长的岁月从未被老去的忧伤笼罩，就像山顶的雾终于散去，时间失去了从前的神秘。</li><li>告别，寻找告别</li><li>春天总是一去不返</li><li>无法思考，无法摆脱，3点，神秘力量，是……</li></ul></li><li><p>Amy Lowell</p><ul><li><p>A enthralling person with all her persistence, intelligence , energy and fecundity.  “ The god made me  a business woman , but I made myself a poet .”</p></li><li><h1 id="The-Garden-by-Moonlight"><a href="#The-Garden-by-Moonlight" class="headerlink" title="The Garden by Moonlight"></a>The Garden by Moonlight</h1><p>BY <a class="link"   href="https://www.poetryfoundation.org/poets/amy-lowell" >AMY LOWELL <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>A black cat among roses,</p><p>Phlox, lilac-misted under a first-quarter moon,</p><p>The sweet smells of heliotrope and night-scented stock.</p><p>The garden is very still,  </p><p>It is dazed with moonlight,</p><p>Contented with perfume,</p><p>Dreaming the opium dreams of its folded poppies.</p><p>Firefly lights open and vanish  </p><p>High as the tip buds of the golden glow</p><p>Low as the sweet alyssum flowers at my feet.</p><p>Moon-shimmer on leaves and trellises,</p><p>Moon-spikes shafting through the snow ball bush.  </p><p>Only the little faces of the ladies’ delight are alert and staring,</p><p>Only the cat, padding between the roses,</p><p>Shakes a branch and breaks the chequered pattern</p><p>As water is broken by the falling of a leaf.</p><p>Then you come,</p><p>And you are quiet like the garden,</p><p>And white like the alyssum flowers,  </p><p>And beautiful as the silent sparks of the fireflies.</p><p>Ah, Beloved, do you see those orange lilies?</p><p>They knew my mother,</p><p>But who belonging to me will they know</p><p>When I am gone.</p></li></ul></li><li><p>A imagist poet as Pound .  Some intellectual game when you read some volume of her you just feel like .</p></li></ul><h1 id="Sylvia-Plath"><a href="#Sylvia-Plath" class="headerlink" title="Sylvia Plath"></a>Sylvia Plath</h1><ul><li>At her most articulate, meditating on the nature of poetic inspiration, [Plath] is a controlled voice for cynicism, plainly delineating the boundaries of hope and reality. At her brutal best—and Plath is a brutal poet—she taps a source of power that transforms her poetic voice into a raving avenger of womanhood and innocence.</li></ul><h2 id="When-I-am-dead-my-dearest"><a href="#When-I-am-dead-my-dearest" class="headerlink" title="When I am dead , my dearest."></a>When I am dead , my dearest.</h2><p>When I am dead, my dearest,</p><p>Sing no sad songs for me;</p><p>Plant thou no roses at my head,</p><p>Nor shady cypress tree:</p><p>Be the green grass above me</p><p>With showers and dewdrops wet;</p><p>And if thou wilt, remember,</p><p>And if thou wilt, forget.</p><p>I shall not see the shadows,</p><p>I shall not feel the rain;</p><p>I shall not hear the nightingale</p><p>Sing on, as if in pain:</p><p>And dreaming through the twilight</p><p>That doth not rise nor set,</p><p>Haply I may remember,</p><p>And haply may forget.</p><ul><li><p>徐志摩的翻译也很美，似乎也是他有首诗的foutainhead.</p></li><li><p>Not a red rose or a satin heart.</p></li></ul><h2 id="Valentine"><a href="#Valentine" class="headerlink" title="Valentine"></a>Valentine</h2><p>I give you an onion.<br>It is a moon wrapped in brown paper.<br>It promises light<br>like the careful undressing of love.</p><p>Here.<br>It will blind you with tears<br>like a lover.<br>It will make your reflection<br>a wobbling photo of grief.</p><p>I am trying to be truthful.</p><p>Not a cute card or a kissogram.</p><p>I give you an onion.<br>Its fierce kiss will stay on your lips,<br>possessive and faithful<br>as we are,<br>for as long as we are.</p><p>Take it.<br>Its platinum loops shrink to a wedding ring,<br>if you like.<br>Lethal.<br>Its scent will cling to your fingers,<br>cling to your knife.</p><h1 id="The-More-Loving-One"><a href="#The-More-Loving-One" class="headerlink" title="The More Loving One"></a>The More Loving One</h1><p>BY <a class="link"   href="https://www.poetryfoundation.org/poets/w-h-auden" >W. H. AUDEN <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>Looking up at the stars, I know quite well</p><p>That, for all they care, I can go to hell,</p><p>But on earth indifference is the least</p><p>We have to dread from man or beast.</p><p>How should we like it were stars to burn</p><p>With a passion for us we could not return?</p><p>If equally affection cannot be,</p><p>Let the more loving one be me.</p><p>Admirer as I think I am</p><p>Of stars that do not give a damn,</p><p>I cannot, now I see them, say</p><p>I missed one terribly all day.</p><p>Were all stars to disappear or die,</p><p>I should learn to look at an empty sky</p><p>And feel its total dark sublime</p><p>Though this might take me a little time.</p><p>September 1957</p><ul><li><p>今天读到的一首灵动的诗 引用的那篇文章也写得很好</p></li><li><p><font color=salmon>今天读完了那篇文章，很有趣，文笔也很优美，提到了很多有趣的idea，一个有趣的失恋者，哈哈，the more loving one .<a class="link"   href="https://www.poetryfoundation.org/articles/162120/but-there-are-other-geometries" > <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 在另一个世界 会有不一样的几何，有不一样的命运，也许我们就能够在理智与爱的坐标上相遇</font></p></li><li><h2 id="四月"><a href="#四月" class="headerlink" title="四月"></a>四月</h2><p>四月适合叶赛宁，四月不适合我。 （有点中二的发言  :)</p><p> Yesenin led an erratic, unconventional life that was punctuated by bouts of drunkenness and insanity. Before hanging himself in a Leningrad hotel, Yesenin slit his wrists, and, using his own blood, wrote a farewell poem.</p></li></ul><h1 id="Sergey-Esenin-I-do-not-lament-call-out-or-cry…"><a href="#Sergey-Esenin-I-do-not-lament-call-out-or-cry…" class="headerlink" title="Sergey Esenin I do not lament, call out, or cry…"></a>Sergey Esenin I do not lament, call out, or cry…</h1><p>I do not lament, call out, or cry.<br>All will pass like apple-blossom smoke.<br>Seized by golden glories of decay,<br>I shan’t see my youthful years come back.</p><p>You’ll no longer throb with equal passion,<br>Weary heart touched with a subtle chill;<br>Nor will you, green realm of birchen satin,<br>Lure me barefoot over dale and hill .</p><p>Vagrant spirit! Nowadays you scarcely<br>Stir these lips’ abiding secret blaze.<br>Ah, goodbye, my boyish effervescence,<br>Riot of eyes, and sentiments in spates!</p><p>I’ve become more frugal in my yearning.<br>My dear life, are you a dream where I<br>In the echoes of an early morning<br>Mount a rosy steed and gallop by?</p><p>We’re all mortal here without exception.<br>Maples shed their copper on the ground.<br>Blessed be, accept this benediction,<br>What has come to bloom and face its end.</p><ul><li>一切终将逝去，如苹果花丛中的薄雾，金色的树叶堆满心间，我已不再是青春少年。 打算试着翻译一些诗歌 : )</li></ul><h2 id="无论是什么-我都必须不断地进行探索存在"><a href="#无论是什么-我都必须不断地进行探索存在" class="headerlink" title="无论是什么 我都必须不断地进行探索存在"></a>无论是什么 我都必须不断地进行探索存在</h2><ul><li>环也许是存在的，但是并不妨碍探索本身。！！！！ 思考，想象力，好奇。</li></ul><h2 id="为什么Cowboy-Bebop-是伟大的-于我而言"><a href="#为什么Cowboy-Bebop-是伟大的-于我而言" class="headerlink" title="为什么Cowboy Bebop 是伟大的 于我而言"></a>为什么Cowboy Bebop 是伟大的 于我而言</h2><ul><li>因为有的只有最纯粹的故事。仅此而已，再无其他。</li></ul><h2 id="为何西语诗歌如此性感迷人"><a href="#为何西语诗歌如此性感迷人" class="headerlink" title="为何西语诗歌如此性感迷人"></a>为何西语诗歌如此性感迷人</h2><ul><li><p>主角不多，不是玫瑰也不是流云， 是你也是我，是某个男人，某个女人，是某段伤心的爱，是直面事实的激情，懊悔，是对生命最直接的参与。 是在诗歌中的自白，自白中的诗歌。 </p></li><li><p><font color=pink>努力使得自己的思想变得对人类有价值，就如同化石对于考古学家，即使不能留下完整的骨骼，也要努力使自己的屎变成化石。 goooood 今天想出来的笑话。 :)</font></p></li><li><p>上述观点有些武断，谨个人观点。————再读，看来十分武断，以后表述应当加上前缀 我觉得， 避免将事物描述成一种法则，给读者产生一种生硬的强加感。</p></li></ul><h2 id="村平"><a href="#村平" class="headerlink" title="村平"></a>村平</h2><ul><li><p>《电影的诞生》——村平的电影以及村平的电影观</p><p>为何叫村平，既有些像中文名，又有些像日文名，依次代表对我生命产生重要影响的中国与日本文化。 同时村 科研来自我的故乡， 这也许暗示了我所具有的电影审美倾向。   之后写的更多是单独的和本子记载的是互补的。</p></li><li><p>未来是否是最不需要关心的。 专注于当下的一种思想观念。是否是我电影观的核心。那么我应该怎样使用我的镜头，故事，人，调度， 取景，台词剧本， 声音 ，感官</p></li></ul><h2 id="lt-再见四月-gt-——-In-my-life-I-love-you-more-2024-4-30"><a href="#lt-再见四月-gt-——-In-my-life-I-love-you-more-2024-4-30" class="headerlink" title="&lt;再见四月&gt;—— In my life , I love you more - 2024.4.30"></a>&lt;再见四月&gt;—— In my life , I love you more - 2024.4.30</h2><h1 id="May你好五月"><a href="#May你好五月" class="headerlink" title="May你好五月"></a>May你好五月</h1><ul><li><p>五月关于三岛由纪夫，黑塞，列宁格勒牛仔和漫长的人生夏日</p></li><li><p>金阁寺  无时无刻拓展”美”的定义？ 或许美失去 褒义词的性质 应当成为一种更为广泛的性质    水手梦见陆地</p></li><li><p>西尔维娅 费尔南多 we make it simple 。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Darling-I-‘m-getting-older-亲爱的，我原来也会变老&quot;&gt;&lt;a href=&quot;#Darling-I-‘m-getting-older-亲爱的，我原来也会变老&quot; class=&quot;headerlink&quot; title=&quot;Darling . I ‘m g</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>CUDA</title>
    <link href="https://spikeihg.github.io/2023/10/24/CUDA/"/>
    <id>https://spikeihg.github.io/2023/10/24/CUDA/</id>
    <published>2023-10-24T06:20:19.000Z</published>
    <updated>2023-11-06T09:00:06.924Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CUDA-amp-Algorithm"><a href="#CUDA-amp-Algorithm" class="headerlink" title="CUDA&amp;Algorithm"></a><strong><font color=darkturquoise>CUDA&amp;Algorithm</font></strong></h1><ul><li><h2 id="Prelace"><a href="#Prelace" class="headerlink" title="Prelace"></a><font color=pink>Prelace</font></h2><p><strong><font color=mediumaquamarine>希望通过CUDA走进计算的前言，并且加深我对计算机体系结构的认知。同时从另一条路走进我们的machine learning 与 deep learning.同时也在这里写下一些算法的学习知识。</font></strong></p></li><li><h2 id="F-amp-Q"><a href="#F-amp-Q" class="headerlink" title="F&amp;Q"></a><font color=DarkSeagreen>F&amp;Q</font></h2><ul><li><p>内存布局具体硬件实现忘了，忘了栈实际上是在cache还是memory里</p></li><li><p>nvidia-smi 才是查看设别的指令 nvidia-smi -q 不错 </p></li><li><p>nvprof 已经弃用了 ncu（nsight-compute) 现在是profiler</p></li><li><p>nsight-compute 需要 全局安装 sudo apt install -y 选项<a class="link"   href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters#AllUsersTag" >issue <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p><font color=pink>driver 是一个硬件用来显示的   tookit就是一个集合 有些下载method可以同时下载适配的 driver 总结就是除了会安装还要学会卸载</font></p></li><li><p>These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case; it is important to not install the <code>cuda-drivers</code> packages within the WSL environment. <font color=red>乐死</font></p></li><li><p>Installation using RPM or Debian packages interfaces with your system’s package management system. When using RPM or Debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in &#x2F;var&#x2F;. Such a package only informs the package manager where to find the actual installation packages, but will not install them.</p><p>If the online network repository is enabled, RPM or Debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper.</p></li><li><p>安装是很复杂的 wsl有单独的教程 然后就是 有 post-installation mandatory actions !!</p></li><li><p>The <code>PATH</code> variable needs to include <code>export PATH=/usr/local/cuda-12./bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</code>. Nsight Compute has moved to <code>/opt/nvidia/nsight-compute/</code> only in rpm&#x2F;deb installation method. When using <code>.run</code> installer it is still located under <code>/usr/local/cuda-12.2/</code>.<font color=cyan>记住这个路径问题 在opt里面</font></p></li><li><p><font color=pink>妈妈我终于解决这个问题了 就是我在安装后没有设置环境变量 啊啊啊啊啊啊 男泵</font></p></li><li><p><a class="link"   href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions" >今后还可能出现的 问题 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>开始在vscode 里面进行配置 nsight debug</p></li><li><p>ctrl + space </p></li><li><p>命令面板很好用目前看来 ctrl shift + p</p><h2 id="我修改了提示-ctrl-t-s-好有用啊-还有就是控制面板太好用了有很多提示键-然后-task-也可以在里面选择生成"><a href="#我修改了提示-ctrl-t-s-好有用啊-还有就是控制面板太好用了有很多提示键-然后-task-也可以在里面选择生成" class="headerlink" title="我修改了提示 ctrl + t + s 好有用啊 还有就是控制面板太好用了有很多提示键 然后 task 也可以在里面选择生成"></a>我修改了提示 ctrl + t + s 好有用啊 还有就是控制面板太好用了有很多提示键 然后 task 也可以在里面选择生成</h2><ul><li><strong>[launch attach&amp; launch.json entry](<a class="link"   href="https://code.visualstudio.com/docs/editor/debugging" >Debugging in Visual Studio Code <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>)</strong></li></ul></li><li><p>dropdown configuration 就是下落的可以滑动的竖直设置栏</p></li><li><h3 id="预定义变量"><a href="#预定义变量" class="headerlink" title="预定义变量"></a><a class="link"   href="https://code.visualstudio.com/docs/editor/variables-reference" >预定义变量 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3></li><li><h3 id="最新发现-ctrl-alt-n-code-runner-似乎无所不能-但是不能调试只是"><a href="#最新发现-ctrl-alt-n-code-runner-似乎无所不能-但是不能调试只是" class="headerlink" title="最新发现 ctrl alt n code runner 似乎无所不能 但是不能调试只是"></a>最新发现 ctrl alt n code runner 似乎无所不能 但是不能调试只是</h3></li><li><p>program 就是 要debug的文件</p><ul><li><p><font color=gold>注意这个launch 是调试 要先生成可执行文件 也就是task 先配置的是task 然后是 launch.json</font></p></li><li><p><font color=green>重点出现了 发现可能的解决方案 就是prelaunch task 原来之前的是task 在debug后运行或者至少同时</font></p></li><li><p><font color=yellow>原来c_cpp_pr 是C++插件的配置文件不会影响</font></p></li></ul></li></ul></li><li><h2 id="Heterogeneous-Computing"><a href="#Heterogeneous-Computing" class="headerlink" title="Heterogeneous Computing"></a><strong><font color=tan>Heterogeneous Computing</font></strong></h2><ul><li><p><font color=teal>host指cpu，host codes run in CPU ,CPU code is responsible for managing the code and environment and device code running in GPUs.</font></p></li><li><p><font color=cornsilk>common GPU architectur GeForce Tesla and Fermi in Tesla  Tesla professional hpc. GeForce consumer GPUs</font></p></li><li><p><font color =cornsilk>two metrics to discribe the GPU compute capability .the core no. and the memory</font></p></li><li><p><font color =cornsilk>互补的 CPU 逻辑复杂 擅长分支预测控制流切换 GPU 擅长大量数据 简单控制 并行计算 Threads of CPU are heavyweighted 上下文切换开销大。 GPU就是相对轻量级 的</font></p></li><li><p><font color =cornsilk>CUDA driver API and CUDA runtime API 我们一般使用 runtime API cuda codes 包含两个部分 一个是host code 另一个是device code </font></p></li><li><p><font color =cornsilk>kernels 就是device code 里的并行函数由 nvcc 编译 nvcc 会区分host code and device code 然后就是完全分开执行 good<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/cucode.png"                                     ></font></p></li><li><p><font color =cornsilk>hello from GPU GPU program structure 5 steps 分配显存 加载数据 invoke kernel 返回数据 销毁显存</font></p></li><li><p><font color =cornsilk>locality temporal locality and spatial locality 这是编写cpu程序注意的 而GPU 将存储架构和线程结构都展示给程序员</font></p></li><li><p><font color =cornsilk>three key abstractions 三个关键抽象对于GPU 1. hierarchy of thread groups 2. hierarchy of memory 3. barrier synchronization </font></p></li><li><p><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/nvcc.png"                                     >nvcc 支持的文件后缀 .c 普通的是可以编译的</font></p></li><li><p><font color=cornsilk>programming model 其实就是 抽象 通过使用compiler and library &amp; OS 对hardware architecture 的抽象   scalability 可拓展性</font></p></li><li><p><font color=cornsilk>Host CPU and its memory ; Device : GPUs and its memory eg h_ for host m; d_ for device space</font></p></li><li><p><font color=cornsilk>Kernel 即跑在GPU 的codes我们可以看作是一个普通函数 实际上 GPU将其分配在多个线程上同时运行 ，当kernel运行后控制会立马交还给cpu以开始其他工作 异步工作。serial code 串行码 complemented by parallel code</font></p><div class="highlight-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Memory management */</span></span><br><span class="line"><span class="built_in">malloc</span>(); -&gt; <span class="built_in">cudaMalloc</span>();</span><br><span class="line"><span class="built_in">memcpy</span>(); -&gt; <span class="built_in">cudaMemcpy</span>();</span><br><span class="line"><span class="built_in">cudaMemset</span>();</span><br><span class="line"><span class="built_in">cudaFree</span>();<span class="comment">// all in device memory which is seperated from host memery!!</span></span><br><span class="line"><span class="comment">// the signature of the func</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="type">void</span>**devPtr,<span class="type">size_t</span> size)</span></span>;<span class="comment">// the pointer is returned in the devPtr</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span> <span class="params">( <span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count,cudaMemcpyKind kind )</span> <span class="comment">// the kind takes one of the following types cudaMemcpyHostToHost --HostToDevice --Dev2Dev D2H this func 是同步的 host 会阻塞知道完成</span></span></span><br><span class="line"><span class="function"><span class="comment">// cudaError_t enumerated type include cudaSuccess .eg </span></span></span><br><span class="line"><span class="function">    <span class="type">char</span>*<span class="title">cudaGetErrorString</span><span class="params">(cudaError_t error)</span></span>;</span><br></pre></td></tr></table></figure></div></li><li><p><font color=cornsilk>Global memory and shared memory in device just like memory and cache in CPU 前面的分配的函数都是在global memory 里面就像我们的malloc一样 目前我们所知道的由于这样内存分类 对应的指针是不能类型转换的，只能用cudaMemcpy来完成转移 后期由unified memory</font></p></li><li><p><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bg.png"                                     >通常而言 grid是二维 block是三维  blockDim gridDim dim3 type 没有初始化的filed自动为1</font></p></li><li><p><font color=cornsilk>P88 warp执行模型 32 个thread 硬件层面都会变成 warp 然后分散在SM上执行 之所以可以是主要是内存资源决定的 32 cores是共享的 前面说到多个warp scheduler 调度将warp的一个指令放到16core的一个组合上运行 其中register file 决定了warp 数量 shared memory 决定sm的block数量 然后warp切换上下文没有开销 都是data分割的 <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/limiter.png"                                     ></font></p></li><li><p><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/warp.png"                                     >warp 注意4 这个数字是由架构中每个SM的scheduler决定的 stall warp eligible warp 因此我们要最大化active warps</font></p></li><li><p><strong><font color=mediumseagreen>divergence 会执行所有分支 我们将分支按warp 划分</font></strong></p></li><li><p><strong><font color=pink>latency hiding <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/latency.png"                                     >类似于CPU的调度 latency就是时间 一般用clock cycle 计算大小</font></strong></p><ul><li><font color=cornsilk>P91 有趣的排队理论 就是需要同时并行的操作数&#x3D;延迟（cycle）*预期throughput throughput 与 bandwidth used interchangably bandwidth refer to as peak data transfer per time unit throughput refer to as any operations       rate metrics都是throughput单位 ops per cycle per SM 也可以进一步用warps表示也就是&#x2F;32 so the underlying thing of latency hidding is that you should increase the parallesiem to move like sequential ops without waiting </font></li></ul></li><li><p><font color=pink>latency hidding 总体而言需要更多的并行操作也就是需要更多的active warps 但是这个数量又是由memory and register 限制的所以configuration 很重要</font></p></li><li><p><font color=cornsilk>有趣的建议 <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/tipss.png"                                     ></font></p></li><li><p><font color=cornsilk>一些使用CUDA 的建议 <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bsize.png"                                     >太大的block size 会使每个thread硬件资源很少 太小的 block size warp 数量太少</font></p></li></ul></li><li><h2 id="GPU-ARCH"><a href="#GPU-ARCH" class="headerlink" title="GPU ARCH"></a><font color=Pink>GPU ARCH</font></h2><ul><li><strong><font color=cyan>The GPU architecture is built around a scalable array of <em>Streaming Multiprocessors</em> (SM). GPU hardware parallelism is achieved through the replication of this architectural building block. </font></strong></li><li><strong><font color=cyan>P 68我们先可以把SM看作一个比较强的硬件 一个grid 对应一个 kernel 一个grid的block可以分配到多个 SMs 然后一个SM 可以有多个block 也可能来自不同的grid(kernel 并发)每一个线程都具有流水线</font></strong></li><li><strong><font color=cyan>SIMT warp为一个基本管理 thread warp中每一个线程的内存与寄存器与计算资源都是独立的 SM将 block划分为warps 所以最好为32的倍数</font></strong></li><li><strong><font color=cyan>Even though all threads in a warp start together at the same program address, it is possible for individual threads to have different behavior. SIMT enables you to write thread-level </font></strong></li><li><strong><font color=cyan><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/SM.png"                                     ></font></strong></li><li><strong><font color=red>一个block只能安排在一个SMs！！！记住知道执行结束都在一个SMs 同样的一个SM可以同时有多个block</font></strong><ul><li><strong><font color=cyan>一个grid其实就是整个device了只是支持kernel的并行操纵 然后有个SM商店 shared memory 按照block划分 register 几万个按照thread划分因此 一个block间的thread可以shared mem 交流 While all threads in a thread block run logically in parallel, not all threads can execute physically at the same time. As a result, different threads in a thread block may make progress at a different pace.同一个block中的thread以warp执行 所以实际没有物理并行 这里可能会在 shared mem访存时出现竞争 CUDA提供了block内部的同步函数 但是多个block 之间没有提供同步函数 </font></strong></li></ul></li><li><strong><font color=cyan>一个core通常有一个整数ALU和浮点ALU <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/device.png"                                     >gigathread 就是全局的安排block到SM的</font></strong></li><li><strong><font color=cyan>two warps and issue one instruction from each warp to a group of 16 CUDA cores, 16 load&#x2F;store units, or 4 special function units (illustrated in Figure 3-4). The Fermi architecture, compute capability 2.x, can simultaneously handle 48 warps per SM for a total of 1,536 threads resident in a single SM at a time. 我们的这个关键就是 分组 其实有四个组合 然后选择其中一个一个作为执行选项 然后对于两个warp scheduler 就是两条流水线 然后其实每一组调度都可以看作是并行的了 不用再去管物理上的运行了上面说的48 个warp就是 同时dispatch 48 个 而不是同时运行 48个</font></strong></li><li><strong><font color=cyan>64KB memory 被分成了shared memory 和L1 cache两者关系运行更改通过runtime API </font></strong></li><li><strong><font color=red>Fermi also supports concurrent kernel execution: <font color=bluseagreen>multiple kernels launched from the same applicationtion context executing on the same GPU at the same time.</font> Concurrent kernel execution allows programs that execute a number of small kernels to fully utilize the GPU, as illustrated in Figure 3-5. Fermi allows up to 16 kernels to be run on the device at the same time. Concurrent kernel execution makes the GPU appear more like a MIMD architecture from the programmer’s perspective.<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/conc.png"                                     ></font></strong></li><li><strong><font color=cyan>LD&#x2F;ST 使用来进行转换地址的单元 16 个也是因为并行的原因</font></strong></li><li><strong><font color=cyan>kepler dynamic parallelism 允许 nested kernel invoke ; Hyper-Q 避免一个失败的kernel 调用 idle CPU 太长时间 多个task queue</font></strong></li><li><strong><font color=cyan>P 79 nvprof profiling driven 性能测试初步 类似linux里的一个 profile Event and metric</font></strong></li><li><strong><font color=cyan>memory bandwidth; compute resource ; latency</font></strong></li><li><strong><font color=cyan>Warps are the basic unit of execution in an SM. When you launch a grid of thread blocks, the thread blocks in the grid are distributed among SMs.  就是可以多个相同grid block在一个SM，也可以一个SM有来自不同block 最终硬件上都是一维</font></strong></li><li><strong><font color=cyan>warp 的划分原则 consecutive threadIdx.x !! 最后是向上取取整warps 如果非整数倍会出现空闲的不活跃thread 但是仍然会消耗占用硬件自资源也就是最终都是一维的硬件实现</font></strong></li><li><strong><font color=cyan>Warp Divergence 就是分支判断的问题 会 连串掩码式地执行 在优化等级较高时时间开销接近正常 解决就是 让一个分支用warp size 与运行</font></strong></li><li><strong><font color=cyan><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/reso.png"                                     >仔细想想居然很大程度上我们的这两个是独立影响的</font></strong></li><li><strong><font color=cyan>P89 A thread block is called an <em>active block</em> when compute resources, such as registers and shared memory, have been allocated to it. The warps it contains are called <em>active warps</em>. Active warps can be further classifi ed into the following three types: 三种 selected warp stalled warp eligible warp selected 不多于4个 是不是类似于我所说的流水线 4个选项 </font></strong></li><li><strong><font color=cyan>block太小 可以认为与大block相比同样的共享内存能偶拥有的thread 数量较少 所有register等没有充分利用 ；太大，线程太多，没有足够的thread</font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li></ul></li><li><h2 id="Synchronization"><a href="#Synchronization" class="headerlink" title="Synchronization"></a><font color=slatecyan>Synchronization</font></h2><ul><li><strong><font color=cornsilk>两个层面 host and device 2 thread </font></strong></li></ul></li><li><p><strong><font color=cornsilk><strong>device</strong> void __syncthreads(void) 一个让同一个block 的线程同步的函数</font></strong></p></li><li><ul><li><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a><font color=yellow>Configuration</font></h2></li><li><p><strong><font color=cornsilk>配置函数 对于 blockdim 的innermost x 一般是32 的倍数 这个是有 warp 决定的 同时 一个block的thread数量不能超过 1024</font></strong></p></li><li><p><strong><font color=cornsilk>通常而言 block 数量越多 并行度越高 但是load throughput会下降 但是load efficency 更高 具有更高的achieved occupancy 但是实际上 由于 block 数量的限制反而会限制active warp </font></strong></p></li><li><p><strong><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bal.png"                                     ></font></strong></p></li><li><p><strong><font color=cornsilk>第一个常见的算法就是 reduction 树形结构</font></strong></p></li><li><p><strong><font color=cornsilk>有 neighbor reduction 和 interleave reduction 后者拥有更好的global memory的局部性所以性能更好</font></strong></p></li><li><p><strong><font color=cornsilk>unrolling loop 同样的一个方法循环展开真的非常快 wtf 注意这里是Unrolling loop 注意 同步函数是用来进行 一个block 之间的同步的 block之间无法同步 </font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CUDA-amp-Algorithm&quot;&gt;&lt;a href=&quot;#CUDA-amp-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;CUDA&amp;amp;Algorithm&quot;&gt;&lt;/a&gt;&lt;strong&gt;&lt;font color=darkturquois</summary>
      
    
    
    
    
    <category term="CUDA" scheme="https://spikeihg.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://spikeihg.github.io/2023/10/20/CS224N/"/>
    <id>https://spikeihg.github.io/2023/10/20/CS224N/</id>
    <published>2023-10-20T06:09:31.000Z</published>
    <updated>2023-11-16T12:02:01.890Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224N"><a href="#CS224N" class="headerlink" title="CS224N"></a><font color=velvet>CS224N</font></h1><ul><li><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a><font color=MediumSpringGreen>前置知识</font></h2><ul><li><h3 id="TERM"><a href="#TERM" class="headerlink" title="TERM"></a><font color=yellow>TERM</font></h3><ul><li><font color=green>line up对齐</font></li></ul></li><li><p><font color=aqua>in-place 就是一般是method 直接俄改变变量的 eg 。.add()</font></p></li><li><p><font color=aqua>cross product in matrix 就是我们所学的叉乘</font></p></li></ul></li><li><h3 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a><font color=chocolate>什么是机器学习</font></h3><ul><li><p>机器学习我的理解就是在一定条件下完成一定任务，其中任务的完成由程序本身实现。</p></li><li><p>监督学习 类似回归问题和分类问题</p></li><li><p>无监督学习类似聚类算法，没有提前的正确规则，让机器找规律</p></li><li><p><strong><font color=azure>anaconda 的使用 </font></strong></p><ul><li>原理就是 conda安装更方便 类似aptitude 可以自动帮助安装 然后就是创建虚拟环境在每个虚拟环境下安装自己的包 避免版本和包冲突</li><li>conda create –name <env-name>  <package-name></li><li>conda env list</li><li>conda –help</li><li>conda activate <env></li><li>conda info -e</li><li>conda deactive 退出环境</li><li><font color=red>注意要关代理 创建环境的时候更新版本的时候也要关代理 可以设置使得能够在代理下使用但是有一点麻烦</font></li></ul></li><li><h3 id="pytorch-tutorial"><a href="#pytorch-tutorial" class="headerlink" title="pytorch tutorial"></a><a class="link"   href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" >pytorch tutorial <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><ul><li><font color=pink>tensor 的维度 创建tensor torch.empty(3,4)里面是描述tensor 的shape attribute 两个数字说明是两个dim 然后3 说明有三个行向量 4就是每个行向量有4 个元素 empty 不会进行初始化的 1 dim 就是vector 2 dim is a matrix torch.zeros(),.ones(),.rand(), 要从已有的tensor 创建拥有一样shape 的 tensor 需要用法 *_like () method 具体就是这个中括号的层数 torch 也可以直接用python 的 list 和 tuple创建甚至是混合的；在创建tensor的时候可以确定dtype,然后也看可以使用方法.to(torch.int32)</font></li><li><font color=pink>数学操作 内置的算术操作针对每一个元素做标量操作 对于两个tensor 也是进行的in-place 操作 但是必须要相同的shape 否则runtime error 一个特殊的例外就是broadcast 详细思考一下 </font></li><li><font color=pink>braodcast 从shape的last to first开始比较，当相等 或者其中一个为1 或者其中一个不存在都满足 直到比较完 判断时候broadcastable 然后算数的规则是用prepend 1 不玩较小维度的维度 然后对应的每个维度取最大值 注意 in-place 操作也支持broadcast </font></li><li><font color=pink>理解broadcast 的关键就是意识到dim&#x3D;1 的时候我们可以把这个重复的地去乘</font></li><li><font color=pink>size 就是指一个维度的元素个数 从last 开始 dim 0 dim 1 注意是从0开始的，然后对应的是shape 的第一个(3,2,1) dim&#x3D;0 size&#x3D;3 注意是相反的！！！！！！</font></li><li><font color=pink>创建tensor 的时候指明一个required_grad 这样才能够在后面调用 grad时计算此项的gradient </font></li><li><font color=pink>仔细研读了一下 每一个tensor 都有维护一个.grad 来存储自己的偏导数 </font></li><li><font color=pink>torch.tensor()总是拷贝tensor 要尽量避免拷贝 detach() 改变 required属性</font></li><li><font color=pink>卷积的filter是一个多维的matrices 一定要注意的是我们的结果始终是一个2d matrix 对于一个filter不是多个filter 妈的 kernel 就是filter 带bias 的卷积操作就是卷积的时候加上bias 得到输出 传入kernel size 是一个int默认就是方正</font></li><li><font color=pink>dim 就是一个方向可以堆叠的方向 然后可以看括号来判断</font></li><li><font color=pink>N batch size 自己定义一些东西 dataset 可以抽象成一个 list  每一个元素就是一个 map 例如 path: label 之类的 feature 就是我们的prediction label 就是实际值 定义dataset 和 dataloader 就是第一步 注意一张定制化的思想 很不错面向对象编程</font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li></ul></li><li><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a><font color=orange>流程</font></h3><ul><li><p><font color=lightgreen>custom dataset and dataloader</font></p><p>   <font color=lightgreen>dataset 抽象定义为一个map 有一个annotation file 存储所有的样本的名字 还有存储的路径 两者结合可以得到一个图片的完整路径 然后预定义transformer 三个必要的 函数 ——init—— 就是 self.img_labels read csv 读入csv 然后定义dir和 transform  ——len——  ——getitem——  对于dataloader 设置加载方式 batch shuffle  多线程加速等</font></p></li><li><p><font color=lightgreen>layer</font></p><p>   <font color=lightgreen>Linear 就是一个layer 改变输入的最内层dim 的size 由输入的参数决定 注意我们的这个layer里面已经有预先设定好的bias 和 weight 相当于就是 对于图像可能就是一个 convolve </font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li></ul></li><li><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a><font color=pink>梯度下降算法</font></h3><ul><li>同步更新所有变量 </li><li>出发点是想要拟合一段数据 然后我们想让整个数据组的误差最小。因此我们求导。可以理解为山坡上寻找下降路线。由于公式会随着接近局部最小点而自己缩小前进距离这是一个学习。</li></ul></li><li><p><font color=seagreen>输入是一个特征向量 的函数求偏导本质就是我们在微积分里面学习的矢量函数求导链式法则 θ的每个分量看作一个维度 然后是复合函数求导</font></p></li></ul></li><li><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a><font color =violet>极大似然估计</font></h3><ul><li>就是我们依照描述的事件写出这个事件发生的概率表达式，这个表达式由一个变量（涉及概率密度）决定。我们想求这个变量使得改概率函数取一个最大值。</li></ul></li><li><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a><font color=Maroon>代价函数</font></h3><ul><li>感觉与目标函数类似，一般与误差函数具有相同或者相反的单调性，然后通过一些数学技巧进行改写，以简化计算。</li></ul></li><li><h3 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a><font color=Teal>Batch Gradient Descent</font></h3><ul><li>这个就是传统的梯度下降，每一次前进时都要求遍历整个数据集来更新计算代价函数然后求偏导，计算量是非常巨大与难以实现的。具体原因我们会发现，偏导数求得的公式与每一个样本都有联系，例如差平方求和之类的。</li></ul></li><li><h3 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a><font color=Aqua>Linear Algebra</font></h3><ul><li>在此再次向Pro.Strang致以最崇高的敬意。</li></ul></li><li><h3 id="注意点——向量拓展的梯度下降以及向量函数"><a href="#注意点——向量拓展的梯度下降以及向量函数" class="headerlink" title="注意点——向量拓展的梯度下降以及向量函数"></a><font color=gold>注意点——向量拓展的梯度下降以及向量函数</font></h3><ul><li><p>注意函数变量的两个层面，一个是输入样本的维度，即样本向量的每一个维度，另一个是拟合函数中的变量即θ。h(x)(假设函数)&#x3D;θ0<em>1+θ1</em>x1 + θ2*x2+…… .eg 最后写成矩阵点积 多元线性回归</p></li><li><p>通常向量n+1 个 第0个是1为了简化表达 其余都是一个特征维度</p></li><li><p>根据上述结论重写表达式就是将θ化成对应n+1维向量然后求偏导时乘以一个xj^(i)的值。</p></li><li><p>比列失调的等高线梯度下降可能出现震荡，使用特征缩放相当于变量代换更高效将值约束在-1，1之间大约 还有归一化处理 使得平均值在0 x1-u1 代换 本质就是线性组合 u1 就是平均值 x1-u1&#x2F;s u1 就是平均值 s就是标准差 就是概率论</p></li><li><p>关于学习率α 过大可能会波动或者发散 国小很慢 总之尝试不同的一系列值</p></li><li><p>多项式回归 但是我还是有问题 函数都是人提出来 没有机器自己去寻找</p></li><li><h3 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a><font color=cyan>Normal Equation</font></h3><ul><li>线性代数永远的神，但是我已经忘记了~~~~~ 其实就是线代中的回归方程 男泵投影！！！！！</li><li>似乎用于线性回归，缺点：当n增大时会很慢 复杂度为3次方 而梯度下降可以正常的 大概10000为界限 例如 Word2vec 使用梯度下降法 而且只使用与线性 梯度是通法</li><li>pinv inv pinv 进阶求逆 可以是伪逆 可是当时没看</li></ul></li></ul></li><li><h2 id="Deep-learning"><a href="#Deep-learning" class="headerlink" title="Deep learning"></a><font color=tan>Deep learning</font></h2><ul><li><p><font color=red>简介。机器学习就是找函数function.在台大的课中只会有梯度下降 梯度下降开始的值朴素的是随机的，但是可能存在更好的 初始值全面的回归求解其实就是训练<br>模型就是我们提出的拟合方程 课程采用的是绝对值衡量</font></p></li><li><h2 id="piecewise-linear-curve所有线性的折线都可以用一组z来拟合-同理对于光滑的-我们可以无线细分-由piecewise-linear-curve-来逼近-进一步又由蓝色来逼近-！！！！！！！！！！！"><a href="#piecewise-linear-curve所有线性的折线都可以用一组z来拟合-同理对于光滑的-我们可以无线细分-由piecewise-linear-curve-来逼近-进一步又由蓝色来逼近-！！！！！！！！！！！" class="headerlink" title="piecewise linear curve所有线性的折线都可以用一组z来拟合 同理对于光滑的 我们可以无线细分 由piecewise linear curve 来逼近 进一步又由蓝色来逼近 ！！！！！！！！！！！"></a><font color=lavender>piecewise linear curve所有线性的折线都可以用一组z来拟合 同理对于光滑的 我们可以无线细分 由piecewise linear curve 来逼近 进一步又由蓝色来逼近 ！！！！！！！！！！！</font></h2></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/efunc.png"                                     ></p></li><li><p>y&#x3D;csigmoid(b+wx);  hard sigmoid w slopes b shift  </p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bff.png"                                     ></p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/beauti.png"                                     ></p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/ddd.png"                                     ></p></li><li><p>sigmoid 的个数自己决定</p></li><li><p>实际的y帽 叫做 label</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/theta1.png"                                     ></p></li><li><p>batch 将N划分作batch随机的来求梯度</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/epoch.png"                                     ></p></li><li><p>epoch 是所有包都看了一遍 update就是一次更新 不一样</p></li><li><p>batch size learning rate 都是hyper parameter</p></li><li><p>ReLU rectified linear unit cmax(0,b+wx)就是hard sigmoid</p></li><li><p>就可以在所有sigmoid 使用的地方用ReLU</p></li><li><p>统称为activation function 老师都用的ReLU </p></li><li><p>可以多层进行变换 layers 就是得到a后再带入进去</p></li><li><p>多次ReLU 意思就是</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/network.png"                                     ></p></li></ul></li><li><p>为什么更深 乐 老师太好玩了！！！！</p></li><li><p>overfitting 过拟合问题 worse on unknown data</p></li><li><p>backpropagation </p></li><li><h3 id="anoconda-创建指令是全局的conda-create-然后可以在里面下载包-用vscode-启动可以-注意激活的时候要把代理关了"><a href="#anoconda-创建指令是全局的conda-create-然后可以在里面下载包-用vscode-启动可以-注意激活的时候要把代理关了" class="headerlink" title="anoconda 创建指令是全局的conda create 然后可以在里面下载包 用vscode 启动可以 注意激活的时候要把代理关了"></a><font color=pink>anoconda 创建指令是全局的conda create 然后可以在里面下载包 用vscode 启动可以 注意激活的时候要把代理关了</font></h3></li><li><h2 id="jupyter-notebook-guide"><a href="#jupyter-notebook-guide" class="headerlink" title="jupyter notebook guide"></a><a class="link"   href="https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/install.html" >jupyter notebook guide <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h2></li><li><p>jupyter 可以使用命令行调用 </p></li><li><pre><code class="python">jupyter notebook 然后就进入了browser</code></pre></li><li><h2 id="Colab-使用"><a href="#Colab-使用" class="headerlink" title="Colab 使用"></a><strong><font color=slategray>Colab 使用</font></strong></h2><ul><li><p>python code 和 shell code 其中！接shell cmd cd除外 %cd</p></li><li><p>可以选择执行的硬件 GPU runtime type 里面</p></li><li><p>ctrl+ enter 执行一个代码cell</p></li><li><p>总体而言其实就是jupyter 只不过是个互联的jupyter.</p></li><li><p>左侧的文件图标查看结构 注意下载邮寄 可以上传到google硬盘</p></li><li><p>注意自己使用的时候是在google的GPU上 所以程序结束就会消失  注意自己保存</p></li><li><p><strong><font color=lightcyan>打开新的需要在file 里面upload notebook!!!! 可以的 注意一次只能有一个session 所以需要关掉前面的 notebook maybe<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/save.png"                                     >真的很不错一个tesla 真棒 然后我可以试试ssh之类的</font></strong></p></li><li><p><font color=yellow>然后现在发现了 ctrl+e 普通搜索很快 然后url 对url很快 因为对普通搜索会转换为我们的query 条目 然后会比较慢！！！</font></p></li><li><p><a class="link"   href="https://github.com/virginiakm1988/ML2022-Spring" >ML github repo <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>pytorch tensor 相当于 array 可以GPU 加速</p></li><li><h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a><font color=green>Pytorch</font></h2><ul><li>tensor就是高维数组 </li><li>还得复习一下基本的python 语法 list dict class func 基本的一些使用 顺便复习写一写爬虫</li><li>tensor constructor numpy zero tensor unit tensor</li><li>每个batch 的 loss funct 可能存在不同的差别</li><li>sigmoid 或者 Relu 叫做 neuron 总体叫做 neural network</li></ul></li><li><h2 id="Python-review"><a href="#Python-review" class="headerlink" title="Python review"></a><font color=purple>Python review</font></h2><ul><li><p>if var in list:  if var not in list:</p></li><li><p>if var1,var2 not in list；</p></li><li><p>for key,value in dict:</p></li><li><p>for key in sorted(dict.keys()):</p></li><li><p>for value in sorted(dict.values())</p></li><li><p>answer &#x3D; input(‘please enter your answer’)</p></li><li><p>int(input(‘how old are you’)) </p></li><li><p>f”{var1_has_defined} {var2_has_dafined}” mesg_to_be_printed&#x3D;f””</p></li><li><p>python 函数调用时候 可以直接指定 def fun(var1,var2): …… fun(var1&#x3D;yes,var2&#x3D;no) 但是一定要记住名字 不要出错</p></li><li><p>默认形参也是放在后面</p></li><li><p>while some_list:</p><p> ​item&#x3D;list.pop()</p><p> ​do_with(item)</p></li><li><p>dict[‘new_key’]&#x3D;new_value</p></li><li><p>def fun(list_para):…     fun(list[:]) 传递一个切片  函数都是引用一定会修改变量的</p></li><li><p>可变形参 def func(*tuple_para): def func2(size,**dict_para):</p></li><li><h3 id="Class-in-Python"><a href="#Class-in-Python" class="headerlink" title="Class in Python"></a><font color=maroon>Class in Python</font></h3></li><li><p>class my_class(): 开头的书写方法</p><ul><li>def __init(self,para1,para2)__self 必须第一个</li><li>然后接着是 self.para&#x3D;para(实际传入实例类的形参)这样写之后this 相当于才拥有这些成员</li><li>​普通方法 def member_func(self): 不要忘记了self</li><li>如果要有具有默认初始值的属性 可以直接在__init()__ 下面进行写 self.prop&#x3D;1000 prop 不用出现在init括号里面</li><li><font color=aqua>继承</font></li><li>首先必须括号里写明继承的类 class derived(base):</li><li>super()._<em>init(para,para,para)</em>_注意里面没有self 继承全部内容</li><li>自己属性接着写就可以</li><li>可以重写父类方法 名字不同就可以</li><li>可以类实例作为成员 self.class_mem&#x3D;classA()</li></ul></li><li><p>这个 <strong>init</strong> 不是必须的方法 只是用来定制实例化时 类似的还有 _<em>self</em>_  _<em>next</em>_  等用来控制 迭代器的 同时呢 生成器 generator 是一个综合了上面方法功能的函数 yield  generator expression</p></li></ul></li><li><p><font color=green>文件操作</font></p><ul><li>with open(‘filename’) as name:  不需要close了 因为with</li><li></li></ul></li></ul></li><li><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a><strong><font color=cornsilk>工具</font></strong></h2><pre><code>    *  training data 上的loss过大</code></pre><ul><li>Model bias 就是 我们的函数太简单 解决方法 一 增加 特征量 二 增加layer deep learning</li><li>优化问题 梯度下降的问题</li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/optm.png"                                     ></li><li>怎么解决 优化的问题 next lecgt</li><li>一定区分 overfitting 和 优化问题 一个是test data 一个是 training data<ul><li>overfitting <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/freestyle.png"                                     ></li><li>解决方法 增加 training data 二 data augmentation 就是自己创造一些条件 创造一些资料 需要有道理</li><li>减小弹性 增加限制</li><li>full- connected比较有弹性目前我们讨论的； CNN 比较无弹性<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/stop.png"                                     ></li></ul></li><li>区分 overfitting 与 model bias  存在一个complexity 与 bias 关系</li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/trade.png"                                     ></li><li>刚刚好的<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/aaa1.png"                                     ></li><li></li></ul></li><li><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a><font color=pink>Tips</font></h3><ul><li><font color=mediumspringgreen>critical point saddle point &amp; local minima 前者更多 通过hessian 矩阵来判断 特征值来判断 全正或者全负local其余就是saddle point</font></li><li><font color=mediumspringgreen>针对local point 的方法 batch size  一般来说越小noise 越多但实际上更好 但是在并行计算下可能更慢 第二 momentum 惯性一样的下一步加成</font></li><li><font color=mediumspringgreen>learning rate 的问题 有一个方法 叫做 Adam Optimizer 就是RSM 加上 momentum 的结合 可以动态改变learning rate 。 也就是说我们的learning rate可能也是 loss stuck的原因，而非 critical point </font></li><li><font color=mediumspringgreen>loss 函数也有影响 对于分类问题而言 使用最多的 是 cross entrophy 原来是用似然函数 好处就是 可以将整个surface 放得平缓</font></li><li><font color=mediumspringgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/Fea.png"                                     >理解权重影响的问题 和这个surface是怎么来的 我们要不断修正的是w1 w2  mean 就是平均值 standard deviation 就是标准差 标准化  数学的影响就是 loss converge 收敛更快 但是还是有个问题，在实际的多层神经网络中 每经过一层 可能分别差别又会变大 所以我们还是需要不断地进行normalization 可以是activationfunc 之前 也可以之后 sigmoid 最好之前 因为可以化到-1 1 之间使得函数的值变化比较大<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/norm.png"                                     > 这个优化提升的是训练速度 主要是<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/arx.png"                                     ></font></li><li><font color=mediumspringgreen>CNN 卷积神经网影像处理，一个图像就是一个RGB的三位channel 的tensor 就是一个高维的叠加的数组 拉直就是一个向量 但是我们一般不会全部进行训练 我们会进行一定的相关的简化receptive field 这样做的一个理论 就是探查pattern 用pattern 去进行识别</font></li><li><font color=mediumspringgreen>一般通常选取都是三个channel 然后此时的长宽称为kernel size 3X3 通常就可以了 stride hyper para 超出的部分进行padding <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/stri.png"                                     ><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/filter.png"                                     ></font></li><li><font color=mediumspringgreen>fully connected layer弹性最大  receptive field 共享参数 减小了弹性这两个加起来就是convolution al layer 对应的就叫 CNN model bias 较大<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/fmap.png"                                     >这里的channel变成了neuron 的个数了</font></li><li><font color=mediumspringgreen>pooling 方法 max pooling  的方法 为了减少运算量 现在开始减少了<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/CNN.png"                                     >下面那个是flatter 还有一个重要应用 就是 playing go</font></li><li><font color=mediumspringgreen>分类问题softmax的原因简单解释 就是 我们用one-hot 向量表示我们的类 然后用1 然后我们将softmax 将其转换为-1 到 1 当然我觉得可能还是因为概率分布的问题就是越大的比例越大</font></li></ul></li><li><h2 id="self-attention-自注意"><a href="#self-attention-自注意" class="headerlink" title="self-attention 自注意"></a>self-attention 自注意</h2><ul><li><font color=pink>问题引入 加入我们处理的input data是一个向量序列 而不是一个向量。 对应的输入也有不同的种类。比如说输入的每一个向量都计算一个label 例如判断文本每个单词的 词性 或者类似的分裂问题 。或者一个输出 比如对一句话进行定义  反正应用情形自己去想象 最复杂也许是seq2seq 输出的已是一个序列 例如翻译</font></li><li><font color=pink>sequence labeling 如果仅仅使用前面的network 然后单独输入的话存在一个巨大的问题就是无法做到考虑上下文使用情形有限</font></li><li><font color=pink>attention is all you need <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/matr.png"                                     ><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/attention.png"                                     ><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/multi.png"                                     ></font></li></ul></li><li><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><ul><li><font color=pink>encoder FFN feed forward network </font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CS224N&quot;&gt;&lt;a href=&quot;#CS224N&quot; class=&quot;headerlink&quot; title=&quot;CS224N&quot;&gt;&lt;/a&gt;&lt;font color=velvet&gt;CS224N&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&quot;前置知识&quot;&gt;&lt;a href=</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://spikeihg.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Begin_Again</title>
    <link href="https://spikeihg.github.io/2023/10/15/Begin-Again/"/>
    <id>https://spikeihg.github.io/2023/10/15/Begin-Again/</id>
    <published>2023-10-15T03:21:50.000Z</published>
    <updated>2023-11-17T13:14:21.612Z</updated>
    
    <content type="html"><![CDATA[<h3 id="计算机科学中的自然原理"><a href="#计算机科学中的自然原理" class="headerlink" title="计算机科学中的自然原理"></a>计算机科学中的自然原理</h3><ul><li><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong><font color =green>感谢过去一年中给予我启发的诸多事物，无论是一本书如CSAPP，一堂课如数字电路，或者一个人如Prof.Strang，所有这些人事都是促使我更加严肃地审视计算机科学与其背后丰富而美丽的思想。所以，我希望继承那些前辈，那群充满热情与想象力的先驱的工作，在这里对CSAPP中的美妙理论进行简答而又深刻的阐释（有点自大哈哈哈哈），希望在有限的时间与文字中，探寻科学与自然的美。</font></strong></p></li><li><h2 id="你好！世界"><a href="#你好！世界" class="headerlink" title="你好！世界"></a><font color=pink>你好！世界</font></h2><blockquote><p>萨冈和她的你好忧愁，我和我的你好世界。Hello World,梦开始的地方，我们就从一个hello world.c 程序的生命开始进行一场快速的计算机世界漫游。</p></blockquote><ul><li><font color=aqua>程序是怎么编写的呢，首先我们会需要一个文本编辑器，也就是我们常用的devc++或者是vscode，vim，emacs。文本编辑器就是编辑文本文件的，我们缩写的源文件也属于文本文件。文本文件就是只含有阿斯克码的文件，其余的文件都是二进制文件。编写后，我们就可以通过一系列指令来使程序运行。对于一个.c文件而言。我们可以用gcc 命令来生成可执行文件。gcc就似乎编译驱动程序 这是nux终端的命令。当启用后，首先运行的是预处理器，对于含#的指令，如#include预处理器会将头文件全部插入到源文件中，同时完成宏的拓展。这是纯粹的文本替换，其他什么都没有发生。然后就是编译器，编译器将.c文件转换为汇编语言格式，可以理解为机械码的助记符，这是程序员可以阅读和编写的。然用汇编器汇编为二进制，此时是一个可重定位的可执行文件，此时通过ld将引用的库一起链接形成一个可执行文件保存在内存中。调用时，通过加载器加载到cpu进行执行。</font></li><li><font color=pink>几个关键概念。首先，计算机的硬件组成。CPU，内存空间极其缓存结构和虚拟地址，网络与I&#x2F;O 进程与线程。这里我们能慢慢接触到抽象与设计的感觉。</font></li><li><font color=blue><em><strong>一切皆文件 ，linux将设备文件都以同一种方式进行处理，让建立一种广泛而统一的接口成为可能</strong></em></font></li></ul></li><li><h2 id="从理论到实践"><a href="#从理论到实践" class="headerlink" title="从理论到实践"></a><font color=MediumAquamarine>从理论到实践</font></h2><p><strong><font color=ForestGreen>在学习每一个章节的过程中，我们会逐渐感觉到与一些相似的内容串联了起来。这种知识路径形成环，环闭合的感觉非常类似于我在高中看科幻小说时形成的想法。哈哈哈哈哈。这里就列举所有对应的知识群，然后随着不断学习深入，持续补充~~（突然想到也许我得去补充几个emoji和颜文字）</font></strong></p><ul><li><strong><font color=Lavender>二进制族群——数字电路的设计非常优美简洁;CPU流水线；</font></strong></li><li><strong><font color=lavender>程序机器级表示——优化技巧</font></strong></li><li><strong><font color=lavender>链接——Makefile脚本和Cmake 的使用 以及Vscode 相关json文件的配置</font></strong></li><li><strong><font color=lavender></font></strong></li><li><strong><font color=lavender></font></strong></li><li><strong><font color=lavender></font></strong></li><li><strong><font color=lavender></font></strong></li><li><strong><font color=lavender></font></strong></li></ul></li><li><h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a><font color=orange>FAQ</font></h2><ul><li><font color=pink>buffer overflow 字符串溢出覆盖栈上内容读取字符串造成</font></li><li><font color=pink>为什么重载函数不能返回区别，因为编译器重整符号时只会考虑函数名字和参数类型</font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li></ul></li><li><h2 id="数据表示"><a href="#数据表示" class="headerlink" title="数据表示"></a>数据表示</h2><ul><li><font color=brown>浮点数与整数两种格式，拥有不同的表示方法，所以进行类型转换时要注意。整型通过补码表示。公式是唯一的-2^n + 源码的二进制。浮点数是一中近似的表示，对于太多小数位，进行加减时可能出现差。还有就是有符号与无符号的区别以及对应的溢出问题，截断问题。最后就是其实很多函数%d 并不关心真正的类型也不会检查，这只是告诉函数将以一个整型的方式进行内存寻找。</font></li><li><font color=cyan>big-end &amp;&amp; small -end 大小端 注意只存在于多个字节的数据的问题 例如0x12345678 小端机就是 78 56 34 12 就是地位在小地址，注意在网络编程获取主机名于端口时可能有影响，需要调用相应修改转换函数。</font></li><li><font color =Orchid>计算机处理加法乘法都远远快于除法。同时可以尽量写位运算，当然编译器可能也帮你优化。数字的表示与实现都很精妙 前辈的只会佩服。</font></li></ul></li><li><h2 id="汇编简介"><a href="#汇编简介" class="headerlink" title="汇编简介"></a><font color=PowderBlue>汇编简介</font></h2><ul><li><font color=Orchid>首先我们要知道，计算机只认识01，01 构成了整个世界，在数字电路的学习中我们也能有这样的体会。事实上，我们所写的程序最终会转换成01的机器代码，所有的文件不论是文本视频图片文件最终都是01串。而汇编代码就是位于机器代码的一种助记符</font></li><li><font color=sandybrown>gcc -Og O1 O2 O3 通常而言 O1 分析 O2 可接受优化 Word因特尔的字就是 16bit 寄存器 6个参数寄存器 rdi rsi rdx rcx r8 r9 然后返回 rax 栈 rsp 计数器PC rip 然后就是被调用者寄存器 我们来看一下机械逻辑是怎么形成过程的。栈，核心，栈帧栈的空间。然后就是三个部分，传递控制，传递数据，分配和释放内存。控制传递依赖两个命令与rip call 会将放回地址即下一条弹入到栈中，然后rip变为label的地址。ret就会压栈然后rip回到返回地址。 数据控制就是通过栈存储多余参数调用，被调用者保存寄存器数据，局部变量存储完成的。</font></li><li><font color=lightcoral>指针与数组，数组就是转化为i指针运算。通过改变内存寻址来实现c语言中的指针类型。结构而言，字段就是基地址偏移量。引入重要的对齐概念：一句话任何大小为K字节的数据类型的首地址都要为K的倍数，intel不强行对齐。因此就可能会补全。buffer overflow 就是字符串溢出覆盖栈上数据，然后对应防范有栈随机化和金丝雀技术。 alloca 栈上分配空间。</font></li><li><font color=wheat>浮点数，单独一组寄存器 ymm 256 64bytes,xmm 32bytes. SSE,AVX架构。包含头文件可以使用。</font></li></ul></li><li><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a><font color=yellowgreen>优化</font></h2><ul><li><font color=greenyellow>优化面对的挑战。memory aliasing 就是同一个地址由多个指针变量使用，一些激进优化可能导致问题。函数调用对全局状态的改变。</font></li><li><font color=lightblue>CPE，周期每元素一个度量单位。延迟，就是严格顺序执行时一个操作需要的实践。发射时间就是两个相同命令执行所需的间隔，为1就是完全流水化时间。吞吐量，就是发射时间的倒数乘以功能单位。优化方向有减少循环内部计算。减少函数调用，合理内联。减少同一值的反复求值过程。循环展开。多路合并，也就是累计变量。写出简短便于求值使用数据传递控制的条件判断。然后就是使用向量操作。减少内存引用，使用局部变量存储。然后就是重新结合提高指令级并行度。 </font></li><li><font color=plum>关键路径，就是分析优化的一种指令级维度的技术。将汇编代码进行分析，观察其中的循环寄存器以及相关数据链，然后数据链对应就是一个关键路径，看关键路径有无线性缩短。乱序处理，流水线，分支预测，投机执行。CPU的技术。程序分析。unix 上的gprof ，linux上的valgrind, intel的vtune，以及nvidia的nsight 好多profiler。其中gprof 使用需要加一个-gp 选项在gcc中</font></li><li><font color=cadetblue>Amdahl’s law 就是想要加速一个系统，其加速比取决于加速部分时间占据整个系统的比重以及加速程度。</font></li></ul></li><li><h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a><font color=cornflowerblue>内存</font></h2><ul><li><font color=coral>DRAM，SRAM，ROM，memory ,cache .等名词分清楚。SRAM 构成高速缓存的物理媒介快贵。DRAM构成主存的媒介也就是运存。ROM一般指磁盘一类的flash disk ,CD , ssd 都是不同的存储。</font></li><li><font color=rosybrown>DRAM 芯片结构。单个芯片有超单元矩阵构成，每个超单元一般有一个byte.然后2位地址引脚，8位数据引脚掌控数据的传输和行列索引。将芯片封装乘模块。叠8个，一次64位。通过内存控制器广播来讲相同索引的8byte数据聚合成一团数据。改进 DDR SDRAM 双倍数据速率同步DRAM 就是两倍的时钟上升沿</font></li><li><font color=olive>ROM是一类统称（非易失性存储），目前最主流的是flash memory。然后还有最新的SSD。ROM 上存储的一些程序叫做固体firmware 例如bios.通过总线进行访存。不过典型计算机使用的是磁盘技术。 连接设备i&#x2F;o 桥 ，系统总线与内存总线。io设备诸如键鼠GPU都是通过io总线尤其是PCI(外围设备互联总线)连接的，io总线比系统和内存总线慢，但是功能更加多样。</font></li><li><font color=springgreen>DMA磁盘。 磁盘直接内存访问，将磁盘内容发送到主存后再发送一个intercurpt型号。SSD使用闪存技术趋势，存储器尤其是DRAM 渐渐跟不上CPU的发展，差距越来愈大。随着单核性能趋近饱和，多核处理器出现，吞吐量成为另一个限制条件，而不再是延迟。</font></li><li><font color=aqua>locality 。深刻而间接的原理——局部性。时间局部就是重复变量，空间就是步长越小的引用</font></li><li><font color=aqua>memory hierarchy缓存实现的基本原理。通常将k+1层的数据c化为连续hunk成为block，与k层的cache交换都是以block作为一个传输单元。然后缓存命中就是k层中找到要访问数据，反之就是不命中。不命中有很多原因种类。冷不命中就是缓存开始时空的。然后由于很难做到随机任意替换的策略，也许是某种倍数一一映射替换策略可能导致冲突不命中。然后如果我们访问的工作集超过缓存的大小会发生容量不命中。寄存器文件由编译器管理，l1 l2 L3 缓存由硬件直接管理可以做到任意替换策略。主存由os 即虚拟内存。磁盘可能由分布式内存软件管理。 TLB 翻译后备缓存器由 硬件MMU管理。</font></li><li><font color=cornsilk>存储器结构实现 地址查询的实现本质是一种简单的哈希查询。S 组数，整个地址空间划分成组，每组有许多行，每行有一个有效位，标记为以及块，块中有好几位。 BxSxE ,依据每个组中的行数分三类，E&#x3D;1 直接映射高速缓存 组选择，行匹配，字抽取</font></li><li><font color=burlywood>理解划分，t,s,e 首先 s 划分组划分的是cache的组数 会比+1层的组数少，所以也许存在倍数映射，这是靠t标记位实现的，所以t+s真的唯一确定k+1内存块的位置。所以判断的 时候不仅要看有效位还要看t标识位。所以有时候会存在内存抖动问题就是冲突不命中刚好同一行反复驱逐</font></li><li><font color=cyan>组相联高速缓存 ，区别就是需要扫描标记位和有效位，因为同一组的所有行都有可能拥有有效数据，同样的在驱逐行时，也需要一定驱逐的策略。最近最少使用.eg</font></li><li><font color=pink>全相连高速缓存 只有一个组，所以关键是在扫描标记位和有效位 需要并行搜索标记，所以一般用于小的 eg TLB</font></li><li><font color=lightgreen>写操作，两个类型。在写命中时，就是cache中有需要被写的对象时，直写就是直接改变内存和cache，写回就是改变cache,维护一个改变位，推迟改变内存。 在写不命中时也有两个，写分配就是写内存的同时把其加载到cache,非写分配就是不加载。通常我们考虑写回加写分配。 i-cache缓存指令只读,d-cache 缓存数据。总结上面的影响，cache大小。越大命中率越高，但访问时间越长。块大小，越大，空间局部性越好，时间局部性越差，因为行数越少越容易被替换，相连度，越大越复杂，命中使&#x3D;时间越长，但是不命中处罚越低，</font></li><li><font color=goldenrod>应用 矩阵乘法优化。。</font></li></ul></li><li><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a><font color=Tan>链接</font></h2><ul><li><font color=green>编译驱动程序gcc 一套流水作业-v查看注意最后执行的时候会调用一个系统的加载器来复制到内存并转移控制ld 静态链接器在详细认识ld前要一下基本认识。ld两大基本任务。符号解析和重定位。symbol resolution and relocation .符号解析式将符号引用与符号定义关联。重定位是将符号定义与内存关联。大部分指令都有汇编器编译器确定好，ld仅仅奉命完成</font></li><li><font color=yellow>可执行文件，三种可重定位，可执行，共享（特殊的可重定位），linux是ELF，windows是PE，可重定位目标文件的格式。ELF头然后中间很多节(.session)然后是节头部表。.text 代码 .rodata .data .bss .symtab .rel.text .rel.data几个比较重要的 .bss 未初始化的全局静态 以及初始化为0的 节省空间函数就在.text里面显然</font></li><li><font color=deepskyblue>符号表 符号：三种：模块m定义的全局符号（函数与全局变量），模块m引用的全局符号，m定义的局部符号（static函数和只被m引用的全局以及静态变量）注意所有局部变量都没有条目（栈）因此可以使用static 隐藏模块的函数(模块就是一个可执行二进制文件)。symtab 包含一个条目的结构数组对应可看书P469 readelf 程序可以看二进制文件</font></li><li><font color=pink>符号解析，对于局部变量编译器确保唯一。编译器处理全局符号。对于全局符号，划分强弱，以处理重命名情况。 一，不允许有多个同名强符号。二，一个强多个弱同名，选择强，三，多个弱同名任意选择。函数与初始化了的全局变量为强，未初始化的全局变量为弱。 -fno-common 来使得不能生成common二唯一变量。我们使用了静态库技术。模块打包成库文件。对于实际链接时，加载器只会复制库文件中实际引用的部分，减少内存浪费。静态库的格式是archive.  .a 后缀 AR工具自己创建 。 –static 参数告诉驱动程序gcc生成一个完全链接的可执行文件，也就是说可以直接执行了，不需要动态链接。</font></li><li><font color=cornsilk>重定位 两步，将所有输入模块的节聚合然后分配到具体的运行时内存，唯一绝对的地址。（定义）。将引用是其指向正确的地址。重定位符号引用：PC相对引用和绝对引用两种。可执行文件，格式多了.init 入口点 组织成片，对齐。加载execve函数可以调用加载器。 运行时内存映像。0x400000开始本质是fork了一个子进程</font></li><li><font color=lightgreen>动态链接，动态链接器.so  DLL -shared 参数指示创建一个共享文件，-fpic 创建位置无关代码。共享库就是不复制模板引用的节，而是一些用于定位到 信息，等到实际执行的时候，再利用信息重定位。运行时动态链接dlopen接口打开，dlsym引用一个符号 dlclose 关闭 java也是类似使用c接口的</font></li></ul></li><li><h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a><font color=Maroon>进程</font></h2><ul><li><p><font color=yellowgreen>进程就是一个运行的程序的实例。可以从功能上理解就是一个具体完成我们程序的过程。从组成上就是包括一系列的物理资源。包括内存，寄存器文件，控制器，内核栈等信息。总之就是上下文。</font></p></li><li><p><font color=salmon>异常，，就是逻辑控制流发生改变的情况，event事件也是如此。 异常发生在内核模式下。异常有中断interrupt 陷阱 trap 故障 终止。 终端就是硬件上，异步发生的。 trap 就是系统调用 system call  对于异常有三种情况 Icur 返回 Icur Inext 或者直接俄abort。 </font></p></li><li><p><font color=yellow>shell 执行一个命令就是fork 了一个新的进程并发 就是多任务 多进程 很多错误都是有定义的 有硬件设计者或者是内核维护 同样的异常处理程序也是。 可以通过 errno来查看 函数就是 stderror(error)</font></p></li><li><p><font color=deepskyblue>进程</font></p><div class="highlight-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line">  <span class="meta">#<span class="keyword">include</span><span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">pid_t</span> <span class="title">getpid</span><span class="params">(<span class="type">void</span>)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">pid_t</span> <span class="title">getppid</span><span class="params">(<span class="type">void</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="type">pid_t</span> <span class="title">fork</span><span class="params">(<span class="type">void</span>)</span></span>;<span class="comment">// 子进程得到的是副本</span></span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">execve</span><span class="params">(...)</span></span>;</span><br><span class="line">  <span class="function">unsighed <span class="type">int</span> <span class="title">sleep</span><span class="params">(ui)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">pause</span><span class="params">(<span class="type">void</span>)</span></span>;</span><br><span class="line">  <span class="comment">// 还有可以设置环境变量的sysm call env</span></span><br></pre></td></tr></table></figure></div><p><font color=deepskyblue>进程三个状态 运行 停止 终止 停止就是挂起 可以接受信号后继续运行 其中终止三种 接受到一个信号，主程序返回，exit函数</font></p><p><font color=pink>理解子进程共享状态 关键： 副本虚拟内存的副本。仔细研究就是 相同的执行程序，内存，data等节。然后相同的用户栈信息，共享文件描述符，共享共享库也就是相同的工具 最大不同就是PID 子进程是0 父进程是子进程的PID返回对于fork 一次调用两次返回 注意这是系统调用 从内核返回到用户模式 并发执行父子进程 </font></p><p><font color=lightgreen>进程操作，主要是理解不用背。 未回收的进程就是僵尸进程交给init 进程 PID&#x3D;1 所有进程的祖先 waitpid wait 可以等待并且有一定的行为可以操作   fork 和 exevce 就是实现 sh的关键 大概就是 fork 一个子进程然后进行 execve 执行 对应的命令</font></p></li><li><p><font color=lightblue>信号 linux 信号 可能来自内核检测到一个系统事件(event) 也可能来自其他程序的kill发送 接受三种 忽略 终止 或者信号处理程序</font></p></li><li><p><font color=cornsilk>strace 命令 可以使用-static 编译后查看程序中所有系统调用的轨迹 pmap 显示进程的内存映射 &#x2F;proc 内核提供的一个可以在用户模式下查看系统信息的文件 还有&#x2F;sys   execve 在内存里拥有一个内存栈 存储有argv 和 env 以及初始化函数的栈帧 非常的good</font></p></li><li><p><font color=pink>信号</font></p><p><font color=pink>信号是操作系统提供的内核。接受信号的物理实现 内核为每一个进程维护两个位向量,pending &amp; blocked.规则：一个类型至多一个待处理信号，多余发送直接丢弃。进程可以 选择性阻塞信号，这样的信号不会被接收，但是能够发送，只要传送信号，pending就会置为，只要接受，pending就会清除，这里接受可以理解为成功捕获并响应。更准确的说是trap 陷入那一刻起就恢复，因此可以执行handler K 时 捕获k<br>进程组 一般子进程会具有父进程的组pid。 job 一个前台job 很多个后台job job 就是一个进程组<br>signal 函数可以修改信号默认行为 每个信号都有默认行为 sigkill sigstop 不可修改 <br>编写信号处理程序的忠告 处理程序简单 调用异步安全函数sprintf 之类不安全。 保存errno 对全局变量访问期间要暂时阻塞信号 volatile声明防止缓存不一致 sig_atomic_t 声明变量 单变量原子操作</font></p><p>Richard Stevens<br><font color=pink>我们每次一个命令执行完后都会return main 所以当前进程会终止 然后被也许 init 或者父进程回收 子进程也会继承信号阻塞的信息向量 信号许多时候会造成竞争，所以我们也需要禁止某种操作，所以我们需要阻塞某些信号。 最后一个 sigsuspend 函数用于等待操作 详细请见 我们的P527 讨论<br>非本地跳转setjump longjump 函数实现的 可以超过一般的处理程序调用的规则进行调用</font></p></li></ul></li><li><h2 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a><font color=lightgreen>虚拟内存</font></h2><ul><li><font color=lightgreen>虚拟内存针对核心对象的是主存</font></li><li><font color=lightgreen>虚拟内存的核心思想就是把磁盘的所有字节划分为一个连续的数组构成一个虚拟地址空间。然后将主存划分为一个物理地址空间，将磁盘的内容缓存在主存上，类似于SRAM缓存体系，这个就是DRAM 缓存体系，很多地方是相同的。我们有操作系统维护一个于虚拟页数（就是缓存里面的块）相同的一个PT（page table 页表） PTE一个条目里可以简化为一个有效位标志是否被缓存，以及一个物理地址字段对应其被缓存在主存上的实际地址。一个page 有三个状态(未分配就相当于磁盘未被使用；分配未缓存，已经被分配例如使用了malloc,但是还没有被加载到主存中；已分配已缓存，此时有效位置位)。现在你直到了malloc 的实际用法。还有就是因为巨大的不命中处罚，所以一个虚拟页较大，并且全相联。同时按需调度，到最后一刻才真正加载到主存。 getrusage 函数可以查看缺页情况。</font></li></ul><p><font color=lightgreen>虚拟内存如何实现一个统一的内存图景。这样想，我们任何一个程序的开始都一样，这个是虚拟地址，因为内核为每一个进程都维护了一个完整相同的独立的虚拟地址页表，并且虚拟地址要翻译为物理地址，所以啦，我们只需要在虚拟地址向物理地址的映射做一点手脚，我们认为控制这个映射就可以实现任意内存分配 简化加载，我们可以划出虚拟页表指向目标文件中的内容，这就是文件内存的映射，mmap 可以在应用层控制。 简化内存分配 malloc 实际的物理地址可以散落各处 实现内存保护。有几位标识读，写，执行权限。CPU 每次产生一个地址都需要查看PTE 所以在翻译的硬件MMU 中有一个缓存 TLB了解就好 翻译过程可以感兴趣再看看。还有就是使用了一个叫做多级页表的方式减少内存占用。</font></p><p><font color=pink>图景的实现原理。内核为每一个进程都维护了一个虚拟地址空间。并将虚拟内存组织成区域也叫做段。例如数据段，代码段，共享库段。实际过程中内核为每一个进程记录一个task_struct 的结构数组，里面包含着进程上下文信息。其中mm_struct 记录了有关虚拟地址的信息。在这个结构中存在一个链表每个node 对应一个段的相关信息。 缺页处理，首先判断是否位于虚拟内存内，如果不在就触发一个段错误。1 .然后判断权限是否对应，如果错误 2.然后开始替换，然后返回到除法信号的指令再次翻译地址。注意虚拟内存上的分配与磁盘上的存储是两个不同概念。只有当前执行的时候我们才加载，而加载的实际含义语义其实是让它具有虚拟地址。让其映射到虚拟地址空间！！！！！！！！！好好理解。抽象成一个空间集合！！！！简洁而有效。</font></p><p><font color=deepskyblue>memory mapping。 定义由上面其实页明白就是将一个虚拟内存区域与一个磁盘的对象关联起来，然后初始化这段虚拟内存的内容。映射磁盘对象的时候，有两种。对于Linux普通文件，将文件区分成片，映射到对应的虚拟内存页面。实际上并没有进入物理内存，而是使用按需调度机制。还有就是匿名文件，其实就是选择物理内存上一个牺牲页然后覆盖为0，实际上也没有磁盘的流量。 现在我们特殊讨论一下我们的共享对象。也就是映射的实际情况。首先，对于我们的这个共享对象，我们只需要一个物理内存中的副本，每一个进程可以在自己各自的不同的虚拟内存段上映射，同时每一个进程对于该共享对象的修改都是公开的。 同时还有私有对象，即每一个都是独立的。这种独立的实际事项方式是，先共享映射不过我们设置只读，当正在有探测到写入的时候，我们在复制该对象给写入的进程，尽量推迟从而提高效率。这就是写时复制技术。这也是fork 保持独立的实现原理。私有的，写时复制。execve函数就是一个删除现存虚拟内存重新映射的一个过程 然后mmap 函数可以让我们要求内核开辟虚拟内空间然后把我想要映射的对象关联起来。至于malloc 显式分配器的实践就是一个应用，可以参看书P587 .</font></p></li><li><h2 id="系统级I-x2F-O"><a href="#系统级I-x2F-O" class="headerlink" title="系统级I&#x2F;O"></a><font color=salmon>系统级I&#x2F;O</font></h2><ul><li><p><font color=salmon>一切皆文件的终极体现。就是将所有的i&#x2F;o设备抽象成文件，提供统一的文件接口。 打开文件得到一个唯一的文件描述符。0 stdin 1 stdout 2 stderr. k seek记录文件位置。读文件，读过大小限制后会自己触发一个EOF 而非有这个东西。文件。普通文件，套接字文件，目录。还有一些其他的。每个进程都会记录当前的工作目录，可以通过 cd 命令改变</font></p></li><li><pre><code class="cpp">#include&lt;sys/types.h&gt;#include&lt;sys/stats.h&gt;#include&lt;fcntl.h&gt;int open();int close();int read();int write();// Richard Stevens 和他的RIO包 R.I.P 缓存的使用减少系统trap// 针对不同的应用情景选择不同的i/o 自己写包装更好的尤其是网络编程//</code></pre></li><li><p><font color=salmon>共享文件的实现，维护三个表，针对进程也有一些东西。同时fork的时候也是这样的。</font></p></li></ul></li><li><h2 id="并行编程"><a href="#并行编程" class="headerlink" title="并行编程"></a><font color=yellow>并行编程</font></h2></li><li><p><font color=cyan>线程，信号量是实现对全局变量的访问。防止竞争。竞争可以使用进程图来直观表示。同时要防止死锁。还有很多，好几个基于线程进程，i&#x2F;o复用的编程模型</font></p></li><li><p><font color=blue>Open MPI 和 mpch 似是两个不同的mpi 实现 我看的教程似乎是 mpich ,但是一般而言都可以如果只是学习的话 我不过上次我似乎安装的是 openmpi</font></p></li><li><h3 id="MPI"><a href="#MPI" class="headerlink" title="MPI"></a><strong><font color=Teal>MPI</font></strong></h3><p><strong><font color=DarkCyan>这里就把mpi的使用在这里写了。先补充一点前置知识。冯诺依曼体系。cpu主存分离。导致大多时钟时间去访存。进程就是一个程序的实例可以看成一个综合体包括I&#x2F;o设备即一组文件描述符表，然后主存，前两者共同由虚拟地址实现。此为被处理器表现为独享。多任务即并发。每一个执行时间片。上下文切换。因此硬件计算的优化集中在对冯诺依曼体系的优化大致有如下几个。<br>Cache 在主存与寄存器之间设置三层高速缓存，SRAM，利用局部性原理<br>虚拟地址，可以看作讲主存作为磁盘文件的Cache.同时还有很多好处，如简化加载链接，提供更安全的地址守护等。<br>指令级并行<br>线程级并行TLP 细粒度多线程就是一个线程每执行他的一条指令就切换。粗粒度就是在遇到需要较长时间的指令才切换<br>SMT 同步多线程<br>SISD 单指令流 单数据流 SIMD 单数据多指令流 处理向量运算 大型简单计算GPU就是 处理图像 大量线程 具体可以在学习CUDA后补充<br>MIMD 两种常见类型 注意有多个处理单元即多个处理器 是异步的没有全局时钟 一个是共享内存系统，多个核共享一个内存系统，分布式内存系统，多个核——内存对。第一类具体有两个<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/UMA.png"                                     ><br>分布式 最常见的就是cluster集群 以太网连接的一组PC就是 而每一台本身可能是共享内存所以称为混合系统<br>互联网络，可以理解为连接结点的结构。性能依赖于信息读取传输，而这有由硬件的互联网络决定。<br>共享内存系统中有两个 总线bus和交叉开关矩阵crossbar容易理解总线结构简单固定 ，小规模时高效，当结点增多可能出现阻塞，抢夺，因为大小是固定的，无法调整。<br>CrossBar <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/Switch.png"                                     ><br>上图结构保证了不会出现信息覆盖<br>分布式网络互联结构 其实就是互联网本身的一些结构了<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/net.png"                                     ><br>带宽是衡量网络传输速度的，宽度就是讲网络划分为两部分最少的同时通信数量<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/net2.png"                                     ><br>延迟和带宽 两个指标<br>共享一致性问题以及解决方案 首先由于Cache 当一个x的内存值改变时，另一个核中cache里缓存的值可能没变。两种方法解决，监听总线和用目录记录。伪共享问题与cache 命中有关 尤其与cache大小有关 当一个核的工作区恰好覆盖一个缓存时，那么就会发生进程间跳跃地对缓存覆盖，最终其实没有共享，反而增加不命中率。<br>对于共享内存系统我们通常派生多线程，分布式我们派生多进程<br>SPMD 单程序多数据流 if(thread0&#x2F;process 0){}elif(1&#x2F;1){}的结构<br>共享内存中的问题：线程不确定性 通过 mutex 和 信号量来互斥实现 同时对于可重入函数的使用 对应的许多拥有static变量的函数就是线程不安全函数当多个线程调用时可能发生问题，解决方法可以是自己上锁或者调用对应库中的线程安全函数<br>分布式中的问题：最多的API就是解决消息传递的。而且其也可以在共享内存中使用，原理就是逻辑上讲共享空间分割为多个独立空间有点像虚拟空间的操作 通常这样的API包含一个send 一个recv函数 然后rank来唯一表示进程 然后缓冲区区分 然后0对应stdout 以及一些广播和归约函数，最常见API MPI message passing interface <br>输入输出问题 输入输出问题常常因为异步而具有不确定性这里有一些规范convention</font></strong></p><p><strong><font color=pink>习惯<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/convention.png"                                     >总结而言就是没有任何两个文件标识符在实际输入输出时交叉，各自分组线程独自管理<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/speedup.png"                                     >可扩展性 增加规模与同时增加核数线程数。效率不变<br>计时通常指程序开始到结束的时间</font></strong></p><p><strong><font color=Teal>并行程序设计步骤Foster方法<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/fosterm.png"                                     >注意就是平均分配 同时要注意凝聚如果下一个依赖于上一个就可以凝聚为一个任务</font></strong></p><ul><li><p><strong><font color=lightyellow>MPI详解</font></strong></p><ul><li><font color=salmon><a class="link"   href="https://www.geeksforgeeks.org/creating-an-mpi-cluster/" >MPI集群的搭建方法 感觉mpi 也逐渐成为其他更高层计算框架实现的底层原理 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></font></li><li><font color=yellow>通信子，通信子可以看作一组可以互相通信的进程，初始时有MPI创建了一组WORLD,可以创建多组。可以调用函数得知对应大小以及每一个的rank。</font></li><li><font color=yellow>Recv 与 Send函数的语义。各自有自己的缓冲区其实就是指定的存储区。tag用于互相匹配。有status结构来实际获取。匹配包括：同一个communicator，rank匹配。tag匹配。传输信息type匹配。接受去内存大于发送区。对于接受函数有两个宏量。MPI_ANY_SOURCE MPI_ANY_TAG 字面意思就是可以任意接受。发送没有 且一定要指定好comm</font></li><li><font color=yellow>MPI_Status参数获取实际传送的字节数。MPI_STATUS_IGNORE</font></li><li><font color=yellow>Send语义，可以阻塞，此时不返回。可以缓冲，放入内部存储器，然后返回。返回时并不知道是否成功发送。实际是如果发送信息小于默认的截止大小就缓存，否则就阻塞。Recv一定阻塞。可能出现悬挂</font></li><li><font color=yellow>前面的是点对点 还有可以广播的集合通有内置的操作和我们自定义的operator 同样的还有信息的传播 API</font></li><li><font color=yellow>线程可以理解为轻量级进程 一个正在运行的程序在一个处理器上的实例 编译器的话需要一个-lpthread 参数</font></li><li><font color=yellow>注意我们的这个全局变量需要定义在所有的函数外面然后 main 就是一个主线程</font></li><li><font color=orange>目前看来我们需要做到就是明白原理，然后学下具体的API  </font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li></ul></li><li><p><font color=yellow></font></p><ul><li><font color=yellow></font></li><li><font color=yellow></font></li></ul></li></ul></li><li><h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a><font color=slateblue>CUDA</font></h2><ul><li><font color=slategrey>CPU 芯片，其实L3缓存占占据了最大的位置 <br>重要的任务就是判断是任务间是否独立 如果独立可能才可以分离task 就是指的一些指令和数据的集合<br>task parallelism 关注多核上的函数并行 data parallelism 关注多核上的数据并行 cuda、主要解决data parallelism <br>核利用率的问题</font></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;计算机科学中的自然原理&quot;&gt;&lt;a href=&quot;#计算机科学中的自然原理&quot; class=&quot;headerlink&quot; title=&quot;计算机科学中的自然原理&quot;&gt;&lt;/a&gt;计算机科学中的自然原理&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; cl</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
</feed>
