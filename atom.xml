<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Winter&#39;s</title>
  
  
  <link href="https://spikeihg.github.io/atom.xml" rel="self"/>
  
  <link href="https://spikeihg.github.io/"/>
  <updated>2024-12-31T06:27:00.065Z</updated>
  <id>https://spikeihg.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>我和我的十六毫米摄影机</title>
    <link href="https://spikeihg.github.io/2024/12/31/%E4%BD%A0%E5%A5%BD%E5%93%87/"/>
    <id>https://spikeihg.github.io/2024/12/31/%E4%BD%A0%E5%A5%BD%E5%93%87/</id>
    <published>2024-12-31T05:11:04.000Z</published>
    <updated>2024-12-31T06:27:00.065Z</updated>
    
    <content type="html"><![CDATA[<h2 id="我和我的十六毫米摄影机-（最后）"><a href="#我和我的十六毫米摄影机-（最后）" class="headerlink" title="我和我的十六毫米摄影机  （最后）"></a>我和我的十六毫米摄影机  （最后）</h2><p>应该怎么讲呢<br>想要给你拍一部电影<br>赶在夏天结束以前<br>因为我总感觉<br>时间不多了<br>你的时间<br>还有<br>我的时间<br>我做梦的时候越来越多<br>清醒的时候越来越少<br>而做梦时是拍不好电影的</p><p>也许现在就是最好的季节<br>光线，温度，一切都适合<br>你我心情，恰到好处<br>你是最好的<br>你的眨眼，叹气，躲闪目光<br>你的微笑都好<br>镜头却有限<br>我该怎样组织有限的胶卷<br>为你创造某种我希望能永恒的画面</p><p>我努力调整感官与摄影机<br>镜头真实清晰<br>我的双眼逐渐模糊<br>记住你是镜头的事<br>做梦是我的事<br>你的样子朦胧的，正好入梦</p><p>梦里，你是我生命电影的主角<br>我的记忆是一卷与你一起衰老的胶片<br>梦里，数不清的梦里<br>我把时间逆转，故事拆解<br>我将你的样子逐帧描绘<br>我把无数画面分离，聚集，聚集又分离<br>只为和你再次相遇<br>在某个夏天的结尾</p><p>也许只有在发黄的胶卷里<br>我们才能一直在一起<br>随时间磨损<br>直到再也分不清彼此<br>我好想和你继续这样的错觉 </p><p>或者，我们可以继续这样的错觉<br>我想听你讲<br>故事是怎样开始的<br>小花是怎样变成小花的，爱是什么形状<br>我想听你讲<br>故事又是怎样结束的<br>在胶卷耗尽的时候，<br>或是时间最终耗尽我们的时候</p><h3 id="Adios"><a href="#Adios" class="headerlink" title="Adios"></a>Adios</h3><h3 id="你好2025"><a href="#你好2025" class="headerlink" title="你好2025"></a>你好2025</h3><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/adios.jpg"                                     ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;我和我的十六毫米摄影机-（最后）&quot;&gt;&lt;a href=&quot;#我和我的十六毫米摄影机-（最后）&quot; class=&quot;headerlink&quot; title=&quot;我和我的十六毫米摄影机  （最后）&quot;&gt;&lt;/a&gt;我和我的十六毫米摄影机  （最后）&lt;/h2&gt;&lt;p&gt;应该怎么讲呢&lt;br&gt;想要</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>渡口牛仔</title>
    <link href="https://spikeihg.github.io/2024/10/14/%E6%B8%A1%E5%8F%A3%E7%89%9B%E4%BB%94/"/>
    <id>https://spikeihg.github.io/2024/10/14/%E6%B8%A1%E5%8F%A3%E7%89%9B%E4%BB%94/</id>
    <published>2024-10-13T16:05:40.000Z</published>
    <updated>2024-10-26T11:54:15.781Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Space-Cowboy"><a href="#Space-Cowboy" class="headerlink" title="Space Cowboy"></a>Space Cowboy</h2><p><img                       lazyload                     src="/images/loading.svg"                     data-src="D:\github.1.0\My_blog_hexo\source\images\cowboy2.png"                                     ></p><h2 id="感谢SpaceX的所有人，让我离成为太空牛仔又近了一步-ω"><a href="#感谢SpaceX的所有人，让我离成为太空牛仔又近了一步-ω" class="headerlink" title="感谢SpaceX的所有人，让我离成为太空牛仔又近了一步 ^ω^"></a><font color =pink>感谢SpaceX的所有人，让我离成为太空牛仔又近了一步 ^ω^</font></h2><h2 id="哎-老了-too-old"><a href="#哎-老了-too-old" class="headerlink" title="哎 老了  too old"></a>哎 老了  too old</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Space-Cowboy&quot;&gt;&lt;a href=&quot;#Space-Cowboy&quot; class=&quot;headerlink&quot; title=&quot;Space Cowboy&quot;&gt;&lt;/a&gt;Space Cowboy&lt;/h2&gt;&lt;p&gt;&lt;img  
                     la</summary>
      
    
    
    
    
    <category term="职业规划" scheme="https://spikeihg.github.io/tags/%E8%81%8C%E4%B8%9A%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>云</title>
    <link href="https://spikeihg.github.io/2024/09/02/%E4%BA%91%E5%90%96/"/>
    <id>https://spikeihg.github.io/2024/09/02/%E4%BA%91%E5%90%96/</id>
    <published>2024-09-02T07:41:08.000Z</published>
    <updated>2024-11-16T08:33:57.342Z</updated>
    
    <content type="html"><![CDATA[<h1 id="拾云"><a href="#拾云" class="headerlink" title="拾云"></a>拾云</h1><h2 id="写在扉页"><a href="#写在扉页" class="headerlink" title="写在扉页"></a>写在扉页</h2><p><em><strong>献给我家的小狗，如果他也喜欢看云并且讨厌java的话（我指的咖啡，确信 : -）</strong></em></p><p><em><strong>To my puppy , if he loves clouds and hates java(I mean the coffee , IMAO : -)</strong></em></p><h2 id="拾云-1"><a href="#拾云-1" class="headerlink" title="拾云"></a>拾云</h2><p><strong>拾云(Cloudsification)，一款给云分类，创建云朵图集的简单应用拾云 （拾取，拾取落叶，贝壳一样哈哈）移除电台LSD，加入绿日牌吗啡（又移除绿日牌吗啡，改为亚当版安慰剂，最后更改，移除亚当，改为迷星）</strong></p><hr><p><a class="link"   href="https://www.bilibili.com/video/BV1YW411h7Pk/?spm_id_from=333.337.search-card.all.click&vd_source=c8c7f6103570a31005f12d5a33a60b47" >绿日吗啡 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link"   href="https://www.bilibili.com/video/BV1yR4y1A74Z/?spm_id_from=333.788.recommend_more_video.6&vd_source=c8c7f6103570a31005f12d5a33a60b47" >亚当安慰剂 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><h2 id="—-gt-apk下载地址点我点我-lt-—"><a href="#—-gt-apk下载地址点我点我-lt-—" class="headerlink" title="—- &gt;apk下载地址点我点我&lt;—-"></a>—- &gt;<a class="link"   href="https://www.bilibili.com/video/BV1Ts411G77N/?spm_id_from=333.337.search-card.all.click&vd_source=c8c7f6103570a31005f12d5a33a60b47" >apk下载地址 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><a href="/doc/%E6%81%B6%E6%84%8F%E8%BD%AF%E4%BB%B6(42%E7%A7%8D%E7%97%85%E6%AF%92%E7%86%AC%E5%88%B6).apk">点我点我</a>&lt;—-</h2><hr><h2 id="WARNING-郑重声明"><a href="#WARNING-郑重声明" class="headerlink" title="WARNING(郑重声明)"></a><font color=gold>WARNING(郑重声明)</font></h2><ul><li>首先，上面写在扉页的话，只是笔者模仿一些文学作品瞎扯的，笔者的小狗没喝过咖啡。但他很忧郁，时常仰望天空。</li><li>该应用只需要几个简单的权限，大概就是没事偷偷调动你的摄像头拍个照，或者看看你内存里qq的聊天信息，或者就是自爆。</li></ul><h2 id="ERROR-警告"><a href="#ERROR-警告" class="headerlink" title="ERROR(警告)"></a><font color = red>ERROR(警告)</font></h2><ul><li>该程序很糟糕，但是无毒无害，笔者已替大家提前品尝过了 (^ω^)</li></ul><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><ul><li>首先感谢组员@twilight，尽管他前期一直在划水，但是他后期依旧在划水，什么也没做。bro唯一的贡献就是实验课帮忙占位置。（报告就交给你了）</li><li>感谢组员@？！ 在数据集与模型训练上的贡献</li><li>感谢bro @43Cas @听风 提供的宝贵反馈意见</li><li>感谢笔者家的小狗，他将永久负责代码库的维护</li></ul><hr><ul><li>github仓库地址 <a class="link"   href="https://github.com/SpikeIHG/Cloudsification" >https://github.com/SpikeIHG/Cloudsification <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li></ul><p>下同github readme </p><h1 id="Cloudsification"><a href="#Cloudsification" class="headerlink" title="Cloudsification"></a>Cloudsification</h1><h2 id="cloudsfication-means-clouds-identification"><a href="#cloudsfication-means-clouds-identification" class="headerlink" title="cloudsfication means clouds identification"></a>cloudsfication means clouds identification</h2><ul><li><p>An app to help record and identify the clouds 一款使用本地模型和调用接口的云朵分类app</p></li><li><p>平台 : Android Studio  本地模型 : MobileNet V2 API接口 : gpt-4o-2024-08-06</p></li><li><p>更多应用有关配置参数可以查看仓库种的gradle配置文件</p></li><li><p>应用设计细节，详见doc</p></li></ul><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">├── LICENSE</span><br><span class="line"></span><br><span class="line">├── README.md # README</span><br><span class="line"></span><br><span class="line">├── doc   # demo and QaA</span><br><span class="line"></span><br><span class="line">│  └── outline.md  # 项目大纲</span><br><span class="line"></span><br><span class="line">  ├── log.md # 笔者的船长日记，从宁静号退役前的老习惯</span><br><span class="line"></span><br><span class="line">  ├── Reference.md # 技术参考汇总</span><br><span class="line"></span><br><span class="line">  ├── file # 详细的文案以及设计</span><br><span class="line"></span><br><span class="line">  └── image # 一些笔者画的设计草图</span><br><span class="line"></span><br><span class="line">└── .idea # 本地环境配置信息 </span><br><span class="line"></span><br><span class="line">├── .kotlin/error # kotlin 调试信息</span><br><span class="line"></span><br><span class="line">├── app # 程序主体</span><br><span class="line"></span><br><span class="line">├── assets # 笔者的临时文件及</span><br><span class="line"></span><br><span class="line">├── gradle # gradle 依赖</span><br><span class="line"></span><br><span class="line">├── test # as自身测试文件</span><br><span class="line"></span><br><span class="line">└── ...... 一堆配置文件</span><br></pre></td></tr></table></figure></div><ul><li>总之，笔者是临时花了一个多月编写本程序，作为一门实验课程的作业，代码肯定粗糙，欢迎fork 和issue </li><li>还有些东西想不起来了，这两天人有点生病 ~~~ 老矣。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;拾云&quot;&gt;&lt;a href=&quot;#拾云&quot; class=&quot;headerlink&quot; title=&quot;拾云&quot;&gt;&lt;/a&gt;拾云&lt;/h1&gt;&lt;h2 id=&quot;写在扉页&quot;&gt;&lt;a href=&quot;#写在扉页&quot; class=&quot;headerlink&quot; title=&quot;写在扉页&quot;&gt;&lt;/a&gt;写在扉页&lt;/h</summary>
      
    
    
    
    
    <category term="JAVA" scheme="https://spikeihg.github.io/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title>仿生主人会梦见电子狗吗</title>
    <link href="https://spikeihg.github.io/2024/05/29/%E7%BB%84%E5%8E%9F/"/>
    <id>https://spikeihg.github.io/2024/05/29/%E7%BB%84%E5%8E%9F/</id>
    <published>2024-05-29T03:29:24.000Z</published>
    <updated>2024-10-13T16:40:45.948Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于计算机组成原理的相关思考"><a href="#关于计算机组成原理的相关思考" class="headerlink" title="关于计算机组成原理的相关思考"></a>关于计算机组成原理的相关思考</h1><ul><li><font color=pink>想到一个故事，想到一个名字，借用仿生人会梦见电子羊吗——仿生主人会梦见电子狗吗。 大概就是通过一个人设计一个电子狗来简单地串联起计算机组成，尽可能自然地从逻辑上推理出计算机的组成，为什么有这些部件，为什么这样设计，问题都能够通过一种比喻的形式得到解答。</font></li></ul><h2 id="码距"><a href="#码距" class="headerlink" title="码距"></a><font color=salmon>码距</font></h2><ul><li><font color=pink>Diane使用的是从废品点淘来的机械零件，大多来自于第二废世战争的遗留。所以，这些古董零件不可避免具有很大的出错率，为了能过保证绝大多数情况下的成功传送，更近一步，甚至能够定位每一次传送中的错误并且相应地进行修改，Diane研究了传送的条文，提出加入冗余的信息，这就是校验码的提出。 本质 增加码距。利用1的个数，如果我们出错位数大多数都只是1位的情况下的话。多重奇偶校验，构建一个矩阵的思想 </font></li></ul><h2 id="所以，仿生添添会梦见电子我吗"><a href="#所以，仿生添添会梦见电子我吗" class="headerlink" title="所以，仿生添添会梦见电子我吗"></a><font color=gold>所以，仿生添添会梦见电子我吗</font></h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;关于计算机组成原理的相关思考&quot;&gt;&lt;a href=&quot;#关于计算机组成原理的相关思考&quot; class=&quot;headerlink&quot; title=&quot;关于计算机组成原理的相关思考&quot;&gt;&lt;/a&gt;关于计算机组成原理的相关思考&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=pink</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>关于计算机操作系统的一些思考</title>
    <link href="https://spikeihg.github.io/2024/05/11/%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"/>
    <id>https://spikeihg.github.io/2024/05/11/%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/</id>
    <published>2024-05-11T08:28:43.000Z</published>
    <updated>2024-10-13T16:14:12.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OS"><a href="#OS" class="headerlink" title="OS"></a><font color=pink>OS</font></h1><ul><li><font color=skyblue>最近正好在复习计算机系基础，这里就总结一些一些东西，同时记下我自己关于计算机系统学习的一点思考和体会，当然这个也是对CSAPP的再次回顾。</font></li><li><font color=salmon>发现一个奇巧的东西，先按ctrl然后按tab可以实现整个段的tab</font></li></ul><h2 id="补码横空出世"><a href="#补码横空出世" class="headerlink" title="补码横空出世"></a>补码横空出世</h2><ul><li><font color=yellow>简化了底层电路运算的设计，加减统一了，实现运算器的时候只需要根据加还是减法的标记位进行减数的取反。</font></li><li><font color=yellow>注意，对于无符号数和有符号数，他们的最大位为1其他位全0的数互为相反数。这种特殊情况值得注意一下。</font></li><li><font color=yellow>如果同时存在有符号数和无符号数，一个表达式里，会讲有符号提升为无符号，至于编译器确定字面值时，根据范围去确定有无符号，有点奇巧。但是最新标准中是符合直觉的</font></li><li><font color=yellow>浮点数的设计很巧妙，具体记住几个关键点就可以了。尾数省略了一个1，阶数一个bias，等于2^(k-1) -1k&#x3D;8&#x2F;12两种精度下的取值，同时全0和最大是非规格数 ，阶码为0 用于表示0。 以及首位为0的数保证一个平滑地过渡。阶码最大就是NaN 和无穷大的表示。同时 符号位决定这个异常是否通知。1通知 符号位1是负数，注意小数部分的转换成二进制的方法。乘基取余，然后顺序从上至下。 总算知道为什么忽略1可以提高表示范围了，因为一个二进制数的第一位一定是1。 知道一个ascii 中数字刚好对应十六进制表示的30H 开始</font></li><li><font color=yellow>注意按位运算 ~ 和逻辑运算 !</font></li><li><font color=yellow>基于位运算的认识，以及补码，无符号有符号运算的认识，我们可以设计加法器，这个也正式引入一些控制信号，ZF CF OF SF。在汇编语言中我们也会再次见到 这四个标志位的设置非常简洁巧妙</font></li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/ZF.png"                                     ></li><li><font color=yellow>浮点运算 对阶，检查阶数是否正确。总结一下，布尔代数的完整性和数学上的正确性，保证了计算机数据运算的合理，我们只需要做的就是从工程上实现布尔代数。 这一部分可以作为了解，锻炼思考能力，不用强行记忆。</font></li></ul><h2 id="汇编与机器指令"><a href="#汇编与机器指令" class="headerlink" title="汇编与机器指令"></a>汇编与机器指令</h2><ul><li><font color=yellow>今天才知道解释器是逐行翻译运行，微程序其实与机器指令也存在一种对应关系，具体就是指令的操作码相当于微程序的序号，操作数相当于参数 有点类似一种伪函数调用过程</font></li></ul><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul><li><font color=yellow>链接是一个相对新奇的东西，在我第一次接触的时候。现在回想一遍，感觉主要思考一个关于重定位的问题。以及最后的加载执行问题就可以了。不过感觉要想真正的完整的认识链接，还是需要在理解了虚拟内存的基础上。</font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li><li><font color=yellow></font></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;OS&quot;&gt;&lt;a href=&quot;#OS&quot; class=&quot;headerlink&quot; title=&quot;OS&quot;&gt;&lt;/a&gt;&lt;font color=pink&gt;OS&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;font color=skyblue&gt;最近正好在复习计算机系基础，这里就总结一</summary>
      
    
    
    
    
    <category term="Linux" scheme="https://spikeihg.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>JSGKD</title>
    <link href="https://spikeihg.github.io/2024/04/28/JSGKD/"/>
    <id>https://spikeihg.github.io/2024/04/28/JSGKD/</id>
    <published>2024-04-28T02:15:39.000Z</published>
    <updated>2024-04-30T03:22:05.005Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如果今天也是晴天"><a href="#如果今天也是晴天" class="headerlink" title="如果今天也是晴天"></a><font color=pink>如果今天也是晴天</font></h1><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/20s.jpg"                                     ></p><p><font color=orange>村平处女作——《四人分别华尔兹》 大致构思了一下</font></p><p>感觉五六十年代的乡村音乐和越战场景绝配。 库布里克果然大师。</p><p>hello vietnam 。 魔幻世界里的丛林不一定在拉美，也可以在东南亚，越南。献给在宏大叙事下竭力骂人的个体。</p><p><font color =pink>《西贡后摇》</font>&gt; 老早的想法了，用现有电影拼接出一部新的电影出来。可以命名为 《村平狂想曲》001 号 002号 这样遍下去 ，因为难得取名 或者说因为是拼接的作品所以存在无限多的解释，给予所有观众最大的自己理解空间。 看来确实是一项大工程 需要反复看一些候选电影 分析很多段落镜头 然后记忆 联想合乎母题的其他镜头。 good 对脑子里的电影回收再利用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如果今天也是晴天&quot;&gt;&lt;a href=&quot;#如果今天也是晴天&quot; class=&quot;headerlink&quot; title=&quot;如果今天也是晴天&quot;&gt;&lt;/a&gt;&lt;font color=pink&gt;如果今天也是晴天&lt;/font&gt;&lt;/h1&gt;&lt;p&gt;&lt;img  
               </summary>
      
    
    
    
    
    <category term="NATURE" scheme="https://spikeihg.github.io/tags/NATURE/"/>
    
  </entry>
  
  <entry>
    <title>AI_Singer</title>
    <link href="https://spikeihg.github.io/2024/04/12/AI-Singer/"/>
    <id>https://spikeihg.github.io/2024/04/12/AI-Singer/</id>
    <published>2024-04-12T09:16:48.000Z</published>
    <updated>2024-11-09T13:44:13.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AI-SINGER"><a href="#AI-SINGER" class="headerlink" title="AI_SINGER"></a>AI_SINGER</h1><h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><p>罗列相关链接，总结一些FAQ</p><ul><li>训练的时候改一下保存模型的step间隔 400 或者600 也许可以 </li><li>拼接音轨</li><li>后期处理 bandlab 也许不错 melodyne 修音软件 似乎更好一点。可以学学</li></ul><h2 id="PEFT-参数高效微调"><a href="#PEFT-参数高效微调" class="headerlink" title="PEFT 参数高效微调"></a>PEFT 参数高效微调</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a><font color=pink>背景</font></h3><ul><li>深度学习的研究中出现了许多大型预训练模型，例如GPT-3、BERT等，这些模型可以在多种自然语言处理任务中取得优异的性能表现。然而，这些大型预训练模型的训练成本非常高昂，需要庞大的计算资源和大量的数据，一般人难以承受。这也导致了一些研究人员难以重复和验证先前的研究成果。为了解决这个问题，研究人员开始研究Parameter-Efficient Fine-Tuning (PEFT)技术。PEFT技术旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，PEFT技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。</li></ul><h3 id="总览"><a href="#总览" class="headerlink" title="总览"></a><font color=pink>总览</font></h3><ul><li>冻结预训练模型的某些层并仅微调特定于下游任务的最后几层</li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/peft.png"                                     ></li></ul><h3 id="常见的几种方法的基本思想与原理简介"><a href="#常见的几种方法的基本思想与原理简介" class="headerlink" title="常见的几种方法的基本思想与原理简介"></a><font color=pink>常见的几种方法的基本思想与原理简介</font></h3><ul><li><p>BitFit </p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/bitfit.png?raw=true"                                     ></p><p>优点：代码简单，原理简单。 缺点：优化效果不清楚，具体效果随着不同模型的参数规模变化较大，不具有太多的实际用处。</p></li><li><p>prefix tuning</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/prefix.png?raw=true"                                     ></p><p>Prefix Tuning方法由斯坦福的研究人员提出，与Full-finetuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。同时，为了防止直接更新Prefix的参数导致训练不稳定的情况，他们在Prefix层前面加了MLP结构(相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果)，训练完成后，只保留Prefix的参数。</p></li><li><p>prompt tuning</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/prompt.png?raw=true"                                     ></p><p>该方法可以看作是Prefix Tuning的简化版本，只在输入层加入prompt tokens，并不需要加入MLP进行调整来解决难训练的问题。同时还有一个区别就是，prompt tuning只在输入层加入，而prefix 是在transformer计算的每一层都加入。同时，prompt tuning 又分为soft , hard prompt 两种，区别就是hard 通常初始化的prompt是明确指定了与下游任务有关的。soft可以理解为泛化的。同时prompt tuning 在参数规模增大时表现出的效果更加突出。</p><ul><li><p>p tuning<br><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/pt.png?raw=true"                                     ></p><p>用MLP+LSTM的方式来对prompt embedding进行一层处理</p></li></ul></li><li><p>Adapter tuning</p><p>如下图所示的Adapter结构，将其嵌入Transformer的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter结构进行微调。</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/adap.png?raw=true"                                     ></p><p>存在一个问题，因为额外引入的一个adapter层，可能导致最终推理时长增加。</p></li><li><p>lora</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/lora.png?raw=true"                                     ></p><p>核心思想就是将权重更新的Δw分解为两个低秩小矩阵。 在训练时，我们同时走两条通路，一条Δw，另一条就是ab矩阵，最后优化时我们只更新ab矩阵，最后合并回去。具有推理时没有额外计算开销的优点。prompt系列会额外引入参数。</p></li><li><p>IA</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/ia.png?raw=true"                                     ></p><p>原理简言之，就是加入四个可学习向量处理原本transformer中的k,v,q,ffn 。 优化时只改变这几个可学习向量。最终也没有额外计算开销。</p></li></ul><h3 id="效果总览"><a href="#效果总览" class="headerlink" title="效果总览"></a>效果总览</h3><p><img                       lazyload                     src="/images/loading.svg"                     data-src="https://github.com/SpikeIHG/SpikeIHG.github.io/blob/master/images/fitt.png?raw=true"                                     ></p><h3 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h3><p>目前使用最为广泛的peft的现成库是hugging face的peft，高度集成，调用方便，具体使用的时候查看相关的文档。</p><h3 id="更多思考"><a href="#更多思考" class="headerlink" title="更多思考"></a>更多思考</h3><p>上述所有的方法，具体应用时存在的问题，以及超参数的选择都存在很大的调整空间，这里提供几个参考资料供想要深入探索的群友参考</p><p><a class="link"   href="https://arxiv.org/pdf/2303.15647" >更多的方法与更加具体的原理 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link"   href="https://www.bilibili.com/video/BV1Y8411k7yD/?spm_id_from=333.788&vd_source=c8c7f6103570a31005f12d5a33a60b47" >一些方法的具体代码演示 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link"   href="https://link.zhihu.com/?target=https://github.com/huggingface/peft" >peft的代码实现 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a class="link"   href="https://arxiv.org/pdf/1902.00751" >peft的起源 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><h2 id="谢谢大家-：）"><a href="#谢谢大家-：）" class="headerlink" title="谢谢大家 ：）"></a>谢谢大家 ：）</h2><p>PS : 上次没有解答的关于lora 为什么分解成两个矩阵可以优化</p><p><a class="link"   href="https://arxiv.org/pdf/2106.09685" >https://arxiv.org/pdf/2106.09685 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>总结一下就是：是将Δw矩阵分解，但是是一个向低维空间投射，也就是说降秩。也就是说lora有一个假设的前提就是我们微调一个矩阵降秩后的矩阵也可以达到接近的效果。</p><ul><li><p>idea  创建虚拟人格 举个例子 用户自定义加入一个爱观测星空的特质 然后可能设置一个遗忘机制 如果超过多久的对话中没有提及这个特质  就是说 是的 我都很久没有观测星空了</p></li><li><p>github界面的搜索 就是 点击一下变量名就可以了 我去</p></li><li><p>alt + enter 键解决导包 自动导包</p></li><li><p>desc 降序 或者 描述缩写</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AI-SINGER&quot;&gt;&lt;a href=&quot;#AI-SINGER&quot; class=&quot;headerlink&quot; title=&quot;AI_SINGER&quot;&gt;&lt;/a&gt;AI_SINGER&lt;/h1&gt;&lt;h2 id=&quot;Overall&quot;&gt;&lt;a href=&quot;#Overall&quot; class=&quot;he</summary>
      
    
    
    
    
    <category term="DL" scheme="https://spikeihg.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>Bash</title>
    <link href="https://spikeihg.github.io/2024/04/11/Bash/"/>
    <id>https://spikeihg.github.io/2024/04/11/Bash/</id>
    <published>2024-04-11T08:08:31.000Z</published>
    <updated>2024-04-11T08:49:47.676Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bash-脚本总结"><a href="#Bash-脚本总结" class="headerlink" title="Bash 脚本总结"></a>Bash 脚本总结</h1><h2 id="命令总结"><a href="#命令总结" class="headerlink" title="命令总结"></a>命令总结</h2><ul><li><p>多命令执行 ; 顺序执行 &amp;&amp; ，前面执行成功后后面才会执行， || 前面错误后面才会执行</p></li><li><p>*拓展0个或者多个字符， ? 拓展一个任意字符 [] 匹配里面任意的字符 &gt; 重定向 &gt;&gt; 接着后面键加入 &lt; 作为命令的输入</p></li><li><p>readonly 关键字 来声明变量 让变量作为一个只读变量 unset可以删除变量</p></li><li><p>printenv 和 env都可以查看环境变量</p></li><li><p>$0   $1  $2  $3 0 代表的是脚本的名字 1 2 3 分别代表说输入到命令行 的参数名</p></li><li><p>read  read varg 命令可以读取对应的的和终端输入并存入对应的变量中</p></li><li><p>关系比较符号 -eq -ne -lt -gt -ge -le ! &amp;&amp;</p></li><li><p>算术表达式 $(()) $ 这个就是用来引用值的</p></li><li><p>条件控制语句 就是使用 if [] ; then  注意一定还有一个 fi 作为结束 </p></li><li><p>for循环 注意一样的 还是有一个done作为一个结束的标志</p></li><li><p>#!&#x2F;bin&#x2F;bash</p><p>for i in {1..5}; do<br>if [ $i -eq 3 ]; then<br>    continue<br>fi<br>echo “循环次数：$i”<br>if [ $i -eq 4 ]; then<br>    break<br>fi<br>done</p></li><li><p>还有一个函数 有需求再去学习</p></li></ul><h2 id="感觉还是应该需求为导向-想要是实现什么功能的-时候就去看一看"><a href="#感觉还是应该需求为导向-想要是实现什么功能的-时候就去看一看" class="headerlink" title="感觉还是应该需求为导向 想要是实现什么功能的 时候就去看一看"></a><font color=salmon>感觉还是应该需求为导向 想要是实现什么功能的 时候就去看一看</font></h2><ul><li>awk 似乎是一个有趣的指令  来处理文本呢 男泵  seq 还有一个seq 流处理指令以及一些简单的文本处理指令   &amp;&gt; </li><li>著名的这个垃圾桶位置 &#x2F;dev&#x2F;null</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Bash-脚本总结&quot;&gt;&lt;a href=&quot;#Bash-脚本总结&quot; class=&quot;headerlink&quot; title=&quot;Bash 脚本总结&quot;&gt;&lt;/a&gt;Bash 脚本总结&lt;/h1&gt;&lt;h2 id=&quot;命令总结&quot;&gt;&lt;a href=&quot;#命令总结&quot; class=&quot;headerli</summary>
      
    
    
    
    
    <category term="Linux" scheme="https://spikeihg.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Fine_Tuning</title>
    <link href="https://spikeihg.github.io/2024/03/25/Fine-Tuning/"/>
    <id>https://spikeihg.github.io/2024/03/25/Fine-Tuning/</id>
    <published>2024-03-25T08:32:02.000Z</published>
    <updated>2024-04-11T12:43:48.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Fine-tuning-practice"><a href="#Fine-tuning-practice" class="headerlink" title="Fine_tuning practice"></a>Fine_tuning practice</h1><ul><li><p>找点事做，算是。哎，道路曲折。</p></li><li><p><font color=salmon>主机huggingface password iW7GHBpZ</font></p></li><li><p>现在基本上是使用一个预训练模型加上一个prompt 或者是fine tuning</p></li><li><p>transformer 一个生态系列 很多的库 Lora </p></li><li><p>使用的是miniconda</p></li><li><p>后运行job的指令 第一个就是末尾加&amp; 第二个就是ctrl+z stop他和current job然后使用一个bg 指令 jobs 显示当前进行i的job 然后还有 fg指令前台化</p></li><li><p>pip 或者整个虚拟环境的 一个原理就是每个包所具有的依赖都是存在于对应的包里面 然后下载的时候要确定对应的 解释器</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/pipi.png"                                     ></p></li><li><p>离线下载就是下载好了文件 然后scp然后AutoModel.from_pretrained()里面写文件夹的名字</p></li><li><p>三个数据集合 </p></li><li><p>training set 用于训练更新参数的</p></li><li><p>test set 用于最终评价的 不用于 改变参数</p></li><li><p>validation set 可以理解为中间的一个模拟测试 来反馈我们的超参数设置情况。</p></li><li><p>数据集的加载 使用的是dataset 这个包</p></li><li><p>数据集的划分 train_test_split 按照比例划分</p></li><li><p>选取 用 select 过滤用 filter加lamda 函数</p></li><li><p>预处理 使用map 方法 结合tokenizer </p></li><li><p>存放就是 load and save</p></li><li><p>加载本地文件 ——  可以直接加载整个文件夹 对</p></li><li><p>很多方法 殊途同归 然后可以自己写脚本来</p></li><li><p>tokenizer 中 inputs_id 就是转换后的唯一id attentino_mask 就是判断改数字是否是填充padding type——id就是确定这个句子是第几个。</p></li></ul><h2 id="显存优化"><a href="#显存优化" class="headerlink" title="显存优化"></a>显存优化</h2><h2 id="CSAPP-LAB-3-attack-lab"><a href="#CSAPP-LAB-3-attack-lab" class="headerlink" title="CSAPP LAB 3 attack lab"></a>CSAPP LAB 3 attack lab</h2><ul><li><p>几个编译选项的意思 可以使用 man gcc进行查找 &#x2F;keyword n 就是到下一个匹配项， N上一个匹配项</p></li><li><p>top 查看机器进程情况的时候 ， 可以使用 c 查看详细命令</p></li><li><p>占个坑 想学一学基本的shell 脚本<a class="link"   href="https://blog.csdn.net/qq_29864051/article/details/132650005" > <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>解决requirement already satisfied 的方法就是使用target指定包的存放地址 一般再这个lib&#x2F;python3.x&#x2F;site-package里面</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/eason.png"                                     ></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Fine-tuning-practice&quot;&gt;&lt;a href=&quot;#Fine-tuning-practice&quot; class=&quot;headerlink&quot; title=&quot;Fine_tuning practice&quot;&gt;&lt;/a&gt;Fine_tuning practice&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
    <category term="DL" scheme="https://spikeihg.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>GoMars</title>
    <link href="https://spikeihg.github.io/2024/03/05/GoMars/"/>
    <id>https://spikeihg.github.io/2024/03/05/GoMars/</id>
    <published>2024-03-05T00:37:12.000Z</published>
    <updated>2024-03-14T15:31:08.480Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GoMars"><a href="#GoMars" class="headerlink" title="GoMars"></a>GoMars</h1><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><ul><li>fortran syntax __ % like dot . in c++</li><li>isend 必须使用wait 以为我们需要保证这个已经完成communicte isend + wait  &#x3D; send  sendres就是两个的简单集合</li><li>vscode 可以使用这个alt+left right 进行trace back and forth</li><li>几个问题 第一个 什么是incremental builds</li><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GoMars&quot;&gt;&lt;a href=&quot;#GoMars&quot; class=&quot;headerlink&quot; title=&quot;GoMars&quot;&gt;&lt;/a&gt;GoMars&lt;/h1&gt;&lt;h2 id=&quot;Prerequisite&quot;&gt;&lt;a href=&quot;#Prerequisite&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="HPC" scheme="https://spikeihg.github.io/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>6.S081</title>
    <link href="https://spikeihg.github.io/2024/02/29/6-S081/"/>
    <id>https://spikeihg.github.io/2024/02/29/6-S081/</id>
    <published>2024-02-29T11:32:09.000Z</published>
    <updated>2024-04-09T03:49:10.801Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Operating-System"><a href="#Operating-System" class="headerlink" title="Operating System"></a>Operating System</h1><ul><li><p>退出qemu ctrl x a . 注意还有一个点号！！！</p></li><li><p>在google中使用ctrl shift q 使用google scholar!。</p></li><li><p><font color=lavender>需要永久改变vim的配置 可以修改 ~&#x2F;.vimrc 文件</font></p></li><li><p><font color=orange>Spack 是一个包管理系统 主要用于很多科学专业的计算使用  $foo is a variable while $(foo) is a result of a command 变量名可以用{} 包围着 <br> 脚本中常见的if 中对文件使用的短句命令 -d foo 如果foo 存在且为目录 -e 如果存在 -f 如果存在且为普通文件  pushd 指令就是在程序执行的时候对所在的目录位置存在一个栈 ，pushd 后这个目录就位于top位置同时也变成现在的工作目录 功能和cd 类似的 但是在连续多步骤跳跃的时候很有用，这是cd可能需要多次连续使用</font></p></li><li><p>还有一招可以切换桌面 就是win 然后触摸板使用四个指同时滑动。</p></li><li><ul><li>一般來説，我们的这个page有一个高为的全为0  的 guard page in threre  这个保护页没有相应的PTE 所以一旦超过了这个范围就会导致一个page fault<ul><li>所以实际上这个映射是很复杂的 可以是多对多 也可以是多对一 一对多 所有的这些编程技巧都是通OS 实现的</li></ul></li><li>np 代表这个进程的数量</li><li>id 是在process mod 里面确定</li><li>ids ide  是在 mesh里初始化的 大小似乎就是点的个数</li></ul></li><li><p>学习了几个奇淫巧计 </p></li><li><p>tmux 打开窗口 然后可以Ctrl+b c create the new windows and the ctrl+b % to split the window 拆分窗口 ctrl+b “ 水平拆分 ctrl + b o 来回切换 注意这里的一个使用方法就是先点击 ctrl + b 然后在点击其余的按键</p></li><li><p><a class="link"   href="https://stackoverflow.com/questions/10534798/debugging-user-code-on-xv6-with-gdb" >这是xv6 gdb tutorial <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>首先咱们使用的命令是 gdb-multiarch 注意中间的横线一定不能少</p></li><li><p>然后就是上面的网址里写的</p></li><li><p>回顾一下gdb的基本用法</p></li><li><p>tmux 的一些用法 prefix &amp; 关闭当前的windows </p></li><li><p>tmux attach 进入最近使用的session  </p></li><li><p>prefix 数字 进入对应的windows</p></li><li><p>find -name {} 方便查找文件</p></li><li><p>总结而言 一般我们先在一个windows运行 gdb qemu 然后一个新的window split 分为 gdb 和源码</p></li><li><p>prefix o 是在不同 pane 间移动</p></li><li><p>注意启动gdb 后需要使用 target remote localhost:26000来关联上</p></li><li><p>在 gdb 中我们使用 layout split 来同时显示源码和汇编</p></li><li><p>在vim 里面也可以拆分 : split 然后 ctrl + w+w 连按两次 w 来移动窗口 垂直是vsplit  在内部打开另一个文件就是使用 open + filename 注意文件路径</p></li><li><p>qemu 可以使用这个监视模式 ctlr a c</p></li></ul><h2 id="Trap"><a href="#Trap" class="headerlink" title="Trap"></a>Trap</h2><ul><li>为了在用户代码与内核间思华切换 隔离安全性。</li><li>mode 管理员模式其实只是能够读写一些控制控制急寄存器 然后是以哦个PTE中没有PTE_U 的模式位的页表条目</li><li>ecall 开始经过trampoline 的两个中间函数 然后使用sys_call 指c函数看跳转表 感觉中间的两个函数是进行一个硬件准备 然后都是 使用汇编语言写的 感觉这些中间工作差不多就是一些内存上下文保存  还有虚拟地址映射的切换之类的</li><li>系统调用指令 ecall 与普通的call 指令相似 传入的调用参数使用通用寄存器 riscv 就是a0 a1 a2 之类的 </li><li>两个额外的标记位 一个是a 表似乎是否是被读过 一个是d 表示是否是被写过 也许后面驱逐页的时候就需要</li><li>感觉目前接触的最终要的一个感觉就是riscv 尽可能的简单 从而能够给予操作系统设计者最大的自由发挥的空间 ecall实际上只完成三件事 一件事是保存pc 第二就是读取trampoline 到 pc 第三件事就是变换mode 从user到管理员模式 同时很多指令行为的考虑没有一个通用的模式 例如保存在tramframe只是因为假设可能存在没有普通函数栈的情况</li><li>所以我们抽象地来看其实内核模式与用户模式就是简单地在于虚拟空间以及内部使用的指令的不同 还有就是具体的位不同 通过几个底层的东西我们就可以抽象出如此美妙的东西 ！ </li><li>trampoline在用户和内核的虚拟页表都是相同的映射所以可以实现无缝切换</li><li>数据库 本身也是一种建立在集合运算上 的一种简洁的抽象。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Operating-System&quot;&gt;&lt;a href=&quot;#Operating-System&quot; class=&quot;headerlink&quot; title=&quot;Operating System&quot;&gt;&lt;/a&gt;Operating System&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;退出qem</summary>
      
    
    
    
    
    <category term="OS" scheme="https://spikeihg.github.io/tags/OS/"/>
    
  </entry>
  
  <entry>
    <title>New_Life</title>
    <link href="https://spikeihg.github.io/2024/01/01/New-Life/"/>
    <id>https://spikeihg.github.io/2024/01/01/New-Life/</id>
    <published>2024-01-01T04:35:55.000Z</published>
    <updated>2024-01-01T05:04:28.067Z</updated>
    
    <content type="html"><![CDATA[<p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/cowboy.gif"                                     ></p><h1 id="I-can-fix-that"><a href="#I-can-fix-that" class="headerlink" title="I can fix that"></a>I can fix that</h1><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/ww.jpg"                                     ></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img  
                     lazyload
                     src=&quot;/images/loading.svg&quot;
                     data-src=&quot;/../images/cowboy.gif&quot;</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>环形物语</title>
    <link href="https://spikeihg.github.io/2023/12/19/%E7%8E%AF%E5%BD%A2%E7%89%A9%E8%AF%AD/"/>
    <id>https://spikeihg.github.io/2023/12/19/%E7%8E%AF%E5%BD%A2%E7%89%A9%E8%AF%AD/</id>
    <published>2023-12-19T05:22:15.000Z</published>
    <updated>2024-02-28T08:31:33.190Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tales-from-the-Loop"><a href="#Tales-from-the-Loop" class="headerlink" title="Tales from  the Loop"></a><font color=deepskyblue>Tales from  the Loop</font></h1><h1 id="环形物语"><a href="#环形物语" class="headerlink" title="环形物语 "></a><font color=pink>环形物语</font> <br></h1><h2 id="环就在你心中"><a href="#环就在你心中" class="headerlink" title="环就在你心中"></a><font color=lightgreen>环就在你心中</font></h2><ul><li>一个假期，很多东西都忘记了，尤其是一些快捷键和命令之类的，以及blog的使用，这里就记下一些entries,在以后出现这种情况的时候可以快速复原。<ul><li>Blog : 首先powershell到博客文件夹， win x the i;  page up to find the cd command ; the hexo new to create  a md file with the title you give,hexo s just to start the local server, hexo g to generate your commits, hexo d to deploy you blog;</li><li>Google: 使用的一些东西 ctrl + tab tab之间进行切换  ctrl+ 数字 to jump to the given tab, ctrl + t create new tab, ctrl+ n create new window, alt+v translate the chosen word; </li><li>Windows: alt+tab switch the tasks; ctrl + win switch the background;</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Tales-from-the-Loop&quot;&gt;&lt;a href=&quot;#Tales-from-the-Loop&quot; class=&quot;headerlink&quot; title=&quot;Tales from  the Loop&quot;&gt;&lt;/a&gt;&lt;font color=deepskyblue&gt;Tale</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>APSP</title>
    <link href="https://spikeihg.github.io/2023/12/05/APSP/"/>
    <id>https://spikeihg.github.io/2023/12/05/APSP/</id>
    <published>2023-12-05T08:33:50.000Z</published>
    <updated>2023-12-13T08:36:45.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="APSP"><a href="#APSP" class="headerlink" title="APSP"></a><font color=deepskyblue>APSP</font></h1><ul><li><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a><font color=pink>Prerequisites</font></h2><ul><li><font color=yellow><a class="link"   href="https://www.bilibili.com/video/BV1YW411h7Pk/?spm_id_from=333.337.search-card.all.click&vd_source=c8c7f6103570a31005f12d5a33a60b47" >morphine <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 先来一剂Green Day 牌吗啡！<br>在结尾处，准备总结一下实验的一些基本环境配置和有用链接以及FAQ留个之后看到这个网页以及遇到问题的人，所以如果是有问题需要解决可以先看看结尾。保证可以复现乐 : ) </font></li><li><a class="link"   href="https://cmake.org/cmake/help/latest/guide/tutorial/A%20Basic%20Starting%20Point.html" >Cmake <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><font color=yellow>熟悉cmake 的常见函数或命令以及了解下makefile 的原理 有点搞笑 我们的这个prefix 的路径似乎不能使用 ~ 而必须使用绝对路径 </font></li><li><a href="/doc/%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E5%AF%BC%E8%AE%BA.pdf">OpenMP</a><font color=yellow>OpenMP</font><ul><li><font color=lightgreen>首先，我们知道对于floyd算法的三层for循环中 ,最外层k 是无法并行化的，因为每一个k的路径比较依赖于已有的路径，需要顺序进行更新，但是内层i,j循环在src dest一定时，k 可以按照任何顺序选取所以是可并行的。因此private(i,j).</font></li><li><font color=lightgreen>OpenMP 原理 就是启用多线程，具体运行的时候可以跑在多核上，并行运算。底层就是pthread 实现的</font></li><li><font color=lightgreen>平衡负载，我采用的是dynamic ，不过可以多试试看看，static guide 都可以尝试，默认的也可以，感觉每次迭代的计算量似乎是随机的但是总体是均匀的 感觉static 应该就可以，dynamic 还是用在计算量会增加的比较好</font></li><li><font color=lightgreen>线程数的设置，过多会增大合并开销，同时还存在内存分配问题，降低效率，数量过少会导致并行度不够，根据我的猜想，看CUDA简介的经验，使用一个和迭代数以及某些硬件属性数的倍数或者因子书。32 64 之类的。问题不大，实践是检验真理的唯一方式，多试试就可以了，试了再来补充。</font></li><li><font color=lightgreen>编译器优化猜想，首先就是编译器可能帮我做了loop unrolling ,估计是fully peel the loop有可能。 然后就是寻址方式，依照CSAPP 上的，对于一个定长二维数组，确定一些基指针，然后使用定长进行改变。减少访存。然后就是使用局部变量来保存一些结果减少内存访问。同时也可能使用了一些向量化操作(AVX指令等等）。常熟计算(constant propgation)编译时计算，也就是constexpr。将小函数进行内联，但是本例中似乎不存在。</font></li><li><font color=lightgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/omp.png"                                     ></font></li><li><font color=lightgreen>这个只是使用了最基本的for 内层循环并行，和负载动态分配</font></li><li><font color=lightgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/simd_omp.png"                                     ></font></li><li><font color=lightgreen>这个比较客观，在第一次基础上使用了omp的 simd优化最内层循环。4096的加速比接近baseline的100倍，当然我这里选择的是64线程数 使用128线程差距不大。不过我个人感觉没有找到更好的方法进行线程数的调参。不知道有没有除了顺序试错外更高效准确的判断方法，还待我考察，欢迎学长指教。</font></li></ul></li><li><font color=yellow><a class="link"   href="https://arxiv.org/pdf/1811.01201.pdf" >AVX参考 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>avx优化</font><ul><li><font color=lightgreen>使用的是AVX512，用一个 mask store 来实现比较运算。</font></li></ul></li></ul></li><li><p><font color=pink><a class="link"   href="https://www.jstage.jst.go.jp/article/transinf/E95.D/12/E95.D_2759/_pdf" >BFW <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>(算法优化——Blocked Floyd Algorithm</font></p><ul><li><font color=lightgreen>查找了一下资料，似乎这个可以提高数据的局部性，当然需要设置好所分的矩阵块的大小，使处理一个数据块的工作集内存大概等于 L2 cache .但是感觉实现这个算法本身加速比不是很明显，然后分块的话额外内存开销较多。（还有就是懒:) 所以就没有进行应用。</font></li></ul></li><li><p><font color=pink>Pthread接口实现多线程</font></p><ul><li><font color=lightgreen>可以，今天用pthread把内层两个循环并行处理了一下，加速比大概500倍。跑4096的图用了13s左右，我自己设置的线程数为<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/Re1.png"                                     >这个时候文件夹还没改名，当然但就算法运行时间大概11s左右</font></li><li><font color=lightgreen>使用局部变量优化了一下 大概8s 多一点<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/Re3.png"                                     ></font></li></ul></li><li><p><font color=pink>AVX512改写内层循环 速度大概2s提升 4倍</font></p><ul><li><font color=lightgreen>不过这里我有一个问题，就是理论上直接来看，应该会提升16倍，实际上只提升4倍左右，我自己猜测的原因是缓存问题，由于嵌套循环，存取的时候空间局部性不是很好，等后面有时间profile 一下。</font></li><li><font color=lightgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/unrool.png"                                     >循环展开64 后 线程数 90 接近突破2s</font></li><li><font color=lightgreen>记录下首次突破 2s 作了128的循环展开 然后将线程数调到了100<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/2s.png"                                     >后续测试感觉 64循环展开 和 128差不多了。感觉实际上我们的128循环展开可能效果还差一点，因为使每一个线程的工作负载变大了。</font></li><li><font color=lightgreen>感觉实在找不到什么可以优化的地方了（在我目前所学的知识范围内）<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/final.png"                                     >大概在1.85s左右 悲。有一个分块floyd算法，改善局部性，感觉就单独实现而言，确实可以通过减少工作集内存的范围来提高空间局部性。但是因为涉及到要重写pthread,感觉反而会增加线程创建的开销。因为需要不断迭代子方块，然后进行pthread_create和pthread_join。</font></li><li><font color=lightgreen>本来想学一学使用 vtune 来剖析一下，似乎集群没有装，然后数据scp 命令没有使用权限，所以没有profile很多对性能的猜测都是自己的直觉 乐！</font></li><li><font color=lightgreen>至于GPU的算法，看了一下，似乎没有看到集群有装CUDA，所以没打算写，看后面有没有时间参考下资料看一看。</font></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;APSP&quot;&gt;&lt;a href=&quot;#APSP&quot; class=&quot;headerlink&quot; title=&quot;APSP&quot;&gt;&lt;/a&gt;&lt;font color=deepskyblue&gt;APSP&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&quot;Prerequisites&quot;&gt;&lt;a</summary>
      
    
    
    
    
    <category term="hpc" scheme="https://spikeihg.github.io/tags/hpc/"/>
    
  </entry>
  
  <entry>
    <title>DL</title>
    <link href="https://spikeihg.github.io/2023/11/23/DL/"/>
    <id>https://spikeihg.github.io/2023/11/23/DL/</id>
    <published>2023-11-23T11:24:27.000Z</published>
    <updated>2023-12-05T15:35:17.243Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deep-learning-premier"><a href="#Deep-learning-premier" class="headerlink" title="Deep learning premier"></a><font color=pink>Deep learning premier</font></h2><p><font color =violet>志を受け継ぎ世界と戦う</font></p><ul><li><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a><font color=lightgreen>Transformer</font></h2><p>transformer是什么？变形金刚！！！ 好吧transformer 是用来解决seq2seq的一个模型。 seq2seq是一种模式，一些常见的情形：翻译，听译，语音辨识，听译 男泵 万恶之源 硬train一发 chatbot 之类的也是</p><p>BOS</p><ul><li><p>NLP QA模式 问题回答的一种模式</p></li><li><p>multi-label 就是每一个输入对象可能身上有多个标签</p></li><li><p>transformer 就是求解s2s的一个模型 encoder 和 decoder</p></li><li><p>residual connection 这个是与self attention 不一样的地方把input 和 output加起来</p></li><li><p>为什么能够应对seq2seq的情况</p><p>输入有两个部分 一部分来则于自己之前的输出</p><p>masked -attention 为什么需要</p><p>单纯看自己的输入作为输出的时候不可能停止下来，机，器，学。习。惯…… 首先我们需要准备以恶搞special toke as end(断) 所以原理就是让机器学习断，在该停止时候最大的概率输出end</p><p>上面是对于AT NAT是同时产生所有的输出 解决停止问题 有两个，第一个单独训练一个classifier来判断长度，方法二，传入很多begin token.忽略end 后的 输出 NAT 更好的并行化，可以控制长度 但是效果很难达到AT multi-modality</p><p>decoder 和 encoder的连接依靠一个叫做 cross-attention 的操作完成的就是 decoder 产生的向量q然后对每一个encoder的output k做kq，然后得到的新的v作为input丢到fc进行之后操作</p><p>train的时候采用分类的视角 计算向量之间的cross-entrophy 并且在训练情况下，我们给decoder的是正确的答案 就是一个监督 tearcher forcing </p><p>训练的一些 tips copy mechanism 例如chat-bot 将一些从来没有见过的词汇 直接进行复制 例如做摘要的时候 pointer network</p><p>TTS 语音合成 guide attention 就是规定 attention的顺序 有固定的过程 需要提前分享任务的特征monotonic attention</p><p>观察确定 我们的encoder 就是输出简单的数据 作为中间向量</p><p><font color=yellow>beam search</font> greedy path可能存在问题 局部最优不代表全局最优 类似于老师讲的王者问题 加入随机 decoder加一点 noise</p><p><font color=pink>BLEU score 用来检测inference 的指标但是训练的时候使用的是cross -entrophy 这两个没有关联的，没有相关性的blet score无法微分 一种策略使用 rl reinforcing learning reward与 agent来硬train一发</font></p><p><font color=cornsilk>在训练的时候加入一些错误的情况，scheduled sampling 也很直觉啊，面对错误的时候得到正确的这样学习应对错误的能力</font></p></li></ul></li><li><h2 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a><font color=deepskyblue>self-attention</font></h2><ul><li>q,k 矩阵学习 multi-head 就是分出多个 q 矩阵 然后得到bi,j 然后再处理成一个 positional embding , hand-crafted 目前的 目前是一个全新的领域 也可以学习。 self-attention 之间是没有位置关系的，然后q k 也不一定需要包含整个窗口</li></ul></li><li><h2 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a><font color=salmon>Batch normalization</font></h2><p>layer norm 是对同一个feature(sample)不同的dim 进行计算 mean和 deviation 而 batch norm 是对所有不同的feature 的同一个dim 进行处理</p><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/transf.png"                                     ></p><ul><li>position coding 位置资讯 </li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bert.png"                                     ></li></ul></li><li><p>实际设计中还可以在顺序上进行变换 上述大概就是encode 结构</p></li><li><p>decoder</p><ul><li>bos （begin special token）特殊的符号 标记开始</li><li>masked self- attention 和通常的attention 有一些区别</li><li>为什么 因为decoder是一个一个产生的 在产生前面的输出时，后面的右边的输出还没有产生 无法考虑</li><li>autoregressive</li><li>自己不断运作，输出再次作为输入然后一直运行  存在停止的问题 方法就是设置另一个token 有时候与begin其实是同一个 实际实现可能是一个one-hot 向量，end也是自己产生的男泵</li><li>NAT non-autoregressive</li></ul></li><li><h2 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a><font color=white>pytorch</font></h2><ul><li><p>总体流程就是，先定义我们的数据， dataset可以理解为存储我们的数据，feature 和 label ，然后dataloader 以我们想要的方式加载数据，自定义操作，包括batch ,还有 transform 等等。</p></li><li><p>然后就是定义 目标函数 loss ，以及一个优化器 optimizer ，有了之后定义 响应的train 和 test 函数 ，最后还有对模型的保存以及调用 和实际运用。</p></li><li><p>tensor (array)统一的却应用广泛的数据结构 ，就是一个高维数组，最外面的维度为dim&#x3D;0 注意顺序 然后基本上有外面可能用到的所有操作，只需要用时自己去找api 查资料就可以了。</p></li><li><p>dataloader &amp; dataset</p><ul><li>torch.utils.data.dataset torch.utils.data.dataloader 两个基本的提供的模块除此之外很多专门领域的domain-specified 的库都有自己的相关的。 注意一般来说,然后从我们的视角来看，就是一个数组 索引得到feature and label 同时得到。a,b&#x3D;dataset[index]大概这样，然后label 一般存在一个csv文件（逗号分隔文件里面）然后就自定义__init—— len getitem三个函数init 的时候传入的 是一个annotation_file 和 dir 两个都是字符串，然后两个transform 函数一个transform 一个 target_transform</li><li>pandas 是一个数据处理  比如读取csv_file 之类 os 就是处理字符串之类的</li><li>dataloader 的每一次迭代返回两个tensor 对应feature和label</li><li>transform modify featu target_transform modify label<ul><li>常见的totensor 就是实现normalize 并且使得元素在0-1 之间。 vector lambda就是自定义操作</li></ul></li></ul></li><li><p>build the network</p><ul><li>torch.nn 所有存在的neuralnet的父类。 module其实就可以理解为layer 一个神经网络就可以理解为 a module itself that consists of other modules(layers) from torch import nn</li><li>第一步先check一下可不可以用硬件加速</li><li>然后继承 nn.Module 然后定义两个东西 init 和 forward方法</li><li>我们来break down and see every module拆解看看每一层<ul><li>flatten 在图形里面用来将一个image 矩阵转换为一个一维数组</li><li>linear 层就是进行一个线性转换 我们需要预先输入一个weigth 和 bias参数 然后对feature 进行变换</li><li><font color=pink>这里一个额外的补充知识点 就是call 特殊方法可以实现将类作为i函数调用 这就是为什么我们可以调用model，将数据作为参数输入进去 linear其实就是继承了nn.Module</font></li><li>妈的，我逐渐反应过来了，linear层里面的bias ,weight都是默认输出话，随机的其实，因为这个是我们要学习的参数，所以随机初始化就可以了。不需要什么预置输入</li><li>这里提前所以个东西 Relu 可以解决sigmoid 和tanh的过饱和问题缓解过拟合问题 也是目前默认激活函数</li><li>flatten默认改变的是最里面的层 没有改变channel 和batchsize</li><li>nn.sequential 就是一个layer的顺序容器 其实经过一个sequential 我们就可以发现输出了y 相当于fully connected network 啊男泵</li><li>nn.softmax 如果分类还需要过一层 注意通常要指定softmax操作的维度</li><li>parameter() named_parameters()这两个方法可以让神经网络追踪传入的参数哦从而进行学习</li><li>终于逐渐理解了</li></ul></li></ul></li><li><p>automatic differentiation</p><ul><li><p>关于tensor 的矩阵乘法 要了解到tensor最后面的参数一定是列向量 这个有点差异</p></li><li><p>parameter 就是我们需要进行optimize的 weight bias 都是。对于这些参数，我们需要优化，所以我们需要预先设定require_gradient ,默认是false</p><ul><li><p>然后调用Function 类，这个在每一个tensort 的 grad.fn 属性里存这一个指针，表示该tensor所使用的反向传播计算时的函数</p></li><li><p>在tensor的grad 里面维护的是计算得到的梯度值</p></li><li><p><font color=orange>所以现在我们可以这样理解forward就是利用model来进行计算，backword就是train，所以后面我们训练完成的时候，需要使用 with torch.no_grad()的方法</font></p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/DAG.png"                                     ></p></li></ul><p>  现在来审视这个 DAG leaves is input tensor root is output tensor and the node is function !!!!!</p><ul><li>然后实际的是实现中我们使用了雅各布矩阵直接做矩阵乘法！！！</li></ul></li></ul></li><li><p><font color=violet>Optimizing model</font></p><ul><li>在上面的过程我们可以明白了，forward其实就是调用预测时候的算法，所以实际上就是直接调用我们的init 里面的层 然后返回就可以了</li><li><font color=cyan>在这里我们区分一下parameter ,就是在里面起作用的东西，hyperparameter 更像是一些设置配置参数 例如 learning rate ,batch size ,number of epochs</font><ul><li>loss function 。 对于回归问题我们通常使用的是MSELoss, 对于classification 我们通常使用的 是 negative  log likelihood NLLLoss 然后nn.crossentropyloss 就是 logsoftmax和NLLLoss的结合</li><li>我们前面学的adam rmsprop sgd 其实就是不同的optimizer 也就是minimize的时候使用的算法</li><li>实际运行的时候有三个步骤<ul><li>第一步就是先显式置零，同时自动累计grad,以免没重复加</li><li>第二步就是运行反向传播算法</li><li>第三步 调用.step()方法进行更新</li></ul></li><li>最后就是进行实现两个loop 一个train loop 一个 test loop</li></ul></li><li><font color=pink>Save and load the model</font><ul><li>组后要进行保存和加载的化调用响应的方法即可 ，然后注意两个东西就是.eavl() 模式的切换，记住这个是避免dropout和batch normalization 的一个东西</li></ul></li></ul></li><li><h2 id="computational-graph"><a href="#computational-graph" class="headerlink" title="computational graph"></a><font color=salmon>computational graph</font></h2><ul><li><p>反向传播</p><p>前向传播和反向传播 forward and backward torch的抽象 forward 可以理解为求值 从input 到output 从leaf到root 反向传播就是求误差，根据链式法则求取偏导然后更新参数进行optimize。</p></li><li><p>torch的实际实现过程</p><p>根据我们对neuralnet 的定义  一个sequential 里面有很多layer，然后forward就是调用这个module ,在进行forward pass的时候会进行两件事情，第一件事就是按照要求计算得到output tensor第二件事就是建立其DAG，每个结点就是对应的求导函数，在后续的backward pass中调用进行求导</p><p> what you run is what you differentiate.</p><ul><li>tensor 需要保存，用来求导</li><li>对于数学上不可求导的函数或者是未定义情况有指定的数学方式来处理</li><li>先要阻止gradient 可以更改text manager mode 以及.eval(),同时要想控制子图subgraph的性质 可以对单个tensor 调用 require_grad</li><li>torch.nn 中parameter默认要求导，中间变量都会求导</li><li>现在终于懂了</li><li>lossfunction 进行 backward()相当宇求梯度然后得到grad值，optimizer 则是根据不同的类型操作 grad 进行更新所有  parameter  .step()就是进行一步 。 .zero_grad()就是重置tensor</li></ul></li><li><p>梯度消失和爆炸 的原因</p><p>我们在计算梯度的时候采用的是反向传播求取雅各布矩阵，由于很多hidden layer都会采用一些激活函数对于sigmoid而言，其倒数最大值为0.25 如果乘以w 后得到的积仍然小于1 如果很多小于1累计起来可能造成靠近输入层的参数更新极为缓慢，当然输出层的影响较小，爆炸则是产生NaN 或者不稳定。</p></li></ul></li></ul></li><li><h2 id="transformer-is-all-you-need"><a href="#transformer-is-all-you-need" class="headerlink" title="transformer is all you need"></a><font color=deepskyblue>transformer is all you need</font></h2><ul><li><font color=yellow>正向的 rnn 。 双向的rnn 可以正反同时读取我们的，</font></li><li>LSTM 关键四个们 标量也是用来学习的 每个单元可以用来替代之前的neuron 然后只是输入需要四倍，因为三个们和一个输入 。使用的是sigmoid 函数表示打开的程度。实际的模型还会在输入中参考中间步骤的输入</li><li>BPTT learning train  的 方法。考虑时间的关系。</li><li>train的问题 error surface存在很平坦到很陡峭的分界限，所以存在参数的抖动。 clipping 解决方案，就是为gradient 做一个上界，如果超过就直接等于。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Deep-learning-premier&quot;&gt;&lt;a href=&quot;#Deep-learning-premier&quot; class=&quot;headerlink&quot; title=&quot;Deep learning premier&quot;&gt;&lt;/a&gt;&lt;font color=pink&gt;Deep l</summary>
      
    
    
    
    
    <category term="Deep Learning" scheme="https://spikeihg.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Winter</title>
    <link href="https://spikeihg.github.io/2023/11/21/Winter/"/>
    <id>https://spikeihg.github.io/2023/11/21/Winter/</id>
    <published>2023-11-21T08:00:38.000Z</published>
    <updated>2023-11-22T08:53:33.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Winter"><a href="#Winter" class="headerlink" title="Winter"></a><font color=violet>Winter</font></h2><blockquote><p>我是未来世界唯一的程序员。这是一条来自未来的讯息。在我的时代，支配世界的算法被我洞晓。过去的人啦，想要改变未来吗？去探索吧，这个世界背后运行的真理！—— 2079 . 12 . 24 </p></blockquote><p><strong><font color=deepskyblue>世界の仕組みについての真実を理解しました</font></strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Winter&quot;&gt;&lt;a href=&quot;#Winter&quot; class=&quot;headerlink&quot; title=&quot;Winter&quot;&gt;&lt;/a&gt;&lt;font color=violet&gt;Winter&lt;/font&gt;&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;我是未来世界唯一的程序员。这是</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>Algorithm</title>
    <link href="https://spikeihg.github.io/2023/11/02/Algorithm/"/>
    <id>https://spikeihg.github.io/2023/11/02/Algorithm/</id>
    <published>2023-11-02T02:11:09.000Z</published>
    <updated>2023-11-21T06:38:30.103Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a><font color=yellow>Algorithm</font></h1><ul><li><strong><font color=seagreen>最近点对问题</font></strong><ul><li><font color=pink>分治法求解 垂直线分割 实际使用时可以对应实际的问题进行优化 比如使用一个 strip  然后可以在递归的最底层加入一个全局的区间值的判断 双重循环 但是每一层都是O(n^1&#x2F;2)判断方法是否存在在特殊情况失效 例如几乎垂直或者水平的点对。所以可以单独的进行筛选 类似机器学习的算法的改进 多个维度通道 简化的一些思想 </font></li><li><font color=pink>对称性思想 每一个维度都是一样的</font></li><li><font color=pink>建表  对于频繁使用的信息进行见表索引检索哈希码 向量 量化处理</font></li><li><font color=pink>曼哈顿距离</font></li></ul></li><li><strong><font color=seagreen>概率分析与随机算法</font></strong><ul><li><font color=pink></font></li></ul></li><li><strong><font color=seagreen>正太分布问题思考</font></strong><ul><li><font color=pink>解决一种问题 就是关于某个中心点对称任何维度的可能性相同。 同时随着偏离中心概率减小 关键就在于与角度无关 非常的厉害</font></li></ul></li><li><strong><font color=seagreen>随机问题</font></strong><ul><li><font color=pink>随机生成一个数组的方法 一 可以为每个元素生成一个优先级 然后根据rank 进行排序</font></li></ul></li><li><strong><font color=seagreen>排队论</font></strong><ul><li><font color=pink>所有人排在一起，那个窗口空去哪个</font></li></ul></li><li><strong><font color=seagreen>同时获得最大值和最小值</font></strong><ul><li><font color=pink>同时维护两个值比较 n-1</font></li></ul></li><li><strong><font color=seagreen>活动安排问题</font></strong><ul><li><font color=pink>选择一个拥有最多活动的集合 集合的实践区间不存在重叠。使用贪婪算法，按照结束时间排序，然后每次都加入最早结束的时间 最核心的地方就是按结束时间思考，存在多解问题，原因在于开始阶段。</font></li></ul></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li><li><strong><font color=seagreen></font></strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Algorithm&quot;&gt;&lt;a href=&quot;#Algorithm&quot; class=&quot;headerlink&quot; title=&quot;Algorithm&quot;&gt;&lt;/a&gt;&lt;font color=yellow&gt;Algorithm&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;f</summary>
      
    
    
    
    
    <category term="Algorithm" scheme="https://spikeihg.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>佩索呀</title>
    <link href="https://spikeihg.github.io/2023/10/29/%E4%BD%A9%E7%B4%A2%E5%91%80/"/>
    <id>https://spikeihg.github.io/2023/10/29/%E4%BD%A9%E7%B4%A2%E5%91%80/</id>
    <published>2023-10-29T08:04:14.000Z</published>
    <updated>2024-05-28T03:46:03.259Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Darling-I-‘m-getting-older-亲爱的，我原来也会变老"><a href="#Darling-I-‘m-getting-older-亲爱的，我原来也会变老" class="headerlink" title="Darling . I ‘m getting older. (亲爱的，我原来也会变老)"></a>Darling . I ‘m getting older. (亲爱的，我原来也会变老)</h3><ul><li><p>Everyday poetry.</p></li><li><p><a class="link"   href="https://poets.us20.list-manage.com/track/click?u=e329a0cb6f08842f08a05d822&id=50cda6fc43&e=dce612a19d" >Sippokni Sia <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><a href="https://poets.us20.list-manage.com/track/click?u=e329a0cb6f08842f08a05d822&id=a7f92fa877&e=dce612a19d"><strong>Winnie Lewis Gravitt</strong></a></p><p>I am old, Sippokni sia. Before my eyes run many years, Like panting runners in a race. Like a weary runner, the years lag; Eyes grow dim, blind with wood smoke; A handkerchief binds my head, For I am old. Sippokni sia.Hands, once quick to weave and spin; Strong to fan the tanchi; Fingers patient to shape dirt bowls; Loving to sew hunting shirt; Now, like oak twigs twisted. I sit and rock my grandson. I am old. Sippokni sia.Feet swift as wind o’er young cane shoots; Like stirring leaves in ta falla dance; Slim like rabbits in leather shoes; Now moves like winter snows, Like melting snows on the Cavanaugh. In the door I sit, my feet in spring water. I am old. Sippokni sia.Black like crow’s feather, my hair. Long and straight like hanging rope; My people proud and young. Now like hickory ashes in my hair, Like ashes of old camp fire in rain. Much civilization bow my people; Sorrow, grief and trouble sit like blackbirds on fence. I am old. Sippokni sia hoke.</p><ul><li>我不知道人们所说的衰老是什么样子的，也许是透过镜子发现曾经的皮肤已经松弛下垂，也许当他们发现他们跑不过一个最小的孩子，但我感觉衰老是发生在一瞬间，仅仅一瞬间，就像过了一辈子，只是从前漫长的岁月从未被老去的忧伤笼罩，就像山顶的雾终于散去，时间失去了从前的神秘。</li><li>告别，寻找告别</li><li>春天总是一去不返</li><li>无法思考，无法摆脱，3点，神秘力量，是……</li></ul></li><li><p>Amy Lowell</p><ul><li><p>A enthralling person with all her persistence, intelligence , energy and fecundity.  “ The god made me  a business woman , but I made myself a poet .”</p></li><li><h1 id="The-Garden-by-Moonlight"><a href="#The-Garden-by-Moonlight" class="headerlink" title="The Garden by Moonlight"></a>The Garden by Moonlight</h1><p>BY <a class="link"   href="https://www.poetryfoundation.org/poets/amy-lowell" >AMY LOWELL <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>A black cat among roses,</p><p>Phlox, lilac-misted under a first-quarter moon,</p><p>The sweet smells of heliotrope and night-scented stock.</p><p>The garden is very still,  </p><p>It is dazed with moonlight,</p><p>Contented with perfume,</p><p>Dreaming the opium dreams of its folded poppies.</p><p>Firefly lights open and vanish  </p><p>High as the tip buds of the golden glow</p><p>Low as the sweet alyssum flowers at my feet.</p><p>Moon-shimmer on leaves and trellises,</p><p>Moon-spikes shafting through the snow ball bush.  </p><p>Only the little faces of the ladies’ delight are alert and staring,</p><p>Only the cat, padding between the roses,</p><p>Shakes a branch and breaks the chequered pattern</p><p>As water is broken by the falling of a leaf.</p><p>Then you come,</p><p>And you are quiet like the garden,</p><p>And white like the alyssum flowers,  </p><p>And beautiful as the silent sparks of the fireflies.</p><p>Ah, Beloved, do you see those orange lilies?</p><p>They knew my mother,</p><p>But who belonging to me will they know</p><p>When I am gone.</p></li></ul></li><li><p>A imagist poet as Pound .  Some intellectual game when you read some volume of her you just feel like .</p></li></ul><h1 id="Sylvia-Plath"><a href="#Sylvia-Plath" class="headerlink" title="Sylvia Plath"></a>Sylvia Plath</h1><ul><li>At her most articulate, meditating on the nature of poetic inspiration, [Plath] is a controlled voice for cynicism, plainly delineating the boundaries of hope and reality. At her brutal best—and Plath is a brutal poet—she taps a source of power that transforms her poetic voice into a raving avenger of womanhood and innocence.</li></ul><h2 id="When-I-am-dead-my-dearest"><a href="#When-I-am-dead-my-dearest" class="headerlink" title="When I am dead , my dearest."></a>When I am dead , my dearest.</h2><p>When I am dead, my dearest,</p><p>Sing no sad songs for me;</p><p>Plant thou no roses at my head,</p><p>Nor shady cypress tree:</p><p>Be the green grass above me</p><p>With showers and dewdrops wet;</p><p>And if thou wilt, remember,</p><p>And if thou wilt, forget.</p><p>I shall not see the shadows,</p><p>I shall not feel the rain;</p><p>I shall not hear the nightingale</p><p>Sing on, as if in pain:</p><p>And dreaming through the twilight</p><p>That doth not rise nor set,</p><p>Haply I may remember,</p><p>And haply may forget.</p><ul><li><p>徐志摩的翻译也很美，似乎也是他有首诗的foutainhead.</p></li><li><p>Not a red rose or a satin heart.</p></li></ul><h2 id="Valentine"><a href="#Valentine" class="headerlink" title="Valentine"></a>Valentine</h2><p>I give you an onion.<br>It is a moon wrapped in brown paper.<br>It promises light<br>like the careful undressing of love.</p><p>Here.<br>It will blind you with tears<br>like a lover.<br>It will make your reflection<br>a wobbling photo of grief.</p><p>I am trying to be truthful.</p><p>Not a cute card or a kissogram.</p><p>I give you an onion.<br>Its fierce kiss will stay on your lips,<br>possessive and faithful<br>as we are,<br>for as long as we are.</p><p>Take it.<br>Its platinum loops shrink to a wedding ring,<br>if you like.<br>Lethal.<br>Its scent will cling to your fingers,<br>cling to your knife.</p><h1 id="The-More-Loving-One"><a href="#The-More-Loving-One" class="headerlink" title="The More Loving One"></a>The More Loving One</h1><p>BY <a class="link"   href="https://www.poetryfoundation.org/poets/w-h-auden" >W. H. AUDEN <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>Looking up at the stars, I know quite well</p><p>That, for all they care, I can go to hell,</p><p>But on earth indifference is the least</p><p>We have to dread from man or beast.</p><p>How should we like it were stars to burn</p><p>With a passion for us we could not return?</p><p>If equally affection cannot be,</p><p>Let the more loving one be me.</p><p>Admirer as I think I am</p><p>Of stars that do not give a damn,</p><p>I cannot, now I see them, say</p><p>I missed one terribly all day.</p><p>Were all stars to disappear or die,</p><p>I should learn to look at an empty sky</p><p>And feel its total dark sublime</p><p>Though this might take me a little time.</p><p>September 1957</p><ul><li><p>今天读到的一首灵动的诗 引用的那篇文章也写得很好</p></li><li><p><font color=salmon>今天读完了那篇文章，很有趣，文笔也很优美，提到了很多有趣的idea，一个有趣的失恋者，哈哈，the more loving one .<a class="link"   href="https://www.poetryfoundation.org/articles/162120/but-there-are-other-geometries" > <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 在另一个世界 会有不一样的几何，有不一样的命运，也许我们就能够在理智与爱的坐标上相遇</font></p></li><li><h2 id="四月"><a href="#四月" class="headerlink" title="四月"></a>四月</h2><p>四月适合叶赛宁，四月不适合我。 （有点中二的发言  :)</p><p> Yesenin led an erratic, unconventional life that was punctuated by bouts of drunkenness and insanity. Before hanging himself in a Leningrad hotel, Yesenin slit his wrists, and, using his own blood, wrote a farewell poem.</p></li></ul><h1 id="Sergey-Esenin-I-do-not-lament-call-out-or-cry…"><a href="#Sergey-Esenin-I-do-not-lament-call-out-or-cry…" class="headerlink" title="Sergey Esenin I do not lament, call out, or cry…"></a>Sergey Esenin I do not lament, call out, or cry…</h1><p>I do not lament, call out, or cry.<br>All will pass like apple-blossom smoke.<br>Seized by golden glories of decay,<br>I shan’t see my youthful years come back.</p><p>You’ll no longer throb with equal passion,<br>Weary heart touched with a subtle chill;<br>Nor will you, green realm of birchen satin,<br>Lure me barefoot over dale and hill .</p><p>Vagrant spirit! Nowadays you scarcely<br>Stir these lips’ abiding secret blaze.<br>Ah, goodbye, my boyish effervescence,<br>Riot of eyes, and sentiments in spates!</p><p>I’ve become more frugal in my yearning.<br>My dear life, are you a dream where I<br>In the echoes of an early morning<br>Mount a rosy steed and gallop by?</p><p>We’re all mortal here without exception.<br>Maples shed their copper on the ground.<br>Blessed be, accept this benediction,<br>What has come to bloom and face its end.</p><ul><li>一切终将逝去，如苹果花丛中的薄雾，金色的树叶堆满心间，我已不再是青春少年。 打算试着翻译一些诗歌 : )</li></ul><h2 id="无论是什么-我都必须不断地进行探索存在"><a href="#无论是什么-我都必须不断地进行探索存在" class="headerlink" title="无论是什么 我都必须不断地进行探索存在"></a>无论是什么 我都必须不断地进行探索存在</h2><ul><li>环也许是存在的，但是并不妨碍探索本身。！！！！ 思考，想象力，好奇。</li></ul><h2 id="为什么Cowboy-Bebop-是伟大的-于我而言"><a href="#为什么Cowboy-Bebop-是伟大的-于我而言" class="headerlink" title="为什么Cowboy Bebop 是伟大的 于我而言"></a>为什么Cowboy Bebop 是伟大的 于我而言</h2><ul><li>因为有的只有最纯粹的故事。仅此而已，再无其他。</li></ul><h2 id="为何西语诗歌如此性感迷人"><a href="#为何西语诗歌如此性感迷人" class="headerlink" title="为何西语诗歌如此性感迷人"></a>为何西语诗歌如此性感迷人</h2><ul><li><p>主角不多，不是玫瑰也不是流云， 是你也是我，是某个男人，某个女人，是某段伤心的爱，是直面事实的激情，懊悔，是对生命最直接的参与。 是在诗歌中的自白，自白中的诗歌。 </p></li><li><p><font color=pink>努力使得自己的思想变得对人类有价值，就如同化石对于考古学家，即使不能留下完整的骨骼，也要努力使自己的屎变成化石。 goooood 今天想出来的笑话。 :)</font></p></li><li><p>上述观点有些武断，谨个人观点。————再读，看来十分武断，以后表述应当加上前缀 我觉得， 避免将事物描述成一种法则，给读者产生一种生硬的强加感。</p></li></ul><h2 id="村平"><a href="#村平" class="headerlink" title="村平"></a>村平</h2><ul><li><p>《电影的诞生》——村平的电影以及村平的电影观</p><p>为何叫村平，既有些像中文名，又有些像日文名，依次代表对我生命产生重要影响的中国与日本文化。 同时村 科研来自我的故乡， 这也许暗示了我所具有的电影审美倾向。   之后写的更多是单独的和本子记载的是互补的。</p></li><li><p>未来是否是最不需要关心的。 专注于当下的一种思想观念。是否是我电影观的核心。那么我应该怎样使用我的镜头，故事，人，调度， 取景，台词剧本， 声音 ，感官</p></li></ul><h2 id="lt-再见四月-gt-——-In-my-life-I-love-you-more-2024-4-30"><a href="#lt-再见四月-gt-——-In-my-life-I-love-you-more-2024-4-30" class="headerlink" title="&lt;再见四月&gt;—— In my life , I love you more - 2024.4.30"></a>&lt;再见四月&gt;—— In my life , I love you more - 2024.4.30</h2><h1 id="May你好五月"><a href="#May你好五月" class="headerlink" title="May你好五月"></a>May你好五月</h1><ul><li><p>五月关于三岛由纪夫，黑塞，列宁格勒牛仔和漫长的人生夏日</p></li><li><p>金阁寺  无时无刻拓展”美”的定义？ 或许美失去 褒义词的性质 应当成为一种更为广泛的性质    水手梦见陆地</p></li><li><p>西尔维娅 费尔南多 we make it simple 。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Darling-I-‘m-getting-older-亲爱的，我原来也会变老&quot;&gt;&lt;a href=&quot;#Darling-I-‘m-getting-older-亲爱的，我原来也会变老&quot; class=&quot;headerlink&quot; title=&quot;Darling . I ‘m g</summary>
      
    
    
    
    
    <category term="自然" scheme="https://spikeihg.github.io/tags/%E8%87%AA%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>CUDA</title>
    <link href="https://spikeihg.github.io/2023/10/24/CUDA/"/>
    <id>https://spikeihg.github.io/2023/10/24/CUDA/</id>
    <published>2023-10-24T06:20:19.000Z</published>
    <updated>2023-11-06T09:00:06.924Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CUDA-amp-Algorithm"><a href="#CUDA-amp-Algorithm" class="headerlink" title="CUDA&amp;Algorithm"></a><strong><font color=darkturquoise>CUDA&amp;Algorithm</font></strong></h1><ul><li><h2 id="Prelace"><a href="#Prelace" class="headerlink" title="Prelace"></a><font color=pink>Prelace</font></h2><p><strong><font color=mediumaquamarine>希望通过CUDA走进计算的前言，并且加深我对计算机体系结构的认知。同时从另一条路走进我们的machine learning 与 deep learning.同时也在这里写下一些算法的学习知识。</font></strong></p></li><li><h2 id="F-amp-Q"><a href="#F-amp-Q" class="headerlink" title="F&amp;Q"></a><font color=DarkSeagreen>F&amp;Q</font></h2><ul><li><p>内存布局具体硬件实现忘了，忘了栈实际上是在cache还是memory里</p></li><li><p>nvidia-smi 才是查看设别的指令 nvidia-smi -q 不错 </p></li><li><p>nvprof 已经弃用了 ncu（nsight-compute) 现在是profiler</p></li><li><p>nsight-compute 需要 全局安装 sudo apt install -y 选项<a class="link"   href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters#AllUsersTag" >issue <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p><font color=pink>driver 是一个硬件用来显示的   tookit就是一个集合 有些下载method可以同时下载适配的 driver 总结就是除了会安装还要学会卸载</font></p></li><li><p>These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case; it is important to not install the <code>cuda-drivers</code> packages within the WSL environment. <font color=red>乐死</font></p></li><li><p>Installation using RPM or Debian packages interfaces with your system’s package management system. When using RPM or Debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in &#x2F;var&#x2F;. Such a package only informs the package manager where to find the actual installation packages, but will not install them.</p><p>If the online network repository is enabled, RPM or Debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper.</p></li><li><p>安装是很复杂的 wsl有单独的教程 然后就是 有 post-installation mandatory actions !!</p></li><li><p>The <code>PATH</code> variable needs to include <code>export PATH=/usr/local/cuda-12./bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</code>. Nsight Compute has moved to <code>/opt/nvidia/nsight-compute/</code> only in rpm&#x2F;deb installation method. When using <code>.run</code> installer it is still located under <code>/usr/local/cuda-12.2/</code>.<font color=cyan>记住这个路径问题 在opt里面</font></p></li><li><p><font color=pink>妈妈我终于解决这个问题了 就是我在安装后没有设置环境变量 啊啊啊啊啊啊 男泵</font></p></li><li><p><a class="link"   href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions" >今后还可能出现的 问题 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>开始在vscode 里面进行配置 nsight debug</p></li><li><p>ctrl + space </p></li><li><p>命令面板很好用目前看来 ctrl shift + p</p><h2 id="我修改了提示-ctrl-t-s-好有用啊-还有就是控制面板太好用了有很多提示键-然后-task-也可以在里面选择生成"><a href="#我修改了提示-ctrl-t-s-好有用啊-还有就是控制面板太好用了有很多提示键-然后-task-也可以在里面选择生成" class="headerlink" title="我修改了提示 ctrl + t + s 好有用啊 还有就是控制面板太好用了有很多提示键 然后 task 也可以在里面选择生成"></a>我修改了提示 ctrl + t + s 好有用啊 还有就是控制面板太好用了有很多提示键 然后 task 也可以在里面选择生成</h2><ul><li><strong>[launch attach&amp; launch.json entry](<a class="link"   href="https://code.visualstudio.com/docs/editor/debugging" >Debugging in Visual Studio Code <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>)</strong></li></ul></li><li><p>dropdown configuration 就是下落的可以滑动的竖直设置栏</p></li><li><h3 id="预定义变量"><a href="#预定义变量" class="headerlink" title="预定义变量"></a><a class="link"   href="https://code.visualstudio.com/docs/editor/variables-reference" >预定义变量 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3></li><li><h3 id="最新发现-ctrl-alt-n-code-runner-似乎无所不能-但是不能调试只是"><a href="#最新发现-ctrl-alt-n-code-runner-似乎无所不能-但是不能调试只是" class="headerlink" title="最新发现 ctrl alt n code runner 似乎无所不能 但是不能调试只是"></a>最新发现 ctrl alt n code runner 似乎无所不能 但是不能调试只是</h3></li><li><p>program 就是 要debug的文件</p><ul><li><p><font color=gold>注意这个launch 是调试 要先生成可执行文件 也就是task 先配置的是task 然后是 launch.json</font></p></li><li><p><font color=green>重点出现了 发现可能的解决方案 就是prelaunch task 原来之前的是task 在debug后运行或者至少同时</font></p></li><li><p><font color=yellow>原来c_cpp_pr 是C++插件的配置文件不会影响</font></p></li></ul></li></ul></li><li><h2 id="Heterogeneous-Computing"><a href="#Heterogeneous-Computing" class="headerlink" title="Heterogeneous Computing"></a><strong><font color=tan>Heterogeneous Computing</font></strong></h2><ul><li><p><font color=teal>host指cpu，host codes run in CPU ,CPU code is responsible for managing the code and environment and device code running in GPUs.</font></p></li><li><p><font color=cornsilk>common GPU architectur GeForce Tesla and Fermi in Tesla  Tesla professional hpc. GeForce consumer GPUs</font></p></li><li><p><font color =cornsilk>two metrics to discribe the GPU compute capability .the core no. and the memory</font></p></li><li><p><font color =cornsilk>互补的 CPU 逻辑复杂 擅长分支预测控制流切换 GPU 擅长大量数据 简单控制 并行计算 Threads of CPU are heavyweighted 上下文切换开销大。 GPU就是相对轻量级 的</font></p></li><li><p><font color =cornsilk>CUDA driver API and CUDA runtime API 我们一般使用 runtime API cuda codes 包含两个部分 一个是host code 另一个是device code </font></p></li><li><p><font color =cornsilk>kernels 就是device code 里的并行函数由 nvcc 编译 nvcc 会区分host code and device code 然后就是完全分开执行 good<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/cucode.png"                                     ></font></p></li><li><p><font color =cornsilk>hello from GPU GPU program structure 5 steps 分配显存 加载数据 invoke kernel 返回数据 销毁显存</font></p></li><li><p><font color =cornsilk>locality temporal locality and spatial locality 这是编写cpu程序注意的 而GPU 将存储架构和线程结构都展示给程序员</font></p></li><li><p><font color =cornsilk>three key abstractions 三个关键抽象对于GPU 1. hierarchy of thread groups 2. hierarchy of memory 3. barrier synchronization </font></p></li><li><p><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/nvcc.png"                                     >nvcc 支持的文件后缀 .c 普通的是可以编译的</font></p></li><li><p><font color=cornsilk>programming model 其实就是 抽象 通过使用compiler and library &amp; OS 对hardware architecture 的抽象   scalability 可拓展性</font></p></li><li><p><font color=cornsilk>Host CPU and its memory ; Device : GPUs and its memory eg h_ for host m; d_ for device space</font></p></li><li><p><font color=cornsilk>Kernel 即跑在GPU 的codes我们可以看作是一个普通函数 实际上 GPU将其分配在多个线程上同时运行 ，当kernel运行后控制会立马交还给cpu以开始其他工作 异步工作。serial code 串行码 complemented by parallel code</font></p><div class="highlight-container" data-rel="Cpp"><figure class="iseeu highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Memory management */</span></span><br><span class="line"><span class="built_in">malloc</span>(); -&gt; <span class="built_in">cudaMalloc</span>();</span><br><span class="line"><span class="built_in">memcpy</span>(); -&gt; <span class="built_in">cudaMemcpy</span>();</span><br><span class="line"><span class="built_in">cudaMemset</span>();</span><br><span class="line"><span class="built_in">cudaFree</span>();<span class="comment">// all in device memory which is seperated from host memery!!</span></span><br><span class="line"><span class="comment">// the signature of the func</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="type">void</span>**devPtr,<span class="type">size_t</span> size)</span></span>;<span class="comment">// the pointer is returned in the devPtr</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span> <span class="params">( <span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count,cudaMemcpyKind kind )</span> <span class="comment">// the kind takes one of the following types cudaMemcpyHostToHost --HostToDevice --Dev2Dev D2H this func 是同步的 host 会阻塞知道完成</span></span></span><br><span class="line"><span class="function"><span class="comment">// cudaError_t enumerated type include cudaSuccess .eg </span></span></span><br><span class="line"><span class="function">    <span class="type">char</span>*<span class="title">cudaGetErrorString</span><span class="params">(cudaError_t error)</span></span>;</span><br></pre></td></tr></table></figure></div></li><li><p><font color=cornsilk>Global memory and shared memory in device just like memory and cache in CPU 前面的分配的函数都是在global memory 里面就像我们的malloc一样 目前我们所知道的由于这样内存分类 对应的指针是不能类型转换的，只能用cudaMemcpy来完成转移 后期由unified memory</font></p></li><li><p><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bg.png"                                     >通常而言 grid是二维 block是三维  blockDim gridDim dim3 type 没有初始化的filed自动为1</font></p></li><li><p><font color=cornsilk>P88 warp执行模型 32 个thread 硬件层面都会变成 warp 然后分散在SM上执行 之所以可以是主要是内存资源决定的 32 cores是共享的 前面说到多个warp scheduler 调度将warp的一个指令放到16core的一个组合上运行 其中register file 决定了warp 数量 shared memory 决定sm的block数量 然后warp切换上下文没有开销 都是data分割的 <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/limiter.png"                                     ></font></p></li><li><p><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/warp.png"                                     >warp 注意4 这个数字是由架构中每个SM的scheduler决定的 stall warp eligible warp 因此我们要最大化active warps</font></p></li><li><p><strong><font color=mediumseagreen>divergence 会执行所有分支 我们将分支按warp 划分</font></strong></p></li><li><p><strong><font color=pink>latency hiding <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/latency.png"                                     >类似于CPU的调度 latency就是时间 一般用clock cycle 计算大小</font></strong></p><ul><li><font color=cornsilk>P91 有趣的排队理论 就是需要同时并行的操作数&#x3D;延迟（cycle）*预期throughput throughput 与 bandwidth used interchangably bandwidth refer to as peak data transfer per time unit throughput refer to as any operations       rate metrics都是throughput单位 ops per cycle per SM 也可以进一步用warps表示也就是&#x2F;32 so the underlying thing of latency hidding is that you should increase the parallesiem to move like sequential ops without waiting </font></li></ul></li><li><p><font color=pink>latency hidding 总体而言需要更多的并行操作也就是需要更多的active warps 但是这个数量又是由memory and register 限制的所以configuration 很重要</font></p></li><li><p><font color=cornsilk>有趣的建议 <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/tipss.png"                                     ></font></p></li><li><p><font color=cornsilk>一些使用CUDA 的建议 <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bsize.png"                                     >太大的block size 会使每个thread硬件资源很少 太小的 block size warp 数量太少</font></p></li></ul></li><li><h2 id="GPU-ARCH"><a href="#GPU-ARCH" class="headerlink" title="GPU ARCH"></a><font color=Pink>GPU ARCH</font></h2><ul><li><strong><font color=cyan>The GPU architecture is built around a scalable array of <em>Streaming Multiprocessors</em> (SM). GPU hardware parallelism is achieved through the replication of this architectural building block. </font></strong></li><li><strong><font color=cyan>P 68我们先可以把SM看作一个比较强的硬件 一个grid 对应一个 kernel 一个grid的block可以分配到多个 SMs 然后一个SM 可以有多个block 也可能来自不同的grid(kernel 并发)每一个线程都具有流水线</font></strong></li><li><strong><font color=cyan>SIMT warp为一个基本管理 thread warp中每一个线程的内存与寄存器与计算资源都是独立的 SM将 block划分为warps 所以最好为32的倍数</font></strong></li><li><strong><font color=cyan>Even though all threads in a warp start together at the same program address, it is possible for individual threads to have different behavior. SIMT enables you to write thread-level </font></strong></li><li><strong><font color=cyan><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/SM.png"                                     ></font></strong></li><li><strong><font color=red>一个block只能安排在一个SMs！！！记住知道执行结束都在一个SMs 同样的一个SM可以同时有多个block</font></strong><ul><li><strong><font color=cyan>一个grid其实就是整个device了只是支持kernel的并行操纵 然后有个SM商店 shared memory 按照block划分 register 几万个按照thread划分因此 一个block间的thread可以shared mem 交流 While all threads in a thread block run logically in parallel, not all threads can execute physically at the same time. As a result, different threads in a thread block may make progress at a different pace.同一个block中的thread以warp执行 所以实际没有物理并行 这里可能会在 shared mem访存时出现竞争 CUDA提供了block内部的同步函数 但是多个block 之间没有提供同步函数 </font></strong></li></ul></li><li><strong><font color=cyan>一个core通常有一个整数ALU和浮点ALU <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/device.png"                                     >gigathread 就是全局的安排block到SM的</font></strong></li><li><strong><font color=cyan>two warps and issue one instruction from each warp to a group of 16 CUDA cores, 16 load&#x2F;store units, or 4 special function units (illustrated in Figure 3-4). The Fermi architecture, compute capability 2.x, can simultaneously handle 48 warps per SM for a total of 1,536 threads resident in a single SM at a time. 我们的这个关键就是 分组 其实有四个组合 然后选择其中一个一个作为执行选项 然后对于两个warp scheduler 就是两条流水线 然后其实每一组调度都可以看作是并行的了 不用再去管物理上的运行了上面说的48 个warp就是 同时dispatch 48 个 而不是同时运行 48个</font></strong></li><li><strong><font color=cyan>64KB memory 被分成了shared memory 和L1 cache两者关系运行更改通过runtime API </font></strong></li><li><strong><font color=red>Fermi also supports concurrent kernel execution: <font color=bluseagreen>multiple kernels launched from the same applicationtion context executing on the same GPU at the same time.</font> Concurrent kernel execution allows programs that execute a number of small kernels to fully utilize the GPU, as illustrated in Figure 3-5. Fermi allows up to 16 kernels to be run on the device at the same time. Concurrent kernel execution makes the GPU appear more like a MIMD architecture from the programmer’s perspective.<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/conc.png"                                     ></font></strong></li><li><strong><font color=cyan>LD&#x2F;ST 使用来进行转换地址的单元 16 个也是因为并行的原因</font></strong></li><li><strong><font color=cyan>kepler dynamic parallelism 允许 nested kernel invoke ; Hyper-Q 避免一个失败的kernel 调用 idle CPU 太长时间 多个task queue</font></strong></li><li><strong><font color=cyan>P 79 nvprof profiling driven 性能测试初步 类似linux里的一个 profile Event and metric</font></strong></li><li><strong><font color=cyan>memory bandwidth; compute resource ; latency</font></strong></li><li><strong><font color=cyan>Warps are the basic unit of execution in an SM. When you launch a grid of thread blocks, the thread blocks in the grid are distributed among SMs.  就是可以多个相同grid block在一个SM，也可以一个SM有来自不同block 最终硬件上都是一维</font></strong></li><li><strong><font color=cyan>warp 的划分原则 consecutive threadIdx.x !! 最后是向上取取整warps 如果非整数倍会出现空闲的不活跃thread 但是仍然会消耗占用硬件自资源也就是最终都是一维的硬件实现</font></strong></li><li><strong><font color=cyan>Warp Divergence 就是分支判断的问题 会 连串掩码式地执行 在优化等级较高时时间开销接近正常 解决就是 让一个分支用warp size 与运行</font></strong></li><li><strong><font color=cyan><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/reso.png"                                     >仔细想想居然很大程度上我们的这两个是独立影响的</font></strong></li><li><strong><font color=cyan>P89 A thread block is called an <em>active block</em> when compute resources, such as registers and shared memory, have been allocated to it. The warps it contains are called <em>active warps</em>. Active warps can be further classifi ed into the following three types: 三种 selected warp stalled warp eligible warp selected 不多于4个 是不是类似于我所说的流水线 4个选项 </font></strong></li><li><strong><font color=cyan>block太小 可以认为与大block相比同样的共享内存能偶拥有的thread 数量较少 所有register等没有充分利用 ；太大，线程太多，没有足够的thread</font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li><li><strong><font color=cyan></font></strong></li></ul></li><li><h2 id="Synchronization"><a href="#Synchronization" class="headerlink" title="Synchronization"></a><font color=slatecyan>Synchronization</font></h2><ul><li><strong><font color=cornsilk>两个层面 host and device 2 thread </font></strong></li></ul></li><li><p><strong><font color=cornsilk><strong>device</strong> void __syncthreads(void) 一个让同一个block 的线程同步的函数</font></strong></p></li><li><ul><li><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a><font color=yellow>Configuration</font></h2></li><li><p><strong><font color=cornsilk>配置函数 对于 blockdim 的innermost x 一般是32 的倍数 这个是有 warp 决定的 同时 一个block的thread数量不能超过 1024</font></strong></p></li><li><p><strong><font color=cornsilk>通常而言 block 数量越多 并行度越高 但是load throughput会下降 但是load efficency 更高 具有更高的achieved occupancy 但是实际上 由于 block 数量的限制反而会限制active warp </font></strong></p></li><li><p><strong><font color=cornsilk><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bal.png"                                     ></font></strong></p></li><li><p><strong><font color=cornsilk>第一个常见的算法就是 reduction 树形结构</font></strong></p></li><li><p><strong><font color=cornsilk>有 neighbor reduction 和 interleave reduction 后者拥有更好的global memory的局部性所以性能更好</font></strong></p></li><li><p><strong><font color=cornsilk>unrolling loop 同样的一个方法循环展开真的非常快 wtf 注意这里是Unrolling loop 注意 同步函数是用来进行 一个block 之间的同步的 block之间无法同步 </font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li><li><p><strong><font color=cornsilk></font></strong></p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CUDA-amp-Algorithm&quot;&gt;&lt;a href=&quot;#CUDA-amp-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;CUDA&amp;amp;Algorithm&quot;&gt;&lt;/a&gt;&lt;strong&gt;&lt;font color=darkturquois</summary>
      
    
    
    
    
    <category term="CUDA" scheme="https://spikeihg.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://spikeihg.github.io/2023/10/20/CS224N/"/>
    <id>https://spikeihg.github.io/2023/10/20/CS224N/</id>
    <published>2023-10-20T06:09:31.000Z</published>
    <updated>2023-11-16T12:02:01.890Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224N"><a href="#CS224N" class="headerlink" title="CS224N"></a><font color=velvet>CS224N</font></h1><ul><li><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a><font color=MediumSpringGreen>前置知识</font></h2><ul><li><h3 id="TERM"><a href="#TERM" class="headerlink" title="TERM"></a><font color=yellow>TERM</font></h3><ul><li><font color=green>line up对齐</font></li></ul></li><li><p><font color=aqua>in-place 就是一般是method 直接俄改变变量的 eg 。.add()</font></p></li><li><p><font color=aqua>cross product in matrix 就是我们所学的叉乘</font></p></li></ul></li><li><h3 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a><font color=chocolate>什么是机器学习</font></h3><ul><li><p>机器学习我的理解就是在一定条件下完成一定任务，其中任务的完成由程序本身实现。</p></li><li><p>监督学习 类似回归问题和分类问题</p></li><li><p>无监督学习类似聚类算法，没有提前的正确规则，让机器找规律</p></li><li><p><strong><font color=azure>anaconda 的使用 </font></strong></p><ul><li>原理就是 conda安装更方便 类似aptitude 可以自动帮助安装 然后就是创建虚拟环境在每个虚拟环境下安装自己的包 避免版本和包冲突</li><li>conda create –name <env-name>  <package-name></li><li>conda env list</li><li>conda –help</li><li>conda activate <env></li><li>conda info -e</li><li>conda deactive 退出环境</li><li><font color=red>注意要关代理 创建环境的时候更新版本的时候也要关代理 可以设置使得能够在代理下使用但是有一点麻烦</font></li></ul></li><li><h3 id="pytorch-tutorial"><a href="#pytorch-tutorial" class="headerlink" title="pytorch tutorial"></a><a class="link"   href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" >pytorch tutorial <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><ul><li><font color=pink>tensor 的维度 创建tensor torch.empty(3,4)里面是描述tensor 的shape attribute 两个数字说明是两个dim 然后3 说明有三个行向量 4就是每个行向量有4 个元素 empty 不会进行初始化的 1 dim 就是vector 2 dim is a matrix torch.zeros(),.ones(),.rand(), 要从已有的tensor 创建拥有一样shape 的 tensor 需要用法 *_like () method 具体就是这个中括号的层数 torch 也可以直接用python 的 list 和 tuple创建甚至是混合的；在创建tensor的时候可以确定dtype,然后也看可以使用方法.to(torch.int32)</font></li><li><font color=pink>数学操作 内置的算术操作针对每一个元素做标量操作 对于两个tensor 也是进行的in-place 操作 但是必须要相同的shape 否则runtime error 一个特殊的例外就是broadcast 详细思考一下 </font></li><li><font color=pink>braodcast 从shape的last to first开始比较，当相等 或者其中一个为1 或者其中一个不存在都满足 直到比较完 判断时候broadcastable 然后算数的规则是用prepend 1 不玩较小维度的维度 然后对应的每个维度取最大值 注意 in-place 操作也支持broadcast </font></li><li><font color=pink>理解broadcast 的关键就是意识到dim&#x3D;1 的时候我们可以把这个重复的地去乘</font></li><li><font color=pink>size 就是指一个维度的元素个数 从last 开始 dim 0 dim 1 注意是从0开始的，然后对应的是shape 的第一个(3,2,1) dim&#x3D;0 size&#x3D;3 注意是相反的！！！！！！</font></li><li><font color=pink>创建tensor 的时候指明一个required_grad 这样才能够在后面调用 grad时计算此项的gradient </font></li><li><font color=pink>仔细研读了一下 每一个tensor 都有维护一个.grad 来存储自己的偏导数 </font></li><li><font color=pink>torch.tensor()总是拷贝tensor 要尽量避免拷贝 detach() 改变 required属性</font></li><li><font color=pink>卷积的filter是一个多维的matrices 一定要注意的是我们的结果始终是一个2d matrix 对于一个filter不是多个filter 妈的 kernel 就是filter 带bias 的卷积操作就是卷积的时候加上bias 得到输出 传入kernel size 是一个int默认就是方正</font></li><li><font color=pink>dim 就是一个方向可以堆叠的方向 然后可以看括号来判断</font></li><li><font color=pink>N batch size 自己定义一些东西 dataset 可以抽象成一个 list  每一个元素就是一个 map 例如 path: label 之类的 feature 就是我们的prediction label 就是实际值 定义dataset 和 dataloader 就是第一步 注意一张定制化的思想 很不错面向对象编程</font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li></ul></li><li><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a><font color=orange>流程</font></h3><ul><li><p><font color=lightgreen>custom dataset and dataloader</font></p><p>   <font color=lightgreen>dataset 抽象定义为一个map 有一个annotation file 存储所有的样本的名字 还有存储的路径 两者结合可以得到一个图片的完整路径 然后预定义transformer 三个必要的 函数 ——init—— 就是 self.img_labels read csv 读入csv 然后定义dir和 transform  ——len——  ——getitem——  对于dataloader 设置加载方式 batch shuffle  多线程加速等</font></p></li><li><p><font color=lightgreen>layer</font></p><p>   <font color=lightgreen>Linear 就是一个layer 改变输入的最内层dim 的size 由输入的参数决定 注意我们的这个layer里面已经有预先设定好的bias 和 weight 相当于就是 对于图像可能就是一个 convolve </font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li><li><p><font color=lightgreen></font></p></li></ul></li><li><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a><font color=pink>梯度下降算法</font></h3><ul><li>同步更新所有变量 </li><li>出发点是想要拟合一段数据 然后我们想让整个数据组的误差最小。因此我们求导。可以理解为山坡上寻找下降路线。由于公式会随着接近局部最小点而自己缩小前进距离这是一个学习。</li></ul></li><li><p><font color=seagreen>输入是一个特征向量 的函数求偏导本质就是我们在微积分里面学习的矢量函数求导链式法则 θ的每个分量看作一个维度 然后是复合函数求导</font></p></li></ul></li><li><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a><font color =violet>极大似然估计</font></h3><ul><li>就是我们依照描述的事件写出这个事件发生的概率表达式，这个表达式由一个变量（涉及概率密度）决定。我们想求这个变量使得改概率函数取一个最大值。</li></ul></li><li><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a><font color=Maroon>代价函数</font></h3><ul><li>感觉与目标函数类似，一般与误差函数具有相同或者相反的单调性，然后通过一些数学技巧进行改写，以简化计算。</li></ul></li><li><h3 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a><font color=Teal>Batch Gradient Descent</font></h3><ul><li>这个就是传统的梯度下降，每一次前进时都要求遍历整个数据集来更新计算代价函数然后求偏导，计算量是非常巨大与难以实现的。具体原因我们会发现，偏导数求得的公式与每一个样本都有联系，例如差平方求和之类的。</li></ul></li><li><h3 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a><font color=Aqua>Linear Algebra</font></h3><ul><li>在此再次向Pro.Strang致以最崇高的敬意。</li></ul></li><li><h3 id="注意点——向量拓展的梯度下降以及向量函数"><a href="#注意点——向量拓展的梯度下降以及向量函数" class="headerlink" title="注意点——向量拓展的梯度下降以及向量函数"></a><font color=gold>注意点——向量拓展的梯度下降以及向量函数</font></h3><ul><li><p>注意函数变量的两个层面，一个是输入样本的维度，即样本向量的每一个维度，另一个是拟合函数中的变量即θ。h(x)(假设函数)&#x3D;θ0<em>1+θ1</em>x1 + θ2*x2+…… .eg 最后写成矩阵点积 多元线性回归</p></li><li><p>通常向量n+1 个 第0个是1为了简化表达 其余都是一个特征维度</p></li><li><p>根据上述结论重写表达式就是将θ化成对应n+1维向量然后求偏导时乘以一个xj^(i)的值。</p></li><li><p>比列失调的等高线梯度下降可能出现震荡，使用特征缩放相当于变量代换更高效将值约束在-1，1之间大约 还有归一化处理 使得平均值在0 x1-u1 代换 本质就是线性组合 u1 就是平均值 x1-u1&#x2F;s u1 就是平均值 s就是标准差 就是概率论</p></li><li><p>关于学习率α 过大可能会波动或者发散 国小很慢 总之尝试不同的一系列值</p></li><li><p>多项式回归 但是我还是有问题 函数都是人提出来 没有机器自己去寻找</p></li><li><h3 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a><font color=cyan>Normal Equation</font></h3><ul><li>线性代数永远的神，但是我已经忘记了~~~~~ 其实就是线代中的回归方程 男泵投影！！！！！</li><li>似乎用于线性回归，缺点：当n增大时会很慢 复杂度为3次方 而梯度下降可以正常的 大概10000为界限 例如 Word2vec 使用梯度下降法 而且只使用与线性 梯度是通法</li><li>pinv inv pinv 进阶求逆 可以是伪逆 可是当时没看</li></ul></li></ul></li><li><h2 id="Deep-learning"><a href="#Deep-learning" class="headerlink" title="Deep learning"></a><font color=tan>Deep learning</font></h2><ul><li><p><font color=red>简介。机器学习就是找函数function.在台大的课中只会有梯度下降 梯度下降开始的值朴素的是随机的，但是可能存在更好的 初始值全面的回归求解其实就是训练<br>模型就是我们提出的拟合方程 课程采用的是绝对值衡量</font></p></li><li><h2 id="piecewise-linear-curve所有线性的折线都可以用一组z来拟合-同理对于光滑的-我们可以无线细分-由piecewise-linear-curve-来逼近-进一步又由蓝色来逼近-！！！！！！！！！！！"><a href="#piecewise-linear-curve所有线性的折线都可以用一组z来拟合-同理对于光滑的-我们可以无线细分-由piecewise-linear-curve-来逼近-进一步又由蓝色来逼近-！！！！！！！！！！！" class="headerlink" title="piecewise linear curve所有线性的折线都可以用一组z来拟合 同理对于光滑的 我们可以无线细分 由piecewise linear curve 来逼近 进一步又由蓝色来逼近 ！！！！！！！！！！！"></a><font color=lavender>piecewise linear curve所有线性的折线都可以用一组z来拟合 同理对于光滑的 我们可以无线细分 由piecewise linear curve 来逼近 进一步又由蓝色来逼近 ！！！！！！！！！！！</font></h2></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/efunc.png"                                     ></p></li><li><p>y&#x3D;csigmoid(b+wx);  hard sigmoid w slopes b shift  </p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/bff.png"                                     ></p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/beauti.png"                                     ></p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/ddd.png"                                     ></p></li><li><p>sigmoid 的个数自己决定</p></li><li><p>实际的y帽 叫做 label</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/theta1.png"                                     ></p></li><li><p>batch 将N划分作batch随机的来求梯度</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/epoch.png"                                     ></p></li><li><p>epoch 是所有包都看了一遍 update就是一次更新 不一样</p></li><li><p>batch size learning rate 都是hyper parameter</p></li><li><p>ReLU rectified linear unit cmax(0,b+wx)就是hard sigmoid</p></li><li><p>就可以在所有sigmoid 使用的地方用ReLU</p></li><li><p>统称为activation function 老师都用的ReLU </p></li><li><p>可以多层进行变换 layers 就是得到a后再带入进去</p></li><li><p>多次ReLU 意思就是</p></li><li><p><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/network.png"                                     ></p></li></ul></li><li><p>为什么更深 乐 老师太好玩了！！！！</p></li><li><p>overfitting 过拟合问题 worse on unknown data</p></li><li><p>backpropagation </p></li><li><h3 id="anoconda-创建指令是全局的conda-create-然后可以在里面下载包-用vscode-启动可以-注意激活的时候要把代理关了"><a href="#anoconda-创建指令是全局的conda-create-然后可以在里面下载包-用vscode-启动可以-注意激活的时候要把代理关了" class="headerlink" title="anoconda 创建指令是全局的conda create 然后可以在里面下载包 用vscode 启动可以 注意激活的时候要把代理关了"></a><font color=pink>anoconda 创建指令是全局的conda create 然后可以在里面下载包 用vscode 启动可以 注意激活的时候要把代理关了</font></h3></li><li><h2 id="jupyter-notebook-guide"><a href="#jupyter-notebook-guide" class="headerlink" title="jupyter notebook guide"></a><a class="link"   href="https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/install.html" >jupyter notebook guide <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h2></li><li><p>jupyter 可以使用命令行调用 </p></li><li><pre><code class="python">jupyter notebook 然后就进入了browser</code></pre></li><li><h2 id="Colab-使用"><a href="#Colab-使用" class="headerlink" title="Colab 使用"></a><strong><font color=slategray>Colab 使用</font></strong></h2><ul><li><p>python code 和 shell code 其中！接shell cmd cd除外 %cd</p></li><li><p>可以选择执行的硬件 GPU runtime type 里面</p></li><li><p>ctrl+ enter 执行一个代码cell</p></li><li><p>总体而言其实就是jupyter 只不过是个互联的jupyter.</p></li><li><p>左侧的文件图标查看结构 注意下载邮寄 可以上传到google硬盘</p></li><li><p>注意自己使用的时候是在google的GPU上 所以程序结束就会消失  注意自己保存</p></li><li><p><strong><font color=lightcyan>打开新的需要在file 里面upload notebook!!!! 可以的 注意一次只能有一个session 所以需要关掉前面的 notebook maybe<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/save.png"                                     >真的很不错一个tesla 真棒 然后我可以试试ssh之类的</font></strong></p></li><li><p><font color=yellow>然后现在发现了 ctrl+e 普通搜索很快 然后url 对url很快 因为对普通搜索会转换为我们的query 条目 然后会比较慢！！！</font></p></li><li><p><a class="link"   href="https://github.com/virginiakm1988/ML2022-Spring" >ML github repo <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p></li><li><p>pytorch tensor 相当于 array 可以GPU 加速</p></li><li><h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a><font color=green>Pytorch</font></h2><ul><li>tensor就是高维数组 </li><li>还得复习一下基本的python 语法 list dict class func 基本的一些使用 顺便复习写一写爬虫</li><li>tensor constructor numpy zero tensor unit tensor</li><li>每个batch 的 loss funct 可能存在不同的差别</li><li>sigmoid 或者 Relu 叫做 neuron 总体叫做 neural network</li></ul></li><li><h2 id="Python-review"><a href="#Python-review" class="headerlink" title="Python review"></a><font color=purple>Python review</font></h2><ul><li><p>if var in list:  if var not in list:</p></li><li><p>if var1,var2 not in list；</p></li><li><p>for key,value in dict:</p></li><li><p>for key in sorted(dict.keys()):</p></li><li><p>for value in sorted(dict.values())</p></li><li><p>answer &#x3D; input(‘please enter your answer’)</p></li><li><p>int(input(‘how old are you’)) </p></li><li><p>f”{var1_has_defined} {var2_has_dafined}” mesg_to_be_printed&#x3D;f””</p></li><li><p>python 函数调用时候 可以直接指定 def fun(var1,var2): …… fun(var1&#x3D;yes,var2&#x3D;no) 但是一定要记住名字 不要出错</p></li><li><p>默认形参也是放在后面</p></li><li><p>while some_list:</p><p> ​item&#x3D;list.pop()</p><p> ​do_with(item)</p></li><li><p>dict[‘new_key’]&#x3D;new_value</p></li><li><p>def fun(list_para):…     fun(list[:]) 传递一个切片  函数都是引用一定会修改变量的</p></li><li><p>可变形参 def func(*tuple_para): def func2(size,**dict_para):</p></li><li><h3 id="Class-in-Python"><a href="#Class-in-Python" class="headerlink" title="Class in Python"></a><font color=maroon>Class in Python</font></h3></li><li><p>class my_class(): 开头的书写方法</p><ul><li>def __init(self,para1,para2)__self 必须第一个</li><li>然后接着是 self.para&#x3D;para(实际传入实例类的形参)这样写之后this 相当于才拥有这些成员</li><li>​普通方法 def member_func(self): 不要忘记了self</li><li>如果要有具有默认初始值的属性 可以直接在__init()__ 下面进行写 self.prop&#x3D;1000 prop 不用出现在init括号里面</li><li><font color=aqua>继承</font></li><li>首先必须括号里写明继承的类 class derived(base):</li><li>super()._<em>init(para,para,para)</em>_注意里面没有self 继承全部内容</li><li>自己属性接着写就可以</li><li>可以重写父类方法 名字不同就可以</li><li>可以类实例作为成员 self.class_mem&#x3D;classA()</li></ul></li><li><p>这个 <strong>init</strong> 不是必须的方法 只是用来定制实例化时 类似的还有 _<em>self</em>_  _<em>next</em>_  等用来控制 迭代器的 同时呢 生成器 generator 是一个综合了上面方法功能的函数 yield  generator expression</p></li></ul></li><li><p><font color=green>文件操作</font></p><ul><li>with open(‘filename’) as name:  不需要close了 因为with</li><li></li></ul></li></ul></li><li><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a><strong><font color=cornsilk>工具</font></strong></h2><pre><code>    *  training data 上的loss过大</code></pre><ul><li>Model bias 就是 我们的函数太简单 解决方法 一 增加 特征量 二 增加layer deep learning</li><li>优化问题 梯度下降的问题</li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/optm.png"                                     ></li><li>怎么解决 优化的问题 next lecgt</li><li>一定区分 overfitting 和 优化问题 一个是test data 一个是 training data<ul><li>overfitting <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/freestyle.png"                                     ></li><li>解决方法 增加 training data 二 data augmentation 就是自己创造一些条件 创造一些资料 需要有道理</li><li>减小弹性 增加限制</li><li>full- connected比较有弹性目前我们讨论的； CNN 比较无弹性<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/stop.png"                                     ></li></ul></li><li>区分 overfitting 与 model bias  存在一个complexity 与 bias 关系</li><li><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/trade.png"                                     ></li><li>刚刚好的<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/aaa1.png"                                     ></li><li></li></ul></li><li><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a><font color=pink>Tips</font></h3><ul><li><font color=mediumspringgreen>critical point saddle point &amp; local minima 前者更多 通过hessian 矩阵来判断 特征值来判断 全正或者全负local其余就是saddle point</font></li><li><font color=mediumspringgreen>针对local point 的方法 batch size  一般来说越小noise 越多但实际上更好 但是在并行计算下可能更慢 第二 momentum 惯性一样的下一步加成</font></li><li><font color=mediumspringgreen>learning rate 的问题 有一个方法 叫做 Adam Optimizer 就是RSM 加上 momentum 的结合 可以动态改变learning rate 。 也就是说我们的learning rate可能也是 loss stuck的原因，而非 critical point </font></li><li><font color=mediumspringgreen>loss 函数也有影响 对于分类问题而言 使用最多的 是 cross entrophy 原来是用似然函数 好处就是 可以将整个surface 放得平缓</font></li><li><font color=mediumspringgreen><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/Fea.png"                                     >理解权重影响的问题 和这个surface是怎么来的 我们要不断修正的是w1 w2  mean 就是平均值 standard deviation 就是标准差 标准化  数学的影响就是 loss converge 收敛更快 但是还是有个问题，在实际的多层神经网络中 每经过一层 可能分别差别又会变大 所以我们还是需要不断地进行normalization 可以是activationfunc 之前 也可以之后 sigmoid 最好之前 因为可以化到-1 1 之间使得函数的值变化比较大<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/norm.png"                                     > 这个优化提升的是训练速度 主要是<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/arx.png"                                     ></font></li><li><font color=mediumspringgreen>CNN 卷积神经网影像处理，一个图像就是一个RGB的三位channel 的tensor 就是一个高维的叠加的数组 拉直就是一个向量 但是我们一般不会全部进行训练 我们会进行一定的相关的简化receptive field 这样做的一个理论 就是探查pattern 用pattern 去进行识别</font></li><li><font color=mediumspringgreen>一般通常选取都是三个channel 然后此时的长宽称为kernel size 3X3 通常就可以了 stride hyper para 超出的部分进行padding <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/stri.png"                                     ><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/filter.png"                                     ></font></li><li><font color=mediumspringgreen>fully connected layer弹性最大  receptive field 共享参数 减小了弹性这两个加起来就是convolution al layer 对应的就叫 CNN model bias 较大<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/fmap.png"                                     >这里的channel变成了neuron 的个数了</font></li><li><font color=mediumspringgreen>pooling 方法 max pooling  的方法 为了减少运算量 现在开始减少了<img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/CNN.png"                                     >下面那个是flatter 还有一个重要应用 就是 playing go</font></li><li><font color=mediumspringgreen>分类问题softmax的原因简单解释 就是 我们用one-hot 向量表示我们的类 然后用1 然后我们将softmax 将其转换为-1 到 1 当然我觉得可能还是因为概率分布的问题就是越大的比例越大</font></li></ul></li><li><h2 id="self-attention-自注意"><a href="#self-attention-自注意" class="headerlink" title="self-attention 自注意"></a>self-attention 自注意</h2><ul><li><font color=pink>问题引入 加入我们处理的input data是一个向量序列 而不是一个向量。 对应的输入也有不同的种类。比如说输入的每一个向量都计算一个label 例如判断文本每个单词的 词性 或者类似的分裂问题 。或者一个输出 比如对一句话进行定义  反正应用情形自己去想象 最复杂也许是seq2seq 输出的已是一个序列 例如翻译</font></li><li><font color=pink>sequence labeling 如果仅仅使用前面的network 然后单独输入的话存在一个巨大的问题就是无法做到考虑上下文使用情形有限</font></li><li><font color=pink>attention is all you need <img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/matr.png"                                     ><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/attention.png"                                     ><img                       lazyload                     src="/images/loading.svg"                     data-src="/../images/multi.png"                                     ></font></li></ul></li><li><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><ul><li><font color=pink>encoder FFN feed forward network </font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li><li><font color=pink></font></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CS224N&quot;&gt;&lt;a href=&quot;#CS224N&quot; class=&quot;headerlink&quot; title=&quot;CS224N&quot;&gt;&lt;/a&gt;&lt;font color=velvet&gt;CS224N&lt;/font&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;h2 id=&quot;前置知识&quot;&gt;&lt;a href=</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://spikeihg.github.io/tags/NLP/"/>
    
  </entry>
  
</feed>
